howdy this is Jim rut and this is the
Jim rut show listeners have asked us to
provide pointers some of the resources
we talked about on the show we now have
links to books and articles referenced
in recent podcasts that are available on
our website
we also offer full transcripts go to Jim
rut show com that's Jim rut show com
today's guest is Gary Marcus author of
the recent book rebooting a eye thrilled
to be here Jim hey great to have you
Gary's the founder and CEO robust AI a
new company with the goal to make robot
smart collaborative robust safe flexible
and genuinely autonomous prior who's the
founder and CEO geometric intelligence a
machine learning company acquired by
uber in 2016 he's the author of five
books including one of my favs from the
past which I had no idea was him called
guitar zero which both me and my life
loved if you like music and you like
cognitive science go read guitar zero
but read the new one rebooting AI first
here he's also a cognitive psychologist
he's published extensively in fields
ranging from human and animal behavior
to neuroscience genetics linguistics
evolutionary psychology and artificial
intelligence according to his website
he's perhaps the youngest ever professor
emeritus at NYU I should also note that
he attended Hampshire College as did an
earlier guest on the show physicist Lise
Malin must be something interesting
going on up there they taught us to
speak truth to power at Hampshire
College and that's what Lee and I have
in common we are both going after some
very popular views and a lot of people
don't like that but we speak the truth
as we see it I like that I can see
exactly that that template is the case
both you and Lee are challenging the
conventional wisdom in a major way and
neither of us are afraid to do it I mean
doesn't win us a lot of friends in
certain quarters but you know science
doesn't progress by people being chummy
with each other it progresses by
recognizing limitation
and taking the next step absolutely a
significant part of your new book is an
attempt to be more cautious and
realistic about the short-term prospect
for AI without denying its long-term
potential on what I might call the
oversold side you lay out a three-part
description of what you think a big part
of the problem is you call it the AI
chasm could you take us through that
well and there are multiple parts to it
but I think we could start with the
gullibility gap so the gullibility gap
is that we are very prone to seeing
small signals of intelligent behavior as
if there's more intelligence there than
there really is
so classic example is Eliza which was
just a dumb keyword matching thing that
people would talk to as if it was a
psychiatrist and some people thought it
was really a psychiatrist so it would
match things you would say I'm having
trouble with my girlfriend and it would
say well tell me about your family and
people thought that was really smart but
it didn't actually know what a
girlfriend was it didn't know what a
relationship was it didn't know what a
family was it was just parroting those
things back it might as well have been a
parrot and lots of people thought that
it was smart so they're gullible they
see a small sample of what looks like
intelligence our brains didn't evolve to
say hey that person's intelligent and
that's really just a machine and just
get suckered in another example that is
there was a guy who was so trusting of
his Tesla after to driven him around for
a few hours a few days or whatever it
was that he thought it'd be okay to
watch Harry Potter in the back and and
we all know what happened to that person
right they watched Harry Potter while
their car drove for them and it ran into
a trailer that took a left turn on a
highway and the person was killed so you
know it can be really dangerous to trust
machines too much but we have a strong
tendency to do that so the second thing
that we talked about is the illusory
progress gap you see a little bit of
progress on some piece of a problem and
you think that that means that we've
solved the whole problem so you get some
system that does a tiny bit of language
like it recognizes your request to turn
off the lights Alexa turn off their
lights and you suddenly think well AI
has solved the understanding problem or
you get a system that can interpret a
few of our requests on Google and you
think that Google must understand
natural language
of course google doesn't understand
natural language it understands a small
fragment of natural language but it's
very easy to fool it and it can't
actually go and integrate all the
information from the web it can just put
together a bunch of webpages that
matching keywords or use synonyms for
your keywords and things like that but
just because Google can do a piece of it
doesn't mean you can actually have a
conversation with it so we're very prone
to seeing small steps as if they mean
more than they are the truth is that AI
is really really hard we've been working
on it for 60 years and we don't have
machines that understand the world that
can build what I would call a discourse
representation of the things that we're
talking about and interpret that it just
doesn't exist yet and yet people write
newspapers and stories as if you know
this latest little bit of progress was
huge progress another example was
Microsoft beat this thing called squad
or you could argue about whether it be
but we'll say for president purpose is a
beat squad which was a test of
underlining text as you read passages
and that sounds a bit like reading in
fact it involves a bit of reading but
just because you can read what's
underlined able in the text doesn't mean
that you can make inferences that's them
beyond what's written even though those
are obvious to any person and so the
media accounts were like humans are
going to be replaced by robots because
machines can read as well as humans but
any human even like a nine-year-old can
read things that aren't explicitly
stated and infer the connective tissue
between the things that are stated and
the ones that aren't and no writer worth
their salt will tell you absolutely
everything about everything because it
would be incredibly tedious so things
are left out you don't tell people that
if you drop something that it hits the
ground you because people know that if
it's dropped it will hit the ground but
the machine doesn't realize that if it's
not set explicitly in the text
absolutely in the third party or AI
chasm you called the robustness gap with
the robustness gap is about is we have a
lot of systems that work say 60% of the
time or 70% of the time but don't work a
hundred percent or even close to a
hundred percent and that's fine for some
things like if you're recommending books
and you like my guitar zero book and you
don't like my new book well you know I'm
sorry and you wasted a little money but
nobody dies but if you have a driverless
car system it works 90% of the time and
one
x goes out and as an accident that would
be a disaster so there are many systems
out there that work a little bit I was
just having an argument on Twitter with
someone about a math system that's
between depending on how you count fifty
and seventy-five percent right well it's
idiotic to use a system that is 75
percent right when you've got a
calculator that's a hundred percent
right and if you're only 75 percent
right you're not really that robust and
if you're 50 then yeah I don't even know
why you're pretending that you can do
maths at all they might as well use a
coin yeah the robustness question is
that obviously hugely important and
speaks to you know what I think within
the popular imagination is the most
significant salient application of AI
which is self-driving cars actually look
this up and it's useful to keep in mind
that the number of miles you'd have to
drive to kill somebody with a human
driver and that includes a drunk one a
sleeping one and that one just had an
argument with its girlfriend is 10
million miles I think the average is one
in 134 million is the average I mean of
course it can happen your first day of
driving but I think it's on average 130
K but even if it were 10 million 110
million it would still be much much more
reliable than your smartest driverless
car which was probably away mo you know
looking the published statistics and
what those statistics say is that
driverless cars need an intervention
about 1 in every 10,000 miles that's you
know many orders of magnitude away from
the human safety statistic for fatality
yeah I remember the time I looked this
up the total number of miles by way mo
was less than 10 million all right say
go okay no wonder hadn't killed anybody
right to your anything a you're making a
good second point which is we don't even
have the right number of miles to even
begin to think that we've achieved
reliability so if you had a million
accident-free miles that still wouldn't
really be anywhere near enough big
enough sample to establish with
certainty that you were as safer or
safer than people and I was making a
second point which is the reality is
that these cars need human help every
10,000 mile or on average ones in 10,000
miles that's the best in in breed and
that's just not good enough for what we
call level 5 autonomy where I pick you
up to point a take you to point B the
way that uber works
so you know people sometimes like to
make fun of over goober drivers but
human drivers are way safer way more
reliable require Laila less help human
driven uber cars then then do you know
your average human I mean definitely I
have a driverless car excuse me I did a
quick look up and we were both wrong
about the the actual human kill rate
which is 1.25 deaths per 100 million
vehicle miles so that would be what
about one in every 75 million miles
something like that so but it's still a
probably still a bigger number than all
the miles driven by all the self-driving
cars you will send me a link because I
have seen other numbers yeah this one's
from the whoops it's from Wikipedia
maybe it isn't that good but quoting the
National Safety Council using
methodology that differs weapon to the
same ballpark hundred million plus or
minus hundred million is what I usually
hear yeah and doubtful I don't know the
current number but I wonder if the total
number of miles driven by all the cars
not under human intervention amounts to
100 million yet don't know another way
you've stated this problem is that in a
word current AI is narrow it works for
particular tasks this program for but it
when it encounters something that's even
a little bit different it doesn't know
what to do am I have a friend then burps
don't like to give the example but that
alphago is amazing as it was trained on
a 19 by 19 board if you put it on an 18
by 18 board it wouldn't play very well
if you ask to play checkers it couldn't
play at all that's right we use that
example in rebooting AI as well of
playing on a different board and we riff
on it so alphago doesn't for example
even know that there is a board with
stones on it it doesn't actually have
the ability to recognize stones in
different lighting conditions the way
any go player would be doesn't know how
to pick up the stones it doesn't
understand that stones on a board or a
metaphor for territory and human battles
or whatever he really just knows about
that grid of that size and that's all it
knows about and you could retrain it to
play on 18 by 18 but you'd need it to
play another 30 million games
there absolutely will get the 30 million
also later which I considered to be a
big hole in the and the current
approaches getting back to where you
were first going you know it seems to me
that an awful lot of the difference
between current a is and how humans or
even our dogs deal with the world is we
have context and world models mmm-hmm
here's a quote from your book even a
young child countering a cheese grater
for the first time can figure out why it
has holes with sharp edges which parts
allowed cheese to drop through which
part to grasp with your fingers and so
on but no existing AI can properly
understand how the shape of an object is
related to its function and all that's
got to be driven by world models you
know psychological physics all kinds of
stuff does that make sense to you I mean
that's exactly what we're arguing is
that the current neural networks they
learn a lot about statistics but they
don't really build a model of the world
so you can have a conversation or not
have a conversation but test the
language abilities of this thing called
GP t2 which is one of the most powerful
language models right now over talk to
transformer comm and if you type in
questions of the form here's a sentence
and then you fill in the blanks you get
really bizarre answers that show that
even though it knows the statistics of
what's going on it doesn't actually
understand what's going on I'm just
gonna see if I could quickly pull up an
example of the kind of thing I was
playing around with the other day a
water bottle breaks and all the water
comes out leaving and you know what kind
of answer would you expect like well if
all the water is out you should have
roughly zero but it says roughly in one
example two hundred gallons of water
like no water bottle in the real world
leaves two hundred gallons of water
you know after it's broken or here's
another example from the same thing
water bottle breaks and all the water
comes out leaving roughly and it says
six to eight drops of beer well that's
funny but it doesn't show that the
system actually understands that when
the water comes out of the bottle you
basically have an empty bottle and
there's just millions of examples of
this sort that's frankly what I would
expect from something like deep learning
which is essentially a very large
structure for capturing statistical
regularities and
that's something different than we might
call artificial general intelligence
maybe even before we get to artificial
general intelligence just to emphasize
your last point a step to artificial
intelligence is you see what's going on
in the world with your sensors which
might be cameras or people type things
in or whatever and what you're trying to
do is to build a model of what's going
on right now so your listeners for
example have a model that we have two
people that are talking to each other
may not know exactly where they are but
you can if you're a listener you in your
model for example think that they're
both English speakers that both
knowledgeable about machine learning
they're both opinionated and so you're
building up some view of what's going on
and then you use that in order to
interpret the next thing that goes on
well that cumulative process the model
building process is one of the
fundamental things that AI just hasn't
solved yet and deep learning in
particular doesn't really solve so it
evades it by having large amounts of
statistics and sometimes that tricks
people but that also leads to the lack
of robustness that we're talking about I
would argue you can't possibly get to
eight general intelligence if you don't
have that process of cumulative model
building as one of the core things that
you do yeah that seems very reasonable
to me it seems reasonable to you but
it's been very hard for me to get the
field to engage in that so you have a
lot of people that are working on deep
learning they're extremely well-paid
they're excited about what they're doing
and they're not really familiar with the
literature on cognitive psychology which
a lot of which is about cognitive model
building and people just don't really
appreciate at that point so you know
geoff hinton is saying you shouldn't
listen to Gary Marcus because he thinks
that neural networks don't understand
but look at Google Translate it
understands and the thing is that Google
Translate does not have a deep
understanding of what's going on it does
not build up the kind of model that
we're talking about so you could feed it
in a passage from Harry Potter it might
translate it by looking at the
statistics of how this phrase connects
to this other phrase but if you asked it
at the end what happened in this scene
it doesn't know where to start because
it's not building up an interpretation
of you know this person wound up in this
room and they had a magic wand or
whatever it is that they did it's just
kind of like like china the old Searle
Chinese room thing it's just matching
little pieces together it's not
constructing these cumulative models of
the narrative that it's interpreting in
just the way that we were talking about
and the field does not want to engage in
that issue and so it's kind of a little
bit delusional about how much progress
it goes back to the illusory progress
thing just because you can do machine
translation doesn't mean that you have a
system that can interpret a narrative
yet probably why it seems reasonable to
me is since 2014
I've spent a considerable amount of time
reading the cognitive science and
cognitive neuroscience literature and
particularly focusing on the
intersection between consciousness and
cognition so I have you know a
relatively rich understanding of these
things and I use that as my existence
proof for artificial intelligence and
compare other approaches to that so I
can see whether it falls short as an
example people in the deep learning side
who are so insistent on deep learning
uber alles I had a chat one time with a
guy named Jurgen Schmidt Huber who I'm
sure you the name you're familiar with
and he kept saying just over and over
that his lsdm version of deep learning
was Turing complete and then brushed
aside any need for any other approach
during complete yeah sometimes I feel
like people in the field understand math
a lot better than they do psychology
linguistics the cognitive science is
more generally so I mean it's not
surprising that if you dig into those
fields you start to understand what the
problems of that cognition whereas if
your approach is purely mathematical
then you get very good at kind of moving
the statistics around but you don't
necessarily learn what needs to be
solved if you think about consciousness
or even just cognition in general a lot
of it is about constructing narratives
essentially in internal models and
that's mostly not on the radar to
Schmidt hubris credit he's actually got
a paper about building internal models
last year that it's an interesting paper
but by and large the field does not
engage in that set of issues and does
not engage in the cognitive science
literature in general so it's not that
these people aren't very very bright
they're extremely good at what they do
but like the models that they make
they're narrow in a certain way I think
that that impedes progress to something
that and even though they're focused on
the math side which many of them are I'd
say that a rich understanding of the
mathematics of learning ought to make
you suspicious of one-size-fits-all
particularly one of my very favorite
results is the no free lunch there
yeah which basically states that there
is no best way to solve any problem or
to search any database and that any such
general statement ought to be
inadmissible without evidence from the
domain in question so that would be my
response so this is this technique is
the answer period I say such things are
at empirical question let's see how they
work in specific domains right just to
rephrase that no free lunch theorem it's
really that there's no one technique
that is gonna be general across all
problems so you may have a specific
problem where there is a best technique
but there isn't no it is provable by
that theorem that you're talking about
that there's not a universal technique
and so when a Shamir tuber or something
like that says well I've got this
universal technique you should keep your
hand on your wallet and realize it's
actually going to be good for some
problems and not others what's
interesting about human biology is that
we have a lot of different mechanisms
for solving many different problems that
we've gotten over evolutionary time some
of them are you know pretty good and
some of them have problems and so forth
but we have many different techniques so
observational learning for example is
not the same thing as learning language
or as you know calibrating what you're
gonna do with your hands as you catch a
tennis ball yeah so the we actually use
multiple techniques there isn't a
one-size-fits-all as you're saying and
there's there's a long tradition in
intellectual history of people kind of
overreaching in that way so for example
behaviorism try to capture all of
psychology in a single set of equations
and you know some of what it said is
true like we're all motivated by reward
but the fact that we're motivated by
reward doesn't give you any of the
nuance for example about how we do
physical reasoning to understand whether
something's gonna tip over you're
absolutely right there is for many
people a tendency towards looking for
one answer and in fact why I often say
that I when i once i get to know
somebody of some intellectual depth in
the side
I make a determination do they
understand the no free lunch theorem or
don't they and I would say it's about
seventy-five percent they do not
yeah well the related point I agree with
you I think it's a good litmus test the
the related point is if you don't know
it then you tend not to appreciate the
value of nativism so everybody
appreciates that learning is part of how
we get to be who we are and we acquire a
lot of information from culture but one
implication of no free lunch theorem is
that having innate priors for particular
kinds of problems innate knowledge about
particular kinds of problems can be
really helpful so if you were not born
knowing that there were persisting
objects in the world and you just had a
lot of kind of sense perception it would
take a long time to figure out from all
those correlations that objects are even
things to look at and so people that
really appreciate the no free lunch
theorem I think are often much more
receptive to there being some kind of
nativism in biology and I would argue
that AI is not gonna proceed until we
similarly allow some and eat us in there
and a lot of people don't want it I know
that's very weird they think that from
very basic first principles we can solve
the world and maybe they can but it
doesn't seem to be the way it's been saw
previously I like the example you give
actually if objects object your core to
the work I do on the intersection
between attention and consciousness and
from my bigging around the literature it
seems clear to me that objects are
pretty damn innate right you cannot look
at the world without having it chopped
up into objects for you and that seems
to happen at a very young age I mean we
can't absolutely prove this but it seems
very very likely I like to you know
people humans don't like to think that
humans have innate stuff in it there's a
strong bias my friend IRA's favorite is
actually documented that empirically
with some papers that are coming out
so as lilac Lightman used to say
empiricism is innate believing though we
don't have anything built-in is is a
weird innate bit about human psychology
but if I show you a picture of a baby
ibex climbing down the side of a
mountain a few hours after its birth
there's just no coherent way to do that
other than to think that that baby ibex
already understands that there is a
three-dimensional geometry to the world
that relates to
constituent parts of the things that
it's traversing like you have to assume
that or else you just can't explain what
the behavior is you can't say that in
you know 180 minutes of experience that
the baby ibex has learned all this stuff
about objects in three-dimensional
geometry just does not make any sense
you mentioned the ibex is what I use as
my model animal for my conscious
cognition work is the white-tailed deer
and why did I choose that I've been a
deer hunter for 50 years I have a fair
amount of theory of mind about how beer
behave from the time they're born to the
time they're you know the the top bunk
in a woods and I find that to be very
very useful way to think about problems
of this orcas you're exactly right a
white-tailed deer is up and running
within two hours and is clearly
navigating a complex object filled space
if it didn't understand it be running
into trees all of them and then I mean
you know systems that can actually
navigate trees like sky dioz drones they
they have a lot of innate structure in
order to help them recognize that they
don't learn that from scratch then we'll
go back to the later we talk about
hybrid systems about how perhaps in a
sub symbolic neural net approaches may
conform at least approximately to our
perceptual and the higher end of
perceptual object recognition systems in
a hybrid architecture that's I'll get to
later another problem I see with neural
nets oh I think you pointed this out as
well is the very large number of
examples that these systems so far have
had to get any reasonable level of
confidence probably the world's best
chess player is played maybe fifty
thousand or a hundred thousand games as
compared to the hundreds of millions so
that piece of self learning a eyes have
to have and let me give you an example
for my own personal history that really
picked my interest in this small
learning set size and not me hooked up
with Josh Tenenbaum up at MIT and I
chatted fair amount about this problem
is one of my hobbies since I was a kid
was wargames I've been playing war games
since I was ten years old which is 55
years ago starting with the old Avalon
Hill cardboard pieces on maps kind of
thing and that played everything up to
the current state-of-the-art
computerized games and anyway about 2014
right when I was getting interested in
cognitive science cognitive
and AGI I started to learn a new game
called advanced tactics gold which is
for people who are old-timey turn-based
strategy war game players this is like
the ultimate game damn close to it it's
huge it has every aspect it's got sort
of rudimentary politics economics all
kinds of different military units that
you can mix and match they did a just a
very rough calculation and decided that
it had a branching rate per turn on the
order of ten to the sixtieth compare
that to checkers which is three or four
chests maybe thirty and go maybe to
fifty advanced tactics gold was ten to
the sixtieth so way higher search space
than these other games and sure enough
when I played it the first time it
kicked my butt really badly I mean it
was humiliating the next three times it
beat me easily then we had two games
that were pretty good struggles and then
game seven I beat it I've never lost
again I probably played it a hundred
times and so mmm how could that be how
could I learn so quickly I went back and
looked at what was my knowledge base of
having been a war game player for fifty
five years and I estimated that I'd
perhaps played 2500 games to completion
across maybe 200 titles that's not a lot
2500 games I'd also perhaps read 200
books on military history and strategy
so what seems like you know not a whole
lot of input you know 200 books is just
a few seconds a download today on a fast
computer and 2500 games isn't very many
but I was able to beat a
state-of-the-art
10 to the 60th branching rate game after
seven tries how did I do that that
question has been nagging at me ever
since and that's just a qualitatively
different approach than these brute
force deep learning approaches well I
mean I don't know anything about the
particulars of the system that you were
playing but I would say that human
beings have a lot of concepts like
notion of evolving an economy or notion
of say splitting forces or flanking the
opposing forces and that those concepts
are often very helpful or the what is it
the OODA loop you know orient I forget
the second node
enact observe decide and act in this
case it was since it's not real time
judo loops apply in real time games for
sure but not really in turn-based games
but certainly the other things
definitely do you know the turns out
that the basics of strategy and tactics
happen to work in this game and I had to
induced strategy and tactics from
playing 2500 games and reading 200 books
which is relatively a small amount of
input to have been able to produce those
models and principles and apply them in
a complex setting so I don't know the
specific game that you're talking about
but there are lots of general concepts
that humans have that you can start to
apply quickly whereas most of the
current best systems are fundamentally
just learning about contingencies they
don't have a lot of abstraction there so
if you're in a game where there's a
limited number of possibilities then it
can be a pretty broad limit you've got
enough data you can kind of graph out a
space and interpolate between the cases
that you've done if you're a machine if
you're a person you're using concepts
like I'm gonna divide the other person's
forces on a flank them in this way or
maybe use the OODA loop of Orient
observe the side act there are lots of
conceptual frameworks that you can bring
to a war game or to kind of any problem
in the world the narrower it is the
easier it is to use brute force the more
that your options percolate the more
that you need some grasp of what's
actually going on in order to perform
well yes in fact I refer to that my own
work as heuristics induction you know in
a game with branching of 10 to the 60th
per turn there's no way you can do
anything like deep learning
reinforcement learning to try to learn
that problem space you have to create
some heuristics and how do you create
the heuristics by some form of induction
I wish I knew how that was done by us
humans well you know the other thing
that you did that a machine current
machine cannot actually do is to read a
book so people forget that current AI
systems are essentially illiterate deep
mine showed off that they quote master
go without human knowledge which is not
actually correct there was some human
knowledge that we could talk about it
but they sort of made a virtue of the
that they don't actually know how to put
in a book about go into their systems or
a book about military strategy and go is
very well documented the particular war
game that you're playing maybe there
aren't any books about it but you read
books about other things and transfer
that knowledge about other things to
this new game so you can imagine for
example deep learning system learning
how to play a first-person shooter where
you attack zombies but it doesn't learn
enough about how first person shooters
work in general to be able to transfer
that to a first-person shooter where you
blow up Nazis the the system what it's
learning is very superficial very close
to the bone whereas what you're learning
is a human is about more abstract things
like in a game that is a first-person
shooter where am I likely to find
enemies how am i likely to hide how do I
change you know weapons and so forth
you're learning abstract ideas that you
can then apply in a new environment that
doesn't mean that there isn't some very
specific knowledge that you learn you
know the corners of this map in this
game and that doesn't help you with that
game it has a different map but you
learn a lot of general things like how
to work with maps how to work with
inventories and so forth you can apply
from one to the next there is no current
AI system deep learning or otherwise
that can really do that can as you said
in induce the heuristics so you induce
heuristics about how do I manage an
inventory or how do I read a map or how
do I accumulate a map from partial
information or how do I duck behind an
obstacle you're trying to learn these
abstract skills that you can reuse and
what we lack right now is a way of
building reusable skills not saying it
can't be done some form of AI someday
we'll do it and that someday might be in
ten years or 20 years and 50 years it
will happen but the current technology
doesn't give us a way to accumulate
reusable skills so instead people learn
things so-called end to end they learn
the entire game from top to bottom and
nothing that they can carry with them to
the next game over and again that limits
you to games with relatively low
branching rates it does I believe at
least what I have read haven't actually
used reinforcement learning on a project
but I read fair amount about it came
away saying yeah it ought to work good
but only within
domains that have relatively low
branching rage well you know really
telling talk that I saw the other day
and was by Petera Biel we were both
speaking at the Rotman Institute in
Toronto and he's been one of the
pioneers in deep learning for robots and
he gave a very honest candid talk and he
said here's the fundamental issue we can
do all kind of crazy stuff in the lab
like was in his lab but the rubik's cube
stuff that people talked about recently
was an extension of work that he was
part of the team that developed I think
it was his students you know so you know
to remind you that rubik's cube the
system didn't actually learn how to
solve a cube but it did learn how to do
the manipulation to turn the faces of
the cube and that's very exciting but
you don't see stuff like that in the
real world so people build laboratory
demonstrations that rubik's cube had
like sensors inside and you know it's in
a well-lit room or whatever you build a
robot to go into the real world and
things just don't work that well so it's
okay maybe in a factory environment
where things are very very well
controlled but more open that ended the
world is which is related to your notion
of branching factor it's not identical
to but related to it the more trouble
robots have so you know the best-selling
domestic robot of all time is still
Roomba that doesn't manipulate objects
at all and nobody knows how to build
something that could do what my
housekeeper can do of like um tidying up
a room that might have any kind of stuff
in it or the famous Steve Wozniak test
you know I thought this was a very
clever alternative to the Turing test
which is the plop a robot down in a
random American kitchen and tell it to
make a cup of coffee good luck
nothing could do that sure currently
right not even close not even close not
even those cool robots from Boston
Dynamics right no I mean the Boston
Dynamics doesn't really specialize in
manipulation at all they have you know
one robot that does some manipulation
most of which has been teleported didn't
claim to fame for their the robots is
that like they can walk around and you
can kick them and they'll they'll remain
stable which is a kind of robustness
which is really important but they're
not robust and autonomous able to make
their own decisions let's say like I'm
looking in my room right now and there's
a fan that really should be put away the
summer time is over and so you know what
kind of robot is going to be able to
pick up this fan that this one
particular one is a battery-operated fan
with a long stalk and so you'd have to
go over to it decide which access is
convenient and then grip it and then
you'd realize that the bottom piece is
actually just a holder and it would drop
and you'd have to compensate for that be
no problem for my housekeeper if I said
could you move that and there's no
machine that knows how to do that it
just literally does not exist yet yeah
we're a 5 year old kid could do it right
no problem I have one so I know it for a
fact that she could do that with that
problem I remember being five in fact my
mother the wise woman that she was when
we were out of school I thought it would
be interesting to teach us how to do
housework right and so just like Tom
Sawyer at five years old you're
interested learning anything right so we
learned how to clean a house how to
clean a bathroom how to straighten up
all that stuff show me the robot that
can whitewash a fence I will be
impressed
exactly now we touched on this in
passing but I'm gonna hop back to it
which gets to my mind which damn close
to the meat of the matter of the next
big step I mean heuristic induction is a
really big one but another one is what
you call I think it what I call real
language understanding seems like
untruly understanding language is a
great bottleneck to further progress in
the kinds of general-purpose ai's that
we've been talking about in fact you
gave a nice example which was AI
programs that could automatically
synthesize the vast medical literature
would be a true revolution computers
that could read as well as PhD students
but with raw computational horsepower of
Google would revolutionize science too
but we're not even close to that hell we
don't even have convincing chatbots I
tried a few supposedly the top ones and
they're not even close they wouldn't
fool me for two minutes let alone 20
minutes what do you think the issues are
around real language understand what
real language understanding is what we
might see there in the future
so we've been trying to cone a phrase of
deep understanding to distinguish
between deep learning so you could say
that there's a shallow understanding in
systems now right so Alexa can
understand particular requests or you
could feed into this GPT cue system and
if you're talking about I don't know
baseball it'll come back with baseball
related terms or if you talk to it about
chairs it'll come back with terms that
are chairs so you could say there is a
superfish
understanding but there's not a deep
understanding there's no way that the
system understands that chairs are for
sitting on or if you took away one of
the legs the chair might fall over and
so it wouldn't be as good a chair to sit
on so they're a couple things that are
missing one we already talked about
these cumulative models of the world
that's what language is about is I build
in your head a model of something and
then you kind of check your model
against mine or you elaborate it that
whole process is missing and then common
sense is missing so one of the ways that
we can do this that I can tell you could
you move that chair over there is
because you know what a chair does and
what it's for so you know a reasonable
orientation for that chair is with the
flat surface up so that somebody could
sit on it and that's spinning it upside
down so that they would have to sit
between the feet of the chair is not a
reasonable thing because that doesn't
accommodate the way that people's butts
work and so you know something about how
the world works and you integrate that
with a model of the things that we're
talking about and that's what language
is about and that none of that's really
there yet so I'm gonna hop ahead on my
questions list because you opened up
something that's interesting which is
common sense or knowledge engineering as
we used to call it a good old-fashioned
AI or knowledge engineers or the closest
approximation we had graduate students
would create representations of
knowledge or common sense within send
domain and then we've had some larger
scale projects like psych and concept
net and yet none of that seems to have
really gotten the kind of traction that
we need you know any thoughts on why not
and is there something like the kind of
brute force way to extract common sense
that deep learning has been able to
extract statistical regularities well
there's a whole lot of different
questions there so I may lose some but
I'll start with psych psych is the to my
knowledge the biggest knowledge
engineering project of all time although
there might be some medical taxonomy is
that that are in some way similar in
scope and what Doug Leonard tried to do
and is in fact still trying to do is to
codify much of the world's knowledge in
a machine interpretable form I think
that the specifics of how he did it
maybe aren't quite what we would do
today so he did it very much informal
logical terms and that may or may not be
flexible enough so some things that
people would emphasize now would be
uncertainty probability distributions so
when I know about chairs I don't just
know some formal things about them but I
know kind of the distribution of chairs
that I've seen before and what their
properties are that kind of stuff I
believe is not represented in there I
think as a system one of the issues that
it has is there's actually not a lot of
natural language interface it's mostly
the formal logic part and it's possible
that taking lenox reasoning system in
conjunction with a good natural language
interface might actually be very useful
because that doesn't exist I think it
limits the utility of what he's got
generally it's regarded as a failure I'm
not sure that's actually the right
interpretation it might be that he
solved the piece of the problem but
since he hasn't solved the whole problem
it's not very commercial and then people
infer falsely that because he hasn't
solved the whole problem been a
commercial success that there's no value
there and that may not be correct as I'm
implying I would do it differently
nowadays I would have a lot more
representation of uncertainty and he's
got pretty good arguments that you do
want the power of a formal logic and not
just a first-order logic you want to be
able to represent things like I believe
that you think that such-and-such is
going on and that requires fairly
sophisticated logic there is no formal
argument out there that he's wrong
there's a lot of people who are allergic
to what he did but none of them so far
as I know have a serious alternative
maybe the closest alternative is some of
the work that agent joy has been doing
recently at University Washington and
the Allen Institute for AI and I think
it's very interesting but it's not as
stable and robust in drawing an
inference as Leonard stuff and you know
there's probably room for both and
ultimately I think you want to do some
learning stuff like joy is trying to do
and you want to have the formal logical
reasoning abilities that Leonard
emphasized thing you need to bring this
together in some fashion interesting
I've also wondered about psych you know
it's been out there for I don't 20-30
years and I've sort of seen it out the
corner of my eye but I've never done a
deep dive into it the best currently
public available thing is Len it has a
piece in Forbes I believe on making
inferences
Romeo and Juliet that just came out
earlier this year and he actually walked
me through some of it and when it works
it's really impressive how much it can
infer about like why certain characters
took certain actions
what's unimpressive about it is there's
a lot of it's a sort of hand written
about Romeo and Juliet so it doesn't
really teach you how the knowledge would
be acquired there isn't really a natural
language front-end to it but once that
knowledge is represented there can make
much more sophisticated inferences than
anything else that's out there and so
you know maybe it's been miscast at
least in people's minds you know what
it's really good at I think is making
logical inferences about the nature of
people and actions and their interaction
with one another nobody else really has
that right now it's not an end-to-end
system and what people are trying to
build now our systems that kind of go
from pixels to action and if we could
get those the work that would be great
but the reality is they're incredibly
fragile that we give in the book the
example of the deep mind Atari game
system reinforcement learning system or
deep reinforcement learning system they
plays Atari games and you read the
original paper and it says it learns to
break through the tunnel and have the
you know the paddle hits the ball
through the wall and it does that
ricocheting thing but that's that's this
illusion of progress and gullibility gap
just because you see the machine and
think that that's what it's doing
doesn't mean they do it that's what the
machines actually doing if you move the
paddle up a few pixels as vicarious did
in a really cool paper then you see the
system doesn't actually understand
anything at all it doesn't really
understand that there's a ricochet
technique when the paddle has moved a
few pixels from all these memorized
positions the system is just makes
mistakes left and right well that's how
you know reinforcement learning works
right it just basically takes one step
after the other and then it figures out
at the end was there a payoff and then
it up boats all those links that led to
a payoff at the end so that's what you'd
expect and if you have enough data
relative of let's say a particular level
in a video game it can play that game
better than a human but that doesn't
mean they can play the next level and
you could think of this demo was kind of
another level okay we will you know in
level two the battle will be in a
different place and it's it's at a loss
it has to start over to learn that new
level it doesn't have emphasizing
something I said earlier transferable
knowledge about what a wall is or you
know kind
physics of that game it just has this
narrow knowledge about contingencies
within this level if you play enough
times you can get contingencies for
anything but the question is what do you
do when the world changes yeah well
enough you can do that if the world is
small enough that's right and it is you
know you've made this point and
different different way talking about in
branching factors and so forth and we've
been talking about narrowness if the
world is potentially complex enough that
suddenly is not the right tool for you
to use and so there's been a lot of work
on that line and maybe something will
come out of it there hasn't been a lot
of commercial applications so far and
whether or not people come up with a
cool commercial application for it it's
really not the solution for AI in the
open-ended world it's just not well
suited to that let's finish off on the
psych concept net area I think I asked a
kind of ill-formed question but I'm
gonna try to form it better do you have
any thoughts on how a next step in AI
research might be able to extract
common-sense knowledge from the real
world in something like a symbolic form
at least analogously to the way deep
learning extracts the statistical
patterns from example sets I think is
probably possible I think that the roots
of that are in things like inductive
logic programming but that we need
richer an eight basis in these systems
so they have prior knowledge about how
objects work and people work not
everything but some like knowing that
objects exist that they persist in time
and so forth in order to kind of have a
scaffolding such that when these
incoming facts arrive that the system
can do something with it what people
have tended to do has no prior knowledge
and doesn't work very well so there's
for example Tom Mitchell's Nobel but I
think unsuccessful attempt called nail
never-ending language learner which I
guess is still running at Carnegie
Mellon and it tries to extract triples
from the world and sometimes it comes up
with things like you know Barack Obama
is the President of the United States
and they're okay they can be outdated
sometimes the the facts are really
really poor because the system doesn't
understand about entity disambiguation
let's say so it comes up with facts like
Barry as a painter and you're like well
which Barry it doesn't really tell me
what kind of painter using our artist
does he do houses and and so the a lot
of the knowledge is kind of under
differentiated in a way that makes it
useless and like the spirit of what I
think Mitchell was trying to do is great
in the age enjoy again is doing some
stuff in in this vein I don't think we
have the tools yet I don't think it's
impossible and I think it would be a
great area to try to make some advances
in yeah it seems to me it's fairly close
to the you know again like well along
with language understanding the core of
getting to the next level just thinking
out loud here
suppose one were to think through a
project somewhat analogous to psyche but
with everything we know today about
what's happened over the last 30 years
and the knowledge we have about
cognitive science at cognitive
neuroscience you think it could be
considering the stakes involved in
getting to high-level AI relatively
quickly what could it make sense to do a
neo psyche using everything we've known
now to start over completely from
scratch to build this grounding basis
for AI I think be a huge deal I think
it's a five hundred million dollar
project and nobody has the appetite for
it right now I wrote down a billion and
I said a billion who cares you know AG
is worth trillions right I think it'd be
well worth doing and you know after my
robots a company becomes a huge success
or maybe as a spin out of it or
something like that maybe I'll take that
on well I guess I think about a billion
your billions what the Europeans wasted
on their brain initiative right with the
original budget was a billion euros yeah
and as far as I can tell not helped lot
came out of that you know think of the
Obama stimulus it was seven hundred
billion right take a billion of that put
it in Neos psyche is it's also less than
the investments that say Facebook or
Google / alphabet making AI each here
but it's not to their taste yep that's
contrary to the religion at least that
Google I mean you know Facebook is
perhaps a little less monomaniacal but
still mostly dominated in the deep
learning approach let's hop back a
little bit
again we hopped around little bit here
but that's part of the fun of doing a
podcast make it somewhat nonlinear we
were talking about language it's
that really hopped out and from the book
which resonated with me is that you said
human thought and language or
compositional you know as our by the way
most good techniques for doing computer
programming could you tell our audience
what you mean by compositional and
without why that's relevant so I'll
actually take a computer programming
first so the way that we build complex
computer systems is we build small
modules and then we but larger modules
out of those smaller modules you make
sure that the small pieces work and then
you make bigger pieces outside of them
so that's a form of compositionality
sentences are the same way so we have
small pieces like nouns and verbs and we
can make more complex pieces like noun
phrases and verb phrases and then we
make sentences out of those and we can
make sentences sub sentences of other
sentences so I can say I like my new
iPhone and then I can say you know that
I like my new iPhone and you can say
your friend thinks that you know that I
like my new iPhone and so forth so you
can put together little pieces inside a
bigger pieces that's what
compositionality is about for the most
part the current approaches don't really
do that they look at all the words in a
sentence before and after and try to
find other sentences that have similar
vocabulary in them and similar as a kind
of complex notion in these systems but
they don't really look at structure in
that way so they don't come back to you
and say that you know this clause is
about somebody's intention and this
other clause is about their perspective
on that intention and we're gonna put it
together in order to make this statement
about the world and we're gonna derive
this cumulative description of what's
going on that just isn't really part of
the current workflow and they I there
has been work on that kind of thing
historically but right now because deep
learning is making better short term
results people are focusing there and I
think losing sight of compositionality
as really the the core thing we're
trying to come up with and make a
distinction for the audience there's
storica Leben divided between symbolic
AI and sub symbolic AI which deep
learning and neural nets are the primary
example and historically symbolic AI has
been compositional or at least in theory
could be while mostly sub symbolic has
not been compositional the only thing I
would add to that is that the
distinction itself is confused so
every neural network that I know has for
example output notes that are in fact
symbols and so nobody's been ever that
clear to me about what sub symbols
really mean but it is true that they try
to make do without certain kinds of
representations like operations over
variables and structured representations
at any programming language from
assembly language to Python takes for
granted and I think that in a way
they're kind of tying their hands behind
their back taking away one of the most
valuable discoveries in the history of
humanity which is how to write computer
programs compositionally and that was
relatively recent he'll I'm old enough
that I remember when people wrote these
horrifying in fact I did something
myself he's horrifying 1,500 line
Fortran programs that were just ugly
bags of spaghetti right yeah I just
write in basic with all these go-to
lines then it was completely not modular
it was a mess it was hard to debug and
it was hard for someone else to
understand your code if they looked at
it later because it was not sufficiently
modular it's not even those programs
though you have to say we're
compositional in the sense that even a
line of basic code if a equals B is
still a symbolic system it wasn't a very
well structured symbolic system so you
know there's symbols at the level of
your basic operations and even basic or
Fortran or whatever is completely
symbolic in that sense and deep learning
systems for the most part don't allow
that and you have people like geoff
hinton trying to say don't go there
don't look at that kind of stuff it's
evil it's old-fashioned don't touch it
but you know all of the world's computer
programs essentially are built on this
kind of stuff and then the more
sophisticated stuff anything that any
you know reputable software engineer
would right now is compositional a very
high level so you know you build modules
on top of modules and you have
inheritance and all these kinds of stuff
that it's very much from the symbol
manipulating tradition
and you're gonna talk about it before
you do that though you would talk about
how human language is composition let me
do a little sidebar on something I've
stumbled across recently I'm not sure I
understood it well enough to say this
accurately so lay it out - you get your
comments and that's some of the newer
forms of neural networks particularly
graph neural nets and they're closely
related message-passing neural Nets seem
to have some aspects of compositionality
to them or at least they act upon
compositional components they're better
so the graph networks have a graph
structure which is from straight symbol
manipulation and I think that that's a
prerequisite essentially it's saying
that knowledge is represented as things
that look like a network or a tree or a
graph technical terms and that allows
you for example to formally specify the
relationships between things you can say
the molle is the father of Gary or
something like that or my mother if Gary
excuse me so you can you can make
specific claims about relations as
opposed to a lot of neural networks
that's just like character by character
you feed in a sentence and so I think
that's a step in the right direction I
think there's still too little in
general formal reasoning over those
systems not a great way of representing
abstractions as opposed to specific
facts but I think it's a step in the
right direction
okay let's talk back to human language
and it's a composite compositionality
there we go
well so I mean I already gave you one
example of compositionality you know the
sentence about you know I like an iPhone
so an iPhone is a noun phrase iPhone is
a noun we're putting those together to
pick out particular iPhone and then
we're using like as a verb and we make a
verb phrase like an iPhone that we have
a subject to it I like an iPhone and
then we can make that whole Clause part
of another sentence so I can say you
know that I like an iPhone and so you
know you as a noun know as a verb and
then we have this whole clause that
represents an idea a proposition is as
some people would call it of I like an
iPhone and so we have a whole set of
verbs that are what we call
propositional attitude so you know that
I like an iPhone or you deny that I like
an iPhone or you doubt that I like an
iPhone
and you have all of these different
possibilities compositionality it is
about the whole being computed from
parts so once I know how verbs like that
work and I know how nouns like iPhone
work and I can put together the whole
sentence you know once I know all of
those pieces and I can derive an
interpretation of something in terms of
how its pieces work which is actually
exactly how you know computer compilers
and computer interpreters work is they
make a composition of the components
that tells them what to do I like the
phrase was it the whole being computed
from the parts that's right
yeah I love that that should be people's
number one takeaway on what we're
talking about here
let's pop up a level we talked about a
lot of details in and out and around of
deep learning and kind of alternative
approaches more broadly what are some
other approaches to AI that may not be
getting enough attention in your opinion
well I think that all the action right
now is actually on hybrid models and
some people are actually building them
not enough people are talking about why
they're important as opposed to deep
learning so I again worry that Geoff
Hinton has too much influence on the
field he's been going around you know
he's the Godfather of deep learning
going around saying that we shouldn't
build hybrid models that we should just
use deep learning with some simple
manipulation is old-fashioned but if you
look at what people actually do when
they want to get something done they
actually do build hybrid models so
alphago is an example of this he uses
deep learning to recognize patterns and
then it uses tree search Monte Carlo
tree search in particular which is
straight symbolic operation to actually
search the space of possibilities I go
there you go there let's look at a bunch
of different possibilities and add it up
that's a symbolic computer program of
the classic sort they combine that with
deep learning so that's a hybrid model
Josh Tenenbaum our mutual friend works
on hybrid models he's got one on vision
recently with a bunch of people on IBM
where you use deep learning to recognize
parts of images but then you do a lot of
reasoning about what those parts of
those images are and what they relate to
each other you use a lot of some or he
uses a lot of symbolic operations in
order to do that so that half of the
house so to speak
looks like computer programming and the
into the house if you will looks like
deep learning another example of this
Rubik's Cube thing that got so much
brass the part that actually does the
solving of the cube namely figuring out
where I should turn you know the blue
face should rotate 90 degrees that stuff
is actually done by a symbolic system
and then it's some of the mapping
between what you see in the physical
joint courses you should be applying
that are done by the deep learning
system so that's a hybrid system there
is just a new face book kind of web
search thing it's a hybrid system I
think that's where the real action is
going to be is in trying to figure out
theoretically sound and intricate
intimate ways of bringing together these
traditions it's not a binary proposition
you know Hinton presents it as if it's
either that new stuff or the old stuff
and what we really want is even newer
stuff that combines some of the deep
learning stuff or things like that also
probabilistic programming Bayesian
systems a whole lot of kind of
statistical techniques I won't say which
ones but some of those with the more
classical knowledge representation stuff
that's the other half of today's
conversation that's where the action is
right now I think it's just getting
started but that's where you know I
expect the winners to come from how
about things like self-driving cars I
would sort of expect them to have a
hybrid nature a lot of them do and I
mean that doesn't come out in the media
so what comes out in the media's if
there's a neural network involved
anywhere a system is called a neural
network system but you could do the
opposite you could say if it's a if
there are any symbols in the system it's
a symbolic system and that's silly
but there's a there's a huge bias and
how these things are reported
similarly the open AI when they talked
about the rubix system said a pair of
neural networks learns to do
such-and-such and you have to kind of
read the fine print to realize that's
not just the neural networks that are
doing the system here but there's also
this classic 20 year old symbolic
algorithm that's kind of at the core of
the so called solving part of it
so there's a lot of hype around neural
networks and people present their
systems that way even when there's other
stuff going on
yeah I'm funny I had a conversation no
six months ago with one of the smartest
people in the world I would say not in
the field of AI but in a field not too
far from a
he had somehow extracted the idea that
the only thing happening in AI these
days was deep learning I was fairly
shocked I had to disabuse them of that
notion it's a widespread
misunderstanding in part because the
hype machine for deep learning is so
powerful within various corporations
that I have a lot at stake and opening
eyes you know actually it no longer a
nonprofit or no longer fully a
non-profit so that fits under that
umbrella so Zack Lipton called a
weapons-grade PR so you have a lot of
weapons great PR behind neural networks
and it's a nice story to tell the media
about hey these are like brains and they
work great but they're not really that
much like brains and it turns out you
need other systems to kind of prop them
up and so forth but that's a more
complicated story and people aren't as
receptive to it
what about evolutionary approaches and
happens to be something dear to my heart
you know my deepest grounding at a
scientific field is evolutionary
computing and as early as 2001 I did
some work with evolutionary neural nets
and other evolutionary approaches to
building game playing agents what's
going on in evolutionary AI these days I
think it's a lot of potential but it
hasn't succeeded and my take on it is
because everybody's trying to
recapitulate the period between the
beginning of life and getting to I don't
know dog cognition and what they want is
not dog cognition they really want human
cognition but starting from pre
bacterium isn't going to get there so
you start with systems that are total
blank slates they don't have the
equivalent of a genetic basis of a
vertebrate brain plant for example you
know it took close to a billion years of
evolution to get a vertebrate brain plan
this is a very hard one evolutionary
struggle if I can anthropomorphize a
little bit and then things pick up and
pace from there but the AI systems that
people build are like the you know
bacteria or pre bacteria level so you
have a grad student work on it for a
year and at the end they don't have that
much to show for it because they've
basically started from zero you're the
most interesting thing in my view and
it's a little bit species chauvinist but
the most interesting thing that happened
in evolution in the last few hundred
million years is the evolution of people
because people have such a different
niche from other creatures and they have
these amazing means of cultural
transmission that you know they're a
little dim reflector
than other primates but there's really
something special have this language
thing but the whole thing didn't take
that long to evolve you know maximally
seven million years and maybe as few as
like a few hundred thousand well why did
it happen so fast is because the primate
brain plan on which the human brain plan
evolved was already incredibly
sophisticated so it already had color
vision it already had you know very
sophisticated obstacle avoidance very
sophisticated social cognition maybe not
as sophisticated as ours but still
pretty sophisticated and so if you
evolve from a primate brain plant you
get all kinds of interesting things that
happen if you if all from a bacterium
it's just gonna take a long time it's
not impossible but you know you need to
have a lot of kind of lucky selections
or it may be not lucky you know natural
selection you know very fit selection
you need a lot of them to get from point
A to point B which is why I took you
know so many hundreds of millions of
years actually took longer than you
think life's at least 3.5 billion years
old and the vertebra lines didn't really
get rollin till the Cambrian explosion a
mere 550 million years ago so the single
cell and the colonial species were
basically doing their thing for almost 3
billion years that's early and and most
of the work is recapitulating that 3
billion and most of the interest would
be in recapitulating the more recent 500
million yeah and that goes to your
earlier suggestion and hint that maybe
the answer is building you know these
structures from which to let ai's work
from these databases of what is known
about the worldwide well I have to learn
all that stuff the hard way right yeah
so you know I wrote a book about this
called the birth of the mind which was
about how a small number of genes builds
a complex brain and then part of the
basic takeaway was that let's say the
vertebrate brain plan is a very complex
library of self assembling subroutines
and you need the library in order to get
going the reason that people can build a
website and the day now is because you
have huge library stairs on lots of
subroutines and that's why we were able
to evolve or you know not we but natural
selection was able to evolve people
relatively quickly from a primate basis
are a lot of subroutines and libraries
in the genetic code that are really
useful whereas if you look at a bacteria
there's just
that much of the library there's some
stuff for metabolism but there's not a
lot of stuff there for cognition and so
if you're you're working with that II
you have to reinvent a lot I think
there's a good hint there on on ways to
move faster one last area I want to ask
you your thoughts about is the old field
of cognitive architectures you know
things like sore and akhtar etc is there
anything happening in those kinds of
fields that might be relevant to the
next big step I mean there's a lot of
people still working on it I think John
Laird is maybe one of the biggest he's
at Michigan and his stuff is worth
looking at I think that the general
issue is first of all those things were
mostly built originally for cognitive
psychology rather than AI in a
discussion right now is mostly around AI
another issue in psychology is they
might be true insofar as they go but in
a way they go too far so they're
compatible with a lot of different
things I think that's okay but people
are expecting that if I choose soar than
automatically or akhtar whatever that
that gives me the answer to cognition
whereas it's kind of like a notation
that could allow you to build a model of
cognition rather than the full model of
cognition in terms of AI they're not as
far as I know all that practical right
now I think that the intuitions behind
what those people are trying to do are
good though and I think that there are
lessons to be learned from looking at
how they go about problems like how do
you train a person to fly an airplane or
learn algebra what are the steps that a
person goes through we don't necessarily
want our AI to recapitulate those but
knowing something about what people do
in the process of learning these higher
order skills might be very helpful
towards AI even if you know there's not
like off-the-shelf
library code that you know you want to
plug into your natural language
understanding system so maybe to put
words in your mouth there's may not be
anything right there today with the
existing models but it's probably useful
for people to know about them as we
think about the future I think it's a
hundred percent useful to know about
them and it goes back to something else
that we've been talking about which is
this kind of intellectual narrowness of
just knowing you know the math of
gradient descent versus understanding
the broader set of problem
that people have tried to approach in
cognition in order to recognize
limitations and recognize avenues of
attack and so forth and really rebooting
AI is a cognitive scientists or a pair
of cognitive scientists perspective on
what you might do in AI to move forward
beyond the statistical techniques that
have been well mastered but aren't
sufficient yeah we'll come back to that
at the very end but I'm gonna now hop to
something else you talked about which
are some of the dangers from the data
mining approach data mining is not I
think at least might be your words I'm
look at my notes your data mining I've
not done with great care and
thoughtfulness can rebuild obsolete
social biases you give an example using
Google Image Search when we search for
professor only about 10 percent of the
top-ranked images were women perhaps
reflecting Hollywood's portrayal of
college life but out of touch with
current reality much closer to 50
percent of professors are women it's a
really endemic problem and people think
it's easy to solve and it's not so that
you're the famous version was that some
African Americans were labeled by Google
as gorillas that was in 2015 and Google
got terrible press out of it and they
quickly solved the problem so it won't
happen anymore but there's just more and
more versions of that problem there's no
currently even I think possibility of
systematically solving it so there's
lots and lots of variations on that and
your listeners can go home and try
things like uncle and niece for example
and you'll probably find that they're
mostly white for example even though you
know why people are I guess a minority
of the world's population but they're
better represented in the data set and
the system doesn't know the difference
between what's represented in the data
system and what's out there in the world
and so it doesn't capture it and every
time one of these things is fixed and
there was you know cases like this in
2012 they were fixed into 13 or whatever
2015 1719 so for the problems just keep
popping up but it's like whack-a-mole
and people put band-aids if I'm mixing
my metaphors sorry people put band-aids
on them they fix one of these problems
but there are hundreds or thousands of
potential instantiations nobody notices
them and they just keep happening and
it's exactly about perpetuating existing
statistics rather than understanding
what's going on
if you think about what a good
economists like Steven Levitt of
Freakonomics Fame does is he takes a
bunch of bad data from a bunch of bad
studies figures out how to decouple them
Deacon found them in order to drive a
sensible conclusion and we need
ultimately for our AI to do that so gets
a bunch of bad samples it needs to
understand what's at stake what's the
history here and compensate for prior
history and so forth and come up with
something that's in line with our values
or objectives or whatever and if you
don't understand our values and what
we're trying to get at it won't work so
if you took as your data set this will
be hypothetical but you took the
proportion of musicians that were ballet
dancers of Orchestra quality or
Orchestra caliber musicians that were
ballet dancers in 1910 and you know you
discover it's zero so you put in your
system that the weight you know that you
should actually penalize people for
being ballet dancers because no ballet
dancers are Orchestra level musicians in
1900 so your system builds in this bias
and then you discover later humans
discovered later that that's partly
because there was bias on the part of
the people who were choosing the
musicians they didn't want to have women
in there they thought women weren't
qualified so then we moved to NEET Blind
Auditions and suddenly there are lots of
women in orchestras but you have this
historical data and you have some data
dredging machine that doesn't understand
the difference between the historical
data and the recent data it doesn't
understand that Blind Auditions change
things and it just puts it all together
gloms all the data and in that way it
perpetuates the bias very interestingly
in that long discussion about data
mining you use the word understand about
12 times which tells us that if we want
our ai's to solve these problems for us
they have to understand which they don't
today
yeah the defense of deep learning people
is to say it's hard to define
understanding which would be like saying
that since we can't quite define
pornography and you know the famous
quote that I'm alluding to that we
shouldn't have any policies about it and
that's silly so it is hard to define
understanding we tried to give it by
example primarily we gave lots of
examples like understanding is being
able to read a children's story make
inferences about who did what to whom
where when and why
you know understandings about being able
to analyze those journalist questions
when you're confronted with a narrative
or an article or things like that I just
had a odd thought perhaps the rejection
of the concept of understanding from
perspective of the neural net folks is
at least analogous to the rejection of
internal process by the behaviorists
it's very very similar there's a similar
instinct behind them so you know both
the behaviorists and their latter-day
reincarnates the the neural network
people want to derive everything from
data they don't want to talk a lot about
mental representation behaviors didn't
want to talk about it at all and neural
network people mostly don't so they
actually come from a very similar part
of intellectual space that both trying
to define everything mathematically and
kind of missing the importance of
knowledge representation and neatness
and so forth that makes sense to me
final a bit on this little section I
love this point that you made which I
had never thought of but it's gonna
become more and more true the heavy
dependence of contemporary AI on
training sets can also lead to a
pernicious echo chamber effect in which
a system ends up being trained on data
that it generated itself earlier I have
noticed that if you use Google to look
for odd things which I do all day every
day more and more of search spam is
obviously generated by rather stupid
ai's right and something mining the web
is gonna be mining this AI generated
garbage and over time that's going to
get better and better and more and more
the content on the net is gonna be AI
generated and then if it's processing
its own as you say its own generated
data or at least it's by relatives of
itself other AIS using similar toolkits
I don't know what a bad effect could
come from that but it seems like that's
not a good idea
sucking your own fumes never a good idea
exactly but yeah that's just actually
true that has to be happening right now
or because it has particular examples I
think we gave unless this was in a
article that we wrote and not in the
book it was about translation and so you
know some translations are done by
Google Translate particularly language
is where not a lot of people are writing
in Wikipedia and then that stuff gets
fed back
to google translate yeah it crystallizes
its own mistakes some of the time I
would imagine that's right right you did
have that example in the book all right
I'm gonna head for the home stretch here
and talk a little bit about artificial
general intelligence when I look at the
page for your company robust AI it said
help us make robot smart collaborative
robust safe flexible and genuinely
autonomous it sounds an awful lot like a
GI to me yeah I mean the if you had a GI
you in a bottle it would do all of that
if you don't have a GI in a bottle and
nobody does then you can try to take
steps towards it
we're not promising that we're gonna
deliver a GI anytime soon of course we'd
like to position ourselves to you know
be able to help in that discovery I
think each of those criteria are steps
towards a GI or you know our metrics
that could measure progress towards a GI
we're trying to find commercial cases
that are steps along the way you know we
can't promise that we're gonna solve all
of it in a day but if we can make robots
that are noticeably more flexible than
the robots that we have now and
noticeably more reliable then that's
obviously gonna really expand the scope
of application of robotics and right now
robots have to be in you know in cages
or they have to be in very carefully
defined environments you have situations
like the part of the big problem with
the Tesla Model 3 is that musk
overestimated how easy it would be to
get robots to work in production in
complex assembly lines so you know when
things aren't exactly the way that they
were in your blueprint and you have
different kinds of objects and different
kind of places and so forth current
systems just aren't that good at it to
really get full service version of Rosie
the robot might take us you know closer
wrote to hei and maybe there is some
intermediate point that's not Rosie the
robot but that's at least you know safe
in a domestic environment and a lot
better than then you know what we've got
now similarly like you can think about
package delivery right now people are
building all of these kind of 4-wheel
cargo carriers that bring something to
your street but they don't bring it up
the stairs
right and so that's not really what
people want and that's not why they've
been paying FedEx for you know the last
30 years to you know have to go out to
the street to collect the package when
the delivery guys there they want FedEx
to bring it to the door and building a
robot to do that wouldn't I think be
fully AGI but it would require a lot of
reliability a lot of autonomy a lot of
flexibility of the sorts that we're
talking about yep I having formerly been
a guy in the domain name business at one
brief period my career I always
registered domain names I think are
indicative of the future one of the ones
I grabbed about a year and a half ago is
proto AGI and the things you're talking
about there strike me exactly as proto
AGI and I think that's gonna be a very
hot area for companies in the coming few
years certainly hope so
hahaha I think you're right well being
right place let's bump it up a little
bit you have to have thought about AGI
what are your thoughts about time frame
they're really getting days yeah it's
hard to know for sure I mean I always
think about like in the you know early
90s nobody had any idea how big the
internet would be and they had no idea
that you know social networking would
change national politics and all kinds
of stuff so you know it's hard to really
predict the future the example we gave
in the book is in Blade Runner
they have androids that are you know
fully able to blend in with human beings
and at one point they stopped at a
payphone and this is sort of
astonishingly anachronistic because you
know cell phones were basically became
widespread in the year 2000 and these
robots are ways off so predicting is
very hard but we can say for sure I
think is no commercial system is
remotely close to natural language
understanding which is certainly a
prerequisite of AGI no commercial system
is remotely close to the kind of
flexible dynamic reasoning that people
can do no current commercial system is
close to being able to transfer between
different tasks in the way that you were
from one war game to another these
things don't exist yet they may exist in
somebody's lap but they're not widely
known and so it's gonna be a while you
know that much we can be sure it's not
happening next week it's probably not
happening in five to ten years it might
happen in 20 it's really hard to project
and it might happen in 100
because the problems are really really
hard nobody has a bead on how to develop
enough understanding of the world in
machine interpretable form right now for
example there are a lot of problems that
have to be solved so you know my guess
is somewhere between twenty and a
hundred years maybe closer to fifty and
it's a wide confidence interval people
always want you know a number like
reycarts while loves to say like a two
thousand thirty one anybody that gives
you a precise number is bullshitting you
it has to be a confidence interval
meaning you know within these boundaries
with some likelihood we can't do that
the way I satirize raise estimate as I
say yeah February 26th at 11:30 p.m. in
2042 we will achieve AGI right on West
5th Street in Dallas right I mean that
hyper precision makes it compelling to
many people but it's actually a clear
sign that it's yeah it's all
about the error bars right now I'm
worried about with you it's somewhere
between 15 and a hundred and if I had to
put down a small bed I'd say forty to
fifty something like that but who the
hell knows good happened tomorrow well
no it couldn't happen tomorrow that's
the one thing that you really can't do
it's just not happening tomorrow five
years this is probably the shortest
possible period but I shouldn't say
short as possible anyway let's not
speculate any further we know it's
uncertain not tomorrow but probably
within the lifetime of people who are
already alive which is interesting which
brings us to the topic that we have to
talk about at least a little bit even
though I'm bored to tears with it is AGI
safety what are your thoughts about that
I mean the line we have in the book is
is don't worry about killer robots at
least any time soon first of all they've
never shown any interest in our affairs
or impacting us so worried about bad
actors misusing AI but don't worry about
the robots rising up and second if they
do come an attack lock your door and if
that doesn't work climb a tree because
robots don't happen doors they don't
know how to climb trees they're really
not that bright right now so we don't
have anything in the near-term to worry
about alrighty I think we will wrap it
up unfortunately we had lost some time
with technical difficulties had about
three or four more questions but that's
alright this has been extraordinarily
interesting extraordinarily useful
and I would strongly encourage people
that are interested in questions like
this to go out and Gary's book dari
handed a co-author who was your
co-author let's give them some credit to
my co-author is Ernie Davis he's a
computer scientist at NYU and 95% of the
really cool examples of the book are his
alright so Ernie and Gary's book
rebooting AI if you find this at all
interesting go read it thank you very
much Gary for a wonderful conversation
thank you very much that's great
production services and audio editing by
jarred Jane's consulting music by Tom
Muller at modern space music.com
[Music]