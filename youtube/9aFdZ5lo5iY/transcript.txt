welcome to untangling the world knot of
consciousness
wrestling with the hard problems of mind
and meaning in the modern scientific age
my name is john verviki i'm a cognitive
psychologist and cognitive scientist at
the university of toronto in canada
throughout the entire series i will be
joined in dialogue by my good friend and
colleague
greg enriquez from james madison
university in the united states
throughout we are going to wrestle with
the hard problems of how we can give an
account
of a phenomenal like consciousness
within
the scientific worldview how we can
wrestle with that problem in conjunction
with the problem that greg calls the
problem of psychology
that is pervasive throughout psychology
which is that psychology has no unified
descriptive metaphysics
by which it talks about mind and or
behavior
throughout this we will be talking about
some of the most important philosophical
cognitive scientific and neuroscientific
accounts of consciousness
so i hope you'll join us through i'm
really excited about it i've been
embodying my modeling of various things
throughout
in the last couple of weeks so you know
i've been i've been feeling it
i'm looking forward to the continuation
so uh last time we did a very long
uh analysis one of the few times i use
slides i don't usually like to use them
but we did an analysis of an argument
that i've been working with dan shapi
about the use of the rovers on mars and
that was a
basically a use that is a test case
and as a vehicle for explicating uh the
relationship
uh uh sorry first of all the nature of
uh perspectival and participatory
knowing and then the relationship
between them
we bring we brought in montague's idea
of mutual modeling and consciousness as
some
mutual modeling between participatory
and perspectival knowing
and so that's sort of where we're at
right now and what i'd like to do now is
take
the next step forward and set up a
course of argument
that will allow us to together you and i
to enter into dialogue not only with
each other
but with some of the most prominent
theories of consciousness on the market
right now
on the psychological cognitive
scientific and neuroscientific market
if i can put it that way so i have a
proposal about how to do this and what i
want to do is i want to propose a
particular okay
let me just pop in real fast so so as a
metatheorist okay so
uh me you know i'm really looking to see
how various frames assimilate and
integrate key insights from various
models
yeah and one of the things i'm so
attracted about this model
is that it seems to precisely do this
okay so we're setting up from my vantage
point to my ear in part
is listen to the assimilation and
integration capacity
and how it pulls the strengths of
various models together and
generates coherence that's what i'm
often paying attention to and this is
what i
love so much about your formulation
thank you greg that was very helpful
that was very helpful
especially for the listener so thank you
for that yeah
um that's that's exactly right um that's
that's in a sense that's what i will be
endeavoring to demonstrate as much as
possible
so i have a particular proposal about a
particular strategy i want to you
i want to try to give a cognitive
scientific argument precisely because it
does the thing that greg just said
cognitive sciences
at least i argue for a version of
cognitive science in which the
this kind of meta theoretical
integration is central to the practice
of cognitive
science because we're trying to bridge
between work in artificial intelligence
neuroscience psychology and philosophy
especially phenomenological philosophy
about the nature and function of
consciousness so
one of the strategies we use in
cognitive science that is different
from uh typically strategies that are
used in psychology
or in neuroscience um
and it overlaps with strategies used in
artificial intelligence it's called the
design stance um and so the design
sense of this idea you try to figure out
what a phenomenon is
by reverse engineering you try to figure
out well if i had to build a machine
until recently this was not a strategy
people could generally use
right but now we're getting to the place
where this is now becoming
a plausibly viable strategy so what what
i wanna i wanna get on this you know i
wanna understand let's say intelligence
general intelligence so and this is a
project it's on right now
my project is artificial general
intelligence i try to make
a machine that has
that i try to design it and by doing
that
i often have to wrestle with some of the
deepest and harder
hardest problems of figuring out what
the phenomena is
and it gives me a new way of trying to
understand it
that can be valuable to people in
psychology who perhaps might be taking a
more experimental approach
or neuroscience who are taking you know
a more direct measurement approach of
fmri and eeg etc and that
so that's what i'm going to try and
offer here because i think uh that will
allow us to powerfully bridge
so yeah no i just it's a to me this is
again this is why we think so much alike
i mean i basically reverse engineering
propositional knowing with the
justification systems theory
basically exactly and and that again
this is this big picture
coherence capacity is when you reverse
engineer and all of a sudden you have a
design feature and you get a
lock key kind of relation potential and
then you see things from different
perspectives it deepens
raises questions and also i think when
it works
gets that lock key correspondence yeah
you know so yeah anyway
no no that's that's well said that's
very well said
so i'm going to make i'm going to make
two what i think are plausible
presumptions in order to try and
take the design stance i'm going to take
it that
um that agi is a
progressive progressively successful
project um
i'm not saying we've made autonomous ai
or anything ridiculous like that
but i think a very reasonable conclusion
to draw from the last
15 years is that this project is
progressing
and it's not even progressing in a
linear fashion it's progressing in a
non-linear fashion
and and then there's a philosophical
justification for that it's
basically the argument of something like
this if
we can't succeed in um coming up with
intelligence out of the out of the
physical
then i think we're doomed in trying to
get the conscious
out of the physical so what i'm saying
is it's plausible that this project
is progressing agi and if it doesn't
succeed
i think we're really doomed in trying to
give an account of consciousness
that can fit into the scientific over
oral view so those are my two
methodological presuppositions
and then what i want to do is i want to
say okay let's say we wanted to
build a machine that had api and so
here's
here's another part of what i'm arguing
for
if we take the design stance we will be
always if this is what the design stance
does
it makes you always move between and you
you alluded to this
always move between and trying to
integrate the function issue in the
nature issue
and when i'm trying to design something
i'm obviously trying to make it function
but if i'm doing the genuine design
stance i'm also trying to bring it into
existence
so the design stance is linking the
design function
thing together and by trying to design
artificial
general intelligence we're we're going
to try and
explicate and justify the intuition
that there's deep connections the deep
intuition that the deep connections
between
intelligence and consciousness yes okay
early
yep that's sort of the methodological
uh just the sort of the justification of
my method
so it's a methodological justification
so what i want to start with
is the idea that what's at the core
of intelligence is a capacity
uh for relevance realization and we've
we've
talked about this a lot i've argued this
in many places
elsewhere you've and you published on it
you've published particularly like
so there's good i'm not going to
recapitulate that argument it's out
there there's a lot of material on that
and we've also touched on it and
discussed it
also at length here so i'm gonna i'm
just gonna assume
that that's one huge component of what
is needed
in order to be in intelligent systems
and when we say relevance realization i
i sometimes want to put in a third r
make it rr because we're actually
talking about a massively recursive
because we've been talking
about that yep also throughout this
whole argument that's right massively
recursive
and then that's that's going to relate
to things like the uh
dynamic interrelation of perspectival
participatory modeling
yeah cross yes yeah exactly
exactly well said it's really good to be
doing this with you man it's just
it's really good um like just that like
you do those little
things and they just they're they're
they're they're wonderful they're
they're seamless but they also tie
things together so
i just want to compliment you on that
because thank you friend appreciate that
great it's fun fun to be here okay so
what uh what i want to do now is uh make
another step and this is there's a sense
in which
i don't need to go into this in great
detail for the argument you and i are
going to explore today
but i do want to indicate to people that
the the deep version of this argument
is forthcoming and i want to i'm working
on this argument right now with a former
student of mine and he's also a student
of andy cox
mark miller and mark miller and then
another person i just met because he
reached out to me
uh because of the awakening from the
meeting crisis area brett anderson and
what
we are we're really working on very hard
is uh the idea that we could integrate
relevance realization theory with the
predictive processing models
especially that have come out you know
around clark's work and friskin's work
and i know you're very familiar with
them
yep familiar with them too and then and
then the idea here is
and then and the version of predictive
processing that mark especially thinks
will help make this work is not so much
the
uh the standard computational one but
very much of 4e
cognitive science where embodiment in
action and all that is
is the best interpretation of how it
works that's super exciting
uh for me and my you know 30 000 uh
foot view that i tend to take i'm
thinking about what is information you
know i talk about
information processing system that's
input recursive computation and output
systems
okay i talk about semantic information
and i talk about information theoretic
okay so now if we have relevance
realization pulling information
processing and semantic
and then we bloop it into information
thorough theoretic
and the predictive capacity of
extracting you know and reducing
uncertainty
man you are pulling uh things yeah
so i think some of the most recent work
on uh integrating
uh the information theoretic approach
with the semantic information idea
it is works exactly along these lines
um and perhaps that that's something we
could get back to
but again assimilation integration and
connecting with coherence
yeah it's it's all converging in that
way so one of the things well first of
all in place people are not aware of it
like a very quick very quick primer
on predictive processing and so the idea
the predictive processing model
um is the idea that what the brain is
primarily trying to do
is uh predict now notice how that
immediately
sounds like we're at the propositional
level and that's why i took took time to
indicate that
you know where brett and where brent and
mark and i are working
is at a deeper level um
uh uh you know more at the perspective
but very much more at the perspective i
will participate
actually i shared on my list uh a little
uh five minute uh clip of a spiderweb
and is it spiderweb an extension of its
mind okay absolutely
and how it utilizes that to model and
sense
and expand its sensory feedback oh
that's just cool that's right
so we can you know make that yeah we're
talking about a level potentially a
spider here okay so yeah
right not at narrative prediction
justification
no exactly exactly and at some point i
want to talk to you about that when we
move to narrative
how that starts to afford the
justification problem but uh
well again i'll take that one i'll take
that one
too many candies okay so uh the basic
idea of the predictive processing model
and so
um is that what this brain is finally
constantly trying to do is sort of
predict its environment the problem with
that
uh the idea is it can't really directly
predict its environment
so what it can directly do is predict
itself so the idea is that
and so this is i think central to it
it's making use of sort of a
a bayesian idea of how you alter
um the uh assigned probability uh
of a belief i don't like this language
uh but based on evidence
because it's all very propositional
we're trying to thought below that level
so i'm not going to keep saying that i'm
going to
i'm just i'm going to accept that people
understand that we're just using these
words
as stand-ins so the idea is what the
brain does
is higher levels of the brain so you
think of the brain as hierarchically
organized and great is
you know emphasized this multiple times
right now i think there's a very much a
consensus view about that
right is that uh it's a neural network
in which uh
here's the lowest level this is the
level that's in direct sensory motor
contact with the world
and then this level what it tries to do
is predict this level
and then the idea is right and then
there's a level
above that that tries to predict that
level and so on and so forth
yep and then the idea is by trying to
reduce
how surprised it is by what the lower
levels are doing
indirectly it will start to predict
pat it will start to zero in and predict
the real patterns in the world
is that fair enough so far greg
absolutely and we're gonna let's just
uh asterisk the word surprise there yeah
yeah because then when you get surprised
you will then
pull and then there's gonna be energy to
try to model that surprise
and then readjust right and so these are
these are places by the way where
relevance realization comes in
because predictive processing talks
about revising the model or making
alternative models
and that's actually an issue around
relevance
um and relevance realization because
what the system is also
facing as a problem is uh it's it's
modeling
um the hidden causes in the world it's
what i mean by hidden causes it's trying
to create
by predicting what's going to happen at
the lowest level
it's actually creating at least a
procedural model
of the causal patterns in the world that
are making those patterns
on the in the sensory motor and the
system can adjust by altering its
predictions
or by altering the world to make them
conform to the predictions and so there
if you can see how it right gets you
right into the sensory motor loop
and then he wrote but the idea there is
okay but it's often having to predict
things
at multiple temporal scales like right
now
like because the question of what should
it model of course the relevance
realization question
yep what's happening right now what's
happening a little deeper in the
environment
longitudinal and then the idea is there
the layers in some sense correspond
to these temporal and spatial scopes
and then what you're doing is this
hugely dynamic
massively recursive uh uh
process in which error signals from the
layers permeate
up and then correction signals or action
signals permeate down and now notice
what we've got greg
we've got what we've been talking about
a lot right we've got a machine that is
set up to do
an important kind of relevance
realization why because as it goes
up it's doing data compression it's
doing a massive data compression
as it comes down it's doing massive data
particularization
it's doing this in a hugely complex
process of dynamical self-organization
and what is it doing it's regulating
the sensory motor loop because it's
updating its models but it's also
changing the world with action
and so you've got this loop and this
loop happen and so you can see
i mean i'm not going to go into the
technical details but you can see how
relevance realization theory and
predictive processing theory just
really go really really nicely together
you can just i mean for me at least you
see the fractal line the fractal line
you know back and forth basically yeah
exactly
well said and so what we're we're seeing
here
it and i want to sort of say this
carefully i wrote it down because it
you can i'm making uh like you said i'm
making an assimilation space
for where the mutual modeling kind of
idea can come in but what we've got
notice how this the brain is actually
modeling itself
in this massively recursive manner in
order to model the world
and in modeling the world it's always
also modeling itself this goes to the
work of hawaii
and michaels and they work on the
predictive processing models of the self
and you and i are going to talk about
the self at some point too indeed
because they because one of the things
one of the error signals
i get right in the environment is
precisely because of the way my body
and my actions are altering my
perception for example i'm blinking
right now
okay so i have to my system has to learn
to discount that i'm a blinker
it has to learn to discount that i have
a
limited visual scope but that it can so
what it does is it's also
reciprocally modeling itself
as it models the world so it's modeling
itself in these layers but it's also
creating a
model of itself as it's modeling the
world so you see
massive mutual modeling going on here
it's gotta have a model so it can factor
itself out as it shifts so that it
maintains
you know the signal uh you know relative
to the noise that's created by its own
shifts so you can think of it even
having so
while it is doing this massive recursive
ma self like modeling it's also
modeling itself in a sense
well this is why i often talk about
animal environment relation that's
between models
yeah animal environment relations model
you gotta model the animal
modeling environment and then model the
dynamic relation and that's
in in order to maintain the control
necessary hierarchical control feedback
systems across time those are the
variables that definitely need
exactly and so ultimately what is
constraining all of this
is how again so that's not completely
you know uh that doesn't completely
answer
like what is modeled and how is it
modeled right it's ultimately about
how is it this notion of self-relevance
how is it ultimately relevant
to an adaptive autonomous autopoetic
system
and again that's another place in which
the relevance realization machinery
comes in it has to do
technically more specifically with the
way the predictive processing model
talks about how attention
uh is altering salience but i won't go
into the details
right what we've got is okay we've
already got
you know massively recursive uh
relevance realization predictive problem
processing
multi-layer and multi-dimensional mutual
modeling going on
is that okay so far um i'm there
okay so let's say we can we've got we've
got predictive processing
uh networks uh let's say the work that
i've been doing over the
all these time all these years um we've
got some
uh we've got a naturalistic account of
relevance realization so we could give
everything i've described so far could
be given to a machine okay now we're
ready to start
is it can we just set the machine in the
world no because we're going to face
some problems
um now these problems it's not i don't
want to i'm not trying to convey that
people in
predictive processing are not aware of
these problems i'm building this out
step by step
so we as a group you and i and potential
uh viewers can contract the development
of the argument
okay so what we we encounter this
problem okay we've got this system and
it's got to be doing all this but
at some point it's taking in signal from
the world
at some point it's engaged in the
project like you've said already
of taking information in the information
theoretic sense and converting into
something that is meaningful and useful
to it
so at the very beginning we have a
problem that is called signal detection
problems
okay and and what we're gonna why i'm
stopping here
is because one of the prominent theories
of consciousness the neuroscientific
theories of consciousness
runs directly off this signal detection
problem
that's why that's why i'm going to stop
at these certain roadblocks right every
roadblock
is associated with maybe roadblock is
the right way but anyways
it is associated with a guide post and
then we want to stop and then organize
this and this is the pieces
uh that we can hone in on a pull from
assimilate and integrate
exactly exactly so what's the signal
detection problem the signal detection
problem is the idea
that there's all the there's
information i want there's information
that is some sense
relevant to the organism but the problem
with that because
information in the technical sense is
just
co-variations between events in the
world right
that the information i want is always
enmeshed and overlapped with the
information
i don't want the way this is referred to
in the literature
is signal and noise noise doesn't mean
just auditory noise
it means any event in the environment
anything that
you are that you could potentially take
a signal
but that you don't want let me give a
concrete example okay
so you're a gazelle right and you hear
a noise in the bush okay
now the problem facing you is this is
that a signal for a leopard
that would be signal so does that noi
does that sound i'll
use the word sound does that sound from
the bush
is that signal is that predicting that a
leopard is there
remember the progressive processing
stuff yep or is it
being caused by the wind and all it's
predicting is the wind is going to
continue to blow
now you can't tell
right you can't tell so the idea is if
if this is the population of all the
signals
it overlaps with all the population so
there's there's lots of times and this
is a perennial problem
for us where we're facing this kind of
pollution of signal
by noise now what you can say is well
i'll what i'll do is i'll gather more
information you can
but but the problem is the problem
regresses as you try to gather more
information
when you step back and try to get
information to
get certainty about the first thing
you'll encounter this same problem
again and again and again okay more
noise comes along yeah
exactly and in fact there's some
versions of this where the noise expands
exponentially as you try and explore for
for the signal that will initially
resolve
your your primary signal you get a
combinatorial explosion
yeah exactly exactly so what does signal
detection
theory say we do so signal detection
theory says
we what we do is we set a criteria so
this is a top
down act what we do is i'm going to have
to use anthropomorphic language
okay but that's because language is
anthropomorphic so somehow the brain
decides
right that it's going to kep it's so
let's say here's the two graphs
overlapping
it's going to set a criterion and what
it means by that is
it's going to count anything right below
a certain threshold as
noise and anything above a certain
threshold as signal
now let's take a look at the gazelle
because
the gazelle the the two errors the
gazelle can make are not
equally in terms of their relevance to
the gazelle
right so if the gazelle makes of this
mistake
it hears the sound
and takes it to be the wind
and it's a leopard whoa that's
a very dangerous mistake you don't make
too many of those mistakes
yeah right but there's another mistake
it can make it could
it could hear the sound predict a
leopard
and it turns out to just be the wind but
now it's running around and all the
other gazelles laugh at it
now it can't do this infinitely because
it will burn up all of its caloric
energy
so it can't just say always run away
that won't work
but what do gazelles do and again not
anthropomorphically even though the
language sounds that way
they set the criterion like really real
depending on how you draw your graph
i'll
write i'll talk about it this way they
set the criteria very low which is
they'll count
almost all of the sounds as signaling
the leopard
right because it's much more dangerous
for them to miss a leopard
than to mistake the wind for a leopard
but notice how this is a relevance
realization project
depending on the situation and depending
on my state
that can be completely inverted where a
mistake
is much more worse for me much more
relevant to me
than a miss so it's not like you can set
the criterion
like here right you have to constantly
adjust
and move the criterion and you're
constantly doing that in terms
well i would argue of relevance
realization
what is your state how are things
relevant and important to you
what's the context what's the relevant
uh what's the relevance
the comparative relevance of the two
risks this is known as error management
theory right right and sensitivity
specificity
yep yep all those things and so like
people even use error management theory
and i think plausibly to try and explain
some of our cognitive biases
why we're biased to make the things we
do the kinds of mistakes we do
uh let me give one that's i think
non-controversial um
you will you when things are approaching
you
you see them approaching you uh in an
illusory fashion
you see them coming towards you much
faster than they actually are
but when they're receding from you
you're accurate it's like
well why well because if i screw this
one up
it's very costly doc right yeah yeah
right
so the idea is and you can see you can
even talk about like confirmation bias
and other biases in this term
again those those heuristic functions
that help us with relevance realization
yep okay is this okay so far totally
i'll just i'll throw in so
we got a dog named benji he's very
skittish okay
so that basically every loud noise
everything is then
very low threshold for the threat you
know so he's just like you know all of a
sudden
jump into to escape and run back up into
those underneath his bed
and hide okay so that's just whatever he
said
i've seen other dogs unbelievably
relaxed actually that gets into a
tendency towards neuroticism at a
dispositional level but anyway
yeah but and we can talk about that
about the degree to which
you know personality is a a very
longitudinal setting of of some of these
things
right yeah we'll flag that for later
yeah because well yeah let's just do
everything
let's just do it okay um so here's where
we get a theory
of consciousness actually on the
literature
um and it's by lao l-a-w
and this goes to some work i've done
with richard wu especially richard wu
but also anderson time
and so what lao argues is that what any
good theory of consciousness has to do
is to be able to tell you the difference
between consciousness
and blind sight so remember what blind
side is blindside is that
where people can intelligently pick up
on the co-variations in the environment
that they are phenomenologically blind
so
they don't have any adjectival qualia
but you can like say you can do stuff
like this you can put a stick in front
of them and say
right and then and they'll say can you
see the stick and they'll say what's
wrong with you you're an
i'm blind you say black where do you
think the stick is
oh and you can oh it's there i'm just
guessing
then you can even say well continue
guessing what angle do you think the
stick is at
45 degrees they can do stuff like that
okay
wise france is the primary guy
consciousness lost and found a really
good book on that yep
exactly so what lao proposes is the
following
that right there's two things that go
into signal detection
there's what he calls the sensitivity of
the system that's just its ability to
pick up on the co-variations right and
he says
that's what blindside is blindside is a
reliable picking up on the co-variations
notice the language from descartes
coming in here right away
right right you're picking up on the
co-variations
but you don't actually have anything
like a representation
right and so what laos says is
what hap what what turns that blind side
detection of co-variation into
consciousness
is the second part of signal detection
which is setting the criteria
so when the brain sets the criterion
this is think about this descartes would
love this this is a way in which the
covariations are being made
ready for reason because they're being
sorted
right in order to generate behavior that
is
relevant to the organism isn't that i
mean that
descartes would go oh my gosh that's
honestly
and so the idea here is the the
difference between consciousness
and um blind sight is
the setting of the criterion that in
some in an important way is now allowing
you to
turn the right turn the co sort
the co-variations you're doing a kind of
like i said looking through
the co-variations you're starting to
sort them in terms of
their behavioral uh and task relevance
right and so i i think that's beautiful
now what lao
doesn't say i mean i don't i don't think
he thinks this is a comprehensive theory
of consciousness he by the way thinks
that the setting of the criterion is
remember we talked about rosenthal
and the higher order he thinks that that
is the
the higher order of action right then
yeah exactly
right that nicely that fits very nicely
but he doesn't think that this is a
comprehensive account of consciousness
it doesn't explain most of the
phenomenal properties
of consciousness or anything like that
but
if we're going to no notice what
happened we're just trying to make an
organism intelligent we're just trying
to give it the ability
to do signal detection and already boom
we're starting to have to bring in
a lot of the machinery that is
intimately associated
with the function and perhaps even the
nature of the consciousness
now here's here's the point that i make
with the help of richard
and uh and anderson is yeah
but lao does not give us any account of
how
the criterion is set and remember the
setting of the criterion
is dynamic the criterion has to be
constantly set
and not only that the example right the
way i'm talking about this is
unit dimension i'm talking about as if
i'm setting one
criterion right what i'm doing is a mult
i'm setting criterion
in for many different signals in terms
of meaning
right so i'm going to need that i'm
going to be needing to se i'm going to
be needing to set this criterion
in a massively recursive right
dynamic self-organizing manner go ahead
so so essentially okay so we're going to
pull some
sensitivity and then pull in some
criteria that then it's going to
have a figure ground relation okay but
then what are we going to ask specialize
about the figure relative to the ground
well that's going to change enormously
depending on a whole bunch of different
kinds of context
exactly exactly and you can see how what
you're going to need
is you're going to need relevance
realization within this
you know this massively recursive
processing model
okay so already intelligence and
consciousness are already
co-emerging together and i want to stop
and take note of that right at the very
beginning of the project
of trying to make a system give a system
agi
we're starting to do that okay so now
let's go into that idea of
prediction and let's talk about some of
the stuff we've already
alluded to what we're actually talking
about i think this is a much better term
by the way
is we're actually talking about
anticipation we're talking about that
the system
is in a highly integrated fashion
predicting the world predicting itself
and predicting the relationship between
those that's really nice actually i
hadn't heard that but it
immediately resonates because it does
get you out of your propositional
you know and get you into intuitive
perspectival participatory you
anticipate
much more than oh i'm going to
hypothesize and predict
i mean at least just at an intuitive
level it resonates more well no and
i think it goes very well with your
behavioral investment theory and here's
how i think so
so the the system is like doing this
prediction and within that it's already
doing we talk about it has to be setting
the criteria and it's doing all this
stuff
right but it's also forming a model of
itself it has to be
and those are being right mutually
modeled and notice what it has to do
we can think of prediction the
anticipation is made up of prediction
which is actually picking up on these
patterns and then setting the criterion
right right but we also can think of it
and here's where i think it comes in
as preparation yes anticipation is
prediction
right preparation and preparation is not
just to have a model of the self
it is to use that model to shape how
you're going to invest
your that's right right that's right and
and when you're actually engaged in the
investment what you're actually doing
is you're breaking up little parts of
investment because a huge amount of what
your goals are
often are nested hierarchies of goals
exactly exactly
and so then you have to anticipate
there's there's the immediate goal and
then there's a mid-level goal there's a
higher order goal and all of that's got
to be sorted
and then you're organizing and
coordinating the pattern of
dynamic investment relation
excellent so let's take all of that
and what we've got is we've got this
hierarchical
multi-scale doing exactly what greg just
said right and it's doing the relevance
realization the predictive processing
but also the criterion setting
and it's all right did you drop signal
there for a second
i did no i'm here okay i'm fine so i
think
i think we're okay okay okay so right
you're doing all of that
and so what we're basically doing is
what
i've already alluded to we're picking up
these these co-variations
that are co-variations that are actually
picking up on affordances
because they're not just predictions of
the world they're anticipations
so this is picking up on co-variations
that are affordances
that my signal for you keeps dropping
i'm not sure what but you're not getting
any problem on your end right actually
mine's been clear
so okay well i'll just assume that it's
just something
insignificant then and so notice what
we've got we've got co-variations that
are actually
importances because we're talking about
anticipation not just but a pure
prediction
and then we're now doing this massively
you know massively recursive process of
compressing and particularizing them
and that's constraining the sensory
motor loop and i propose to you
that is a very powerful way in which
we're doing what descartes wanted
right where we're doing some significant
compression actually deep learning on
the co-variations
and we're detecting through them which
is exactly what the predictive
processing model says we're detecting
through
we're actually figuring out what the
deep causes are really
let's put it actually in the here and
now so what you're as you track
the screen right you're making
anticipations about what my movements
will be or what's
what you're expecting and when it
freezes right then you get surprised
then you activate like well okay
what's actually happening you shift them
free of reference right yeah because the
because the scale modeling across the
various perspectival and participatory
is not as it gets off and then all of a
sudden you have to wait right right
because i don't want to treat all the
surprises as equally salient or relevant
right that's important but you know if
your hair moves in a way that i didn't
really anticipate
because i didn't realize that you've got
a haircut right i don't go oh
i mean i might if it's a particularly
beautiful haircut but right you know
it's not going to do much certainly not
with me
so as as greg said a few minutes ago
we're talking about how we're getting
basically
at least proto-aspectualization
proto-representation
we're getting this process in which
we're starting to do that whole process
of sizing up
we're starting to get the very
beginnings of perspectival knowing
but notice how much of the mesh in
participatory knowing
because the system models itself to
model the world
and in modeling the world it models
itself there is a deep
knowing by conformity there amen
okay so now we get to
the next theory because we talked about
how this is so
massively recursive and self-organizing
which means it's inherently dynamical
which means it's inherently
developmental it's inherently
developmental
as both the relevance realization theory
and the predictive processing theory
are so this takes us to clearness
and clear men's theory is probably the
theory of consciousness that gives pride
of place
to developmental explanation
he has what he calls the radical
plasticity
hypothesis where plasticity is
uh a system's capacity to literally
redesign itself
reshape itself in order to make itself
more adaptively fit to the environment
so there's been a lot over the last two
decades about how plastic the brain is
brain plasticity and he's invoking them
and so he has an
idea he has an argument and this is one
that many people the way
really where he thinks consciousness is
something that develops
he doesn't think now where it begins a
development of course
he doesn't say and that of course
doesn't so it's not going to resolve any
of our
ethical issues uh about you know when
when does consciousness begin
but what he says is that consciousness
is actually something you
learn to do which is really interesting
that's very that's
now this is the part where descartes is
going to get very start what do you mean
all right how can you learn
consciousness as long as you have the
immaterial glue in your head you're
conscious and clearman's
is saying well no what actually has to
happen
and think about what we said all this
machinery has to train up
and evolve before it's going to get some
of the capacities
that are needed for consciousness so he
like lao
follows rosenthal that's why i spend so
much time on roosevelt
right and making the deep connections
between rosenthal's theory and
aspectualization
he says well and notice how this fits in
with the predictive processing
what's happening is right you're doing
this training up and the higher levels
are learning to better represent the
lower levels
we don't we don't have to be bound to
that language because we can already use
this other language
and what's interesting about it he
acknowledges
he acknowledges that it's not enough for
the upper levels to represent or even to
track the lower levels
he says that what turns them into
consciousness
is they they care about
the lower levels right they right they
right there there there's an emotional
or at least an
afternoon would appreciate that yeah i
thought you would
mark those too because mark thinks uh
that the
affective components within the
self-organization of predictive
processing
are doing a lot of the important work
yeah actually i
think i said heidegger would appreciate
it but i appreciate it also
because of course i follow dreyfus in
arguing that heidegger's primordial
notion of care
is relevance realization yay right and
often you who says
you know the difference between our
processing and the computers processing
is that doesn't care about
the information right because it doesn't
have to take care of itself
i was trying to show care by setting you
up
you did it you did extremely well so as
soon as we get to this theory
right and notice how it works because
the system is
like developmental development means a
system is auto-poetic it's taking care
of itself
it's making itself and that and because
of that it
cares for itself and higher levels of
processing
care about right and so you've got right
relevance realization within
that's affording relevance realization
without
and so yeah okay so yeah i'll
throw in a couple of uh so the last
there are six principles of behavioral
investment that organize it
uh principle four is our computational
control principle which is basically
the hardware principle five is the
constant iterative learning process
yeah okay and principle six is the
lifespan developmental history process
yeah okay and uh you know the sort of
example that i use in relationship to my
own personal world they're sort of in
terms of reorganizing
anybody that's been through adolescence
may remember how you get reorganized so
when i was 11 i thought of the world in
one way and let's say
say girls one way and then at 12 and 13
i thought very differently as
some sort of reorganization of relevance
realization happened
in those two-year periods yeah
how development and consciousness really
yeah you put it in those terms you
realize how radically your consciousness
can shift depending on developmental and
learning histories
excellent so notice again why did we
have to bring in development because if
we're going to make an intelligent
system
we're going to give it relevant
recursive relevance realization we're
going to give it deep learning
in right hierarchical predictive
processing networks
and those are inherently self-organizing
inherently developmental
et cetera et cetera autoplecis is
inherently developmental
so we've got we just we we're giving it
the basic machinery of intelligence
relevance realization predictive
processing
signal detection and the capacity for
plasticity and development and we're
just building in more and more of a case
for the machinery of consciousness yep
okay so now let's go to the
something we talked about last time
which is you're starting to get
higher levels of coordination you're
trying to coordinate
right all this processing
and i think we're now at a place where
we can start talking about well you've
got all this mutual modeling going on
but there's a sense in which you're
going to get
very higher order sort of
so the mutual modeling is also going to
lead to the abstraction
of a very generalized model of oneself
and the world a space if you allow me a
metaphor
that can be coordinated to at various
different
degrees and and so this automatically
sets up and you know where this is going
to go
this sets up the global workspace theory
which is
right the idea that consciousness is
a like your desktop of your computer and
so what's the model here okay so the
model is
you have all this processing going on
think about all these layers right
right and all these dimensions and most
of that those
processing is unconscious like most of
the files in your computer are
unconscious
but what you can do is you can activate
them by bringing them on to your desktop
your desktop can
potentially access all or at least most
of the files
yep okay so you can access that
and then when you make it active what
can you do well
you you basically can make it interact
and reintegrate
with other pieces of information and
then you can broadcast it back
you can store it you can store it
multiple locations and so the idea is he
also uses the metaphor of a theater
right consciousness is
what consciousness is the stage is
working memory
and then i don't like this model of
attention but i can put it aside for now
the spotlight of attention i don't mind
it so i i'll take the spotlight a little
[Laughter]
bit no that's fine
i think i think the model of attention
that comes out of uh
out of you know waltz and and
christopher mole has is much more about
relevance realization and integration
than just shining because i don't think
that sailing is shined i think it's it's
it's
much more complex yeah my favorite
broadband and and the attentional filter
iterative process is anyway whatever we
can we can do
about that but we can come back anyway i
think yeah we'll come back we can
potentially come back to
uh consciousness and attention later but
let's go with so you have this model of
you know the people in the audience are
are
right unconscious and the people behind
the stage are like the unconscious
but the stage people are more like the
top down processing
and the audience is more like the bottom
up processing and then
you've got the stage of working memory
and then the spotlight of attention
shines on it
and that's consciousness so the idea is
that
right you have all your or to use the
computer metaphor you have all your
files they're the unconscious
you draw them into the desk space
and when you're manipulating them that's
shining the spotlight of attention
and read and restructuring them remember
that restructuring them
and then sending them back to memory
that's what the function of
consciousness is
yep and this is really cool because uh
obviously now they're
and so the uh the originator of this
theory is its current defender
although there are many people who
defend it it's a very prominent theory
is bernard bars it's called global
workspace theory
and he explicitly argues and i
demonstrated that the model that there's
this terrific overlap
between working memory attention and
consciousness
and what's okay so i want to pause right
here right now
why why is that part of this argument
well what what do measures of what are
working what are measures of working
memory really highly correlated with
measures of general intelligence yeah
exactly
and what does working memory do we used
to think it was just a holding space and
that's what sort of
a bit on uh a bit conveyed by the
the stage model but the work of my
colleague
uh really brilliant lynn hasher at the
university of toronto
is basically said no no that's that's
too simplistic
what working memory is it's a higher
order filter
for relevance and you know how you know
that that's how
what working memory does because of the
phenomena of chunking
remember we've talked about this if i
give you a string of letters and there's
no pattern in it
and there's like 12 of them and i ask
you to remember that string of letters
and you turn and look you'll remember
four or five but if i turn that string
of uh 12 letters into four words like
pig and cat and dog
and sit then you can do it because
they're chunked
right and so chunking shows that what
working memory is doing
is it's some kind of higher order
relevance filter
so notice what we're getting this theory
is deeply connecting
consciousness and intelligence together
and it's doing it in terms of
a function strongly associated with
working memory which is relevance
realization and
that is explicitly what bars says the
global workspace is set up to do
greg are you still there
are you back i did lose you there
that time my screen was fine
so
are we allowed to think looks like we're
stable how far did how far did you hear
uh good question um relevance uh working
memory
uh chunking into just that
right yeah it was basically at the point
of chunking right
so the point i'm making is uh notice
what we've got in this theory
we've got a deep interconnection between
intelligence
and consciousness via working memory
right and that's in terms of
recursive relevance realization going on
at the level
of working memory its capacity to do
chunking and restructuring
right right and then bars and
also shanahan and bars have explicitly
argued
that that is the function of the global
workspace they specifically
argue that the function of the global
workspace is to solve the frame problem
which shanahan has argued is
is now it is basically like he
specifically says this it is the
relevance problem it is the deep problem
of relevance
and they argue that the function of the
global workspace
is in fact to do relevant realization
now i don't totally agree with their
solution uh but that's we don't need
that right now
i just happen to have this on my desk so
this is a
global neuronal workspace okay so just
for people
that so that linkage immediately then
gets into brain so what he does there
some of the beautiful stuff that he does
there
is he he talks about the consciousness
ignition
uh switch okay which is basically so he
does lots of subliminal
processing uh so if you give something
about 200 milliseconds
uh you start to get networking but you
do not get
you know criteria maybe relevance
realization networks that thing right
but at 300 milliseconds when you get
enough top down you get ignition switch
between parietal
and frontal lobe and that very much that
cascade
very much corresponds to conscious
access right where you can then say oh
this is what
i now have this on my screen this got
into uh the stage
and and so now you get that kind of
lincoln excellent
excellent i'm going to pause here
because you're frozen again greg
yeah connection seems to be weak i don't
think it's on my end my signal looks
really good
um i mean yeah maybe it's getting
interference somewhere but i see we seem
to be back
so the thing you just did i hope we got
it all you brought into haiti and you
brought in the
you know the 200 millisecond 300
milliseconds
get the consciousness ignition when you
get the the prefrontal and the
parietal the frontal the parietal
linking up i want to also again go the
other way
one of the most prominent theories of
general intelligence is the p
fit theory the you know the frontal
parietal connection
is what makes us intelligent uh that's
with g so again
the g machinery and the conscious
machinery like this
like this both at the level of the
theory relevance realization
and at the level of the anatomy
all right
okay so what we're doing is
i want to just draw this out again
where we're showing all the way through
as we're building intelligence the
machinery of relevance realization
and the machinery of perspective and
participatory knowing
are coming along we're getting we're
getting the machinery
of consciousness the functionality and
also some of the phenomenology is
emerging because we've spent a lot of
time
showing how you can get a lot of the
phenomenology
the adverbial quality of the
perspectival knowing
the participatory knowing you can get
all of that
out of this kind of relevance
realization machinery
hmm yeah we seem to be getting some
intermittent
interference here they're dead there's
definitely right
we're having trouble mutual modeling
maintain the connection
well no i think what it is is is we're
on the actual we're on the verge of
solving this problem
right and nobody's tracking illuminati
don't want us to know
don't have to figure out consciousness
somebody interrupt the signal quick
okay so what i'll do is uh also because
i have a pretty hard out in about
five minutes i want to do one more
theory and then we'll stop there
we're sort of halfway through this
argument so the next theory a prominent
theory of consciousness because it
follows
directly on this is work not by uh bars
but
bohr and seth and they basically argue
uh that what what is conscious they do
this thing about
what is consciousness for and when do we
need processes to be conscious
and when do they move into unconscious
and so they talk about well
like they talk about the process of how
you make pro
how you make your behavior automatic
what they mean by automatic is
things you can do without having any
conscious awareness of it or
are very minimal subsidiary awareness
like when you're typing right or you
know it can get very bad like highway
hypnotism where you've been driving oh
crap and you realize you haven't been
paying attention
or aware of the road at all you've been
off and mind wandering
who would have thought it but his army
was doing a really good job keeping your
mind
right it's kept me alive so far john
and so they talk about well what the
evidence seems to converge on is we need
consciousness
so the opposite what's the uh so we
don't need consciousness
when we can really proceduralize and
make
our interaction right automatic
right so what's the opposite of that
well we need consciousness where there's
high complexity
null novelty ill-definedness of the
situation
exactly the situations where you need a
lot of what well relevance realization
and what do they say consciousness does
again
again the fact that these are convergent
like they say the job of consciousness
is
restructuring it is doing that higher
order recursive relevance realization
because what that does is it enhances
your ability to sift through the data
find the patterns do the deep learning
so again
what are they saying consciousness
functions to do it's higher order
recursive relevance realization
that allows us to do sophisticated
problem solving makes us more
intelligent
it's what areas of the brain oh look
it's mostly frontal but it's you know
blah blah blah
all this like convergence convergence
total
urgence convergence so i'll just to put
this in
in real-world stuff so uh in my kitchen
we had the silverware i don't think i
told the story of silverware
uh in one drawer then we got a new
dishwasher
and then it had a handle on it so we
couldn't open that damn drawer anymore
so we had to move the silverware
took me like six months that you know
actually to uh because i'd so
procedurally habituated to go over to
the silverware drawer
was downloaded i knew you know my system
knew how to do that
but then every time i'd open the drawer
i'd be like right i was like
what are you gonna learn and that's
because the surprise
of seeing the drawer empty you know and
then i had to rework it and now
you know three years later i never go to
that drawer anymore so uh that gives you
the
frame about how that you know that's
next one and notice how
more and seth feed back into clearman's
right and the whole developmental thing
you have to you have to re-learn to find
it conscious
right and so you can see the boring
stuff also being very very
um well consistent and coherent with
uh the clear means of the developmental
and then that's also consistent as we
said with this
recursive relevance realization
massively hierarchical dynamically
self-organizing predictive processing
the anticipatory relationship and notice
we are getting
so so much of the function of
consciousness
at least is converging on
the idea of consciousness as relevance
realization
within predictive processing and
notice how we aren't trying to make
consciousness all we're trying to do is
make artificial general intelligence
but we keep building all the machinery
the functionality of consciousness
and we keep tapping into the same areas
of the brain
that are deeply associated with
intelligence
and behavioral flexibility
and i have the intelligence to learn for
our silverware drawers after three
months
and so i so i am conscious at some level
john that's good news
i'm sure you are so what i want to do
we've covered off some of the main
theories
next time i want to take a look at the
last remaining
uh theory and draw them all together
which is finoni's integrated information
theory
i won't go into great detail i want to
mention a few ideas
uh from the information closure theory
but i think they gel very well
also with the model we're building here
um and then
we will have done the canvassing and
what the overall
overall art art of this argument has
been
and you've been helpful is it's how how
we can draw
all these things together in such a
coherent fashion
and it's a completely naturalistic
argument because we're doing it from the
design stance
every step along the way this is what we
would add to the machine this is what we
would add to the machine this is what we
react to the machine
i want to now just take a moment and say
one sort of final thing
that sort of helps also to bridge a bit
between
the function and the phenomenology
because i would argue
that salience is relevant to the global
workspace
they have all this relevance realization
going on but when
information is relevant to the global
workspace
such that it does the working memory
processing it looks for the
restructuring
or it's potential it is a protect it it
potentially demands restructuring
within the global workspace i think
that's when we're now talking about
salience and once we have that we now
have the possibility like a
demonstrative indexicalities salience
tangling all that stuff
now coming online that's a lot of pieces
it's coming together it's a lot of
pieces coming together
we've got a bit more to go and then
um we will have sort of made
and then we'll have some time to draw it
all together then there's you're going
to take over
and you're going to make an important
argument again you're going to what did
you say thirty thousand feet
about thirty thousand feet then um and
and
and draw it all together again and then
we'll probably have one or two
conversations
around uh the relationship between this
problem
and the heart problems of of meeting
um the meaning crisis and and then and
then the relationship between that
problem and what you call the problem of
psychology
we can come back to that trial and see
what progress we made on it
right so that that's our work for today
that's our work for today
i i apologize for anybody watching it
for
any of the little
pauses in the in the signal but that's
technology's the god of
that limps and of course our high god
right now is the internet
and high gods always um disappoint you
when you most need them
so there you go
right but we're still sacrificing
we still need you all right
so as always thank you my good friend
again you did it you
like you these you just do these little
things and it was like a light of fire
and connections were just sparked for me
uh it was just fantastic thank you very
very much hey man i really enjoyed it
thank you