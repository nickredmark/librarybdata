[Music]
you
[Music]
welcome back to awakening from the
meeting crisis so last time we were
taking a look at the centrality of
relevance realization how many central
processes central to our intelligence
possibly also to our at least the
functionality of our consciousness
presuppose require are dependent upon
relevance realization so we had gotten
to a point where we saw how many things
fed into this and then I made the
argument that it is probably at some
fundamental level a unified phenomena
because it comports well with the
phenomena of general intelligence which
is a very robust sorry a very robust and
reliable finding about human beings and
then I propose to you that what we need
to do is two things we need to try and
give a naturalistic account of this and
then show if we have naturalized this
can we then use it in an elegant manner
to explain a lot of the central features
of human spirituality and I already
indicated in the last lecture how some
of that was already being strongly
suggested we got an account of
self-transcendence that comes out of
dynamic emergence that is being created
by the ongoing complexification and this
has to do with the very nature of
relevance realization as this ongoing
evolving fitted nosov your sensory motor
loop to its environment under the
virtual engineering of bio economic
logistical constraints of efficiency
that tend to compress and integrate and
assimilate and resiliency that tend to
particularize
and differentiate and when those are
happening in such a dynamically coupled
and integrated fashion within an ongoing
opponent processing then you get
complexification that produces self
transcendence but of course much more is
needed
now I would like to proceed to address
now I can't do this comprehensively not
in a way that would satisfy everybody
who's potentially watching this this is
this is very difficult because there are
aspects of this argument that would get
incredibly technical also to make the
argument comprehensive is beyond what I
think I have time to do here today there
are I'll put notes for things that you
can read I'll point you to if you want
to read it more deeply what I want to do
is try to give an exemplary argument an
argument of an example of how you could
try and bring about a plausible
naturalistic account of relevance
realization now we've gone a long way
towards doing that because we've already
got this worked out in terms of
information processing processes but
could we see them potentially realized
in the brain I mean one more time I want
to advertise for the brain I understand
why people want to resist the urge of
sort of a simplistic reduction that
human beings are nothing but their brain
that's a very bad way of talking
that's like saying it a table there's
nothing but it's atoms that doesn't
ultimately make any sense it's also the
structural functional organization of
the atoms the way those that structural
functional organization interacts with
the world how it unfolds through time so
simplistic reductionism should
definitely be questioned on the other
hand we also have to appreciate how
incredibly complex dynamic self-creating
plastic capable of you know very
significant qualitative development the
brain actually is so propose to you that
you can one aspect of relevance
realization the aspect that has to do
with trading between being able to
generalize and specialize as I've argued
is a system going through compression
remember that's something like what you
with line of the best fit and particular
ization when your function is more
tightly fitted to the contextually
specific data set and again this gives
you efficiency this gives you a
resiliency this tends to integrate and
assimilate this tends to differentiate
never accommodate okay so try to keep
that on mine now what I want to try and
do is argue that there is suggestive
it's by no means definitive and I want
it clearly understood that I am not
proposing to prove anything here that's
not my endeavor my endeavor is to show
that there is suggestive evidence for
something and all I need is that that
makes it plausible that there will be a
way to empirically explain relevance
realization so let's talk about what
this looks like
so there's increasing evidence that when
neurons fire in synchrony together
they're doing something like compression
so if you give for example somebody a
picture that they can't quite make out
in your looking how the brain is firing
the areas of the visual cortex for
example if it's a visual picture are
firing sort of asynchronously and then
when the person gets in though AHA you
get large areas that fire in the
synchrony together interestingly there's
even increasing evidence that when human
beings are cooperating in joint
attention and joint activity their
brains are getting in to patterns of
synchrony so that opens up the
possibility for a very serious account
of distributed cognition I'll come back
to that much later now what we know
what's going on in the cortex right and
this is the point that's very I think
very important this is scale invariant
which what that means is that many
levels of analysis you will see this
process happening why is that important
well if you remember relevance
realization has to be something that's
happening very locally very globally it
has to be happening pervasively
throughout all of your cognitive
processing
so the fact that this this process I'm
describing is also Skellington variant
in the brain is suggestive that it can
be implementing relevance realization
okay now what happens is at many levels
of analysis what you have is right you
have this pattern where neurons are
firing in synchrony and then they become
asynchronous and then they fire in
synchrony and then they become
asynchronous and they're doing this in a
rapidly oscillating manner so this is an
instance of what's called
self-organizing criticality
it's a particular kind of opponent
processing a particular kind of
self-organization so we're getting more
precision in our account of the
self-organizing nature potentially a
relevance realization okay so let's talk
a little bit about this first and then
we'll come back to its particular
implement instantiation in the brain so
self-organizing criticality this goes
originally back to the work of per Bock
and so let's say you have grains of sand
falling like in an hourglass and
initially it's random well random and
from our point of view of we're right
within a zone a individual grains will
end up somewhere in that zone we don't
know where cuz they'll bounce and all
but over time what happens because
there's a virtual engine there friction
and gravity but also the bounce right so
the bouncing introduces variation the
friction and you know the gravity put
constraint and what happens is the sand
grains self-organized there's no little
elf that runs in and shapes the sand
into a mold itself organizes into a
mound like that and it keeps doing this
and keeps doing this now at some point
right it enters a critical phase
criticality means the system is close to
right is potentially breaking down see
when when it's self-organized like this
it demonstrates a high degree of order
order means that as this mound takes
shape the position of any one grain of
sand gives me a lot of information about
where the other grains are likely to be
because they're so tightly organized
it's highly ordered but then what
happens is that order breaks down right
and you get an avalanche it avalanches
down and it's just and if and if this is
too great if the criticality becomes too
great this system will collapse and so
there are people that argue that
civilizations collapse due to call
what's called general systems failure
which namely which is that these and
EPIK forces right are actually
overwhelming the structure of the system
and the system just collapses so
collapse is a possibility with
criticality however what can happen is
the following the sand spreads out due
to the Avalanche and then that
introduces variation important changes
in the structural functional
organization of the sand mound because
now what happens is right there is a
bigger base and what that means is now a
new mound forms and it can go much
higher than the previous mound it has an
emergent capacity that didn't exist in
the previous system and then it cycles
like this it's cycles like this now at
any point again there's no T loss to
this at any point it can just the
criticality can overwhelm the system and
it can collapse at any point you
criticality within you can overwhelm the
system and you can die right but what
you see right is you see the brain
cycling in this manner self-organizing
criticality the neurons structure
together that's like the mound forming
and then they go asynchronous this is
sometimes even called the neural
Avalanche right and then they
reconfigure into a new synchrony and
then they go asynchronous so do you see
what's happening here what's happening
here is the brain is oscillating like
this and what it's doing with
self-organizing criticality is it's
doing data compression and then it does
a neural Avalanche which opens up
introduces variation into the system
which allows a new structure to
reconfigure that is momentarily fitted
to the situation it breaks up right now
do you see what it's doing it's
constantly moment but this is happening
in milliseconds it's evolving that's
fitted nough sits complexify structural
functional organization it is doing
right compression and particular ization
which means it's constantly moment by
moment
Valle ving its sensorimotor fitted mist
to the environment it's doing relevance
realization I would argue now what does
that mean well one thing it we should be
careful of when I'm doing this again I I
I'm using words and gestures and that
and of course to convey and make sense
but right what you have to understand is
this is happening at myriad of levels
right there's this there's this
self-organizing criticality doing this
Fitness at this level and it's
interacting with one another one doing
it at this level all the way up to the
whole brain all the way down to
individual sets of neurons so this is a
highly recursive highly complex very
dynamic evolving fitted Ness and I would
argue that that is there by implementing
relevance realization there is some
evidence to support this
so Thatcher at out did it's important
work 2008 2009 pointing towards us so
here's the argument I'm making right I'm
making that the argument that RR can be
implemented it's not completely
identical to because you remember
there's also exploration and
exploitation but it can be implemented
by this and I've also last time made the
argument that relevance realization is
you're right your general intelligence
if this is correct then we should see
measurable relationships between these
two right of course we have we've known
how to measure this psychometrically for
a very long time and now we're getting
ways of measuring this in the brain and
what Thatcher found was exactly that
they found right Thatcher Adele found
that there's a strong relationship
between measures of self-organization
and how how intelligent you are
specifically what they found was the
more flexibility there is in this the
more in
you are the more it demonstrates a kind
of dynamic evolvability the more
intelligent you are
is this a conclusive thing no there's
lots of controversy around this and I
don't want to misrepresent this however
I would point out that there was a very
good article by Hess and gross in 2014
doing a comprehensive review of the
application of self-organizing
criticality as a fundamental property of
neural systems and they I think made a
very good case that were that it's
highly plausible that self-organizing
criticality is functional in the brain
in a fundamental way and that lines up
it's convergent with this so what we've
got is the possibility I mean I mean and
this this carries with it from so I'm
hesitant here because I don't I don't
want to buy by drawing out the
implications I don't want to thereby say
that this has been proven I'm not saying
that but so remember the if but if this
is right this has important implications
it says that we may be able to move from
psychometric measures of intelligence to
direct measures in the brain much more
in that sense objective measures
secondly if this is on the right track
it will feed member this a lot of this
ideas were derived from sort of emerging
features of artificial intelligence if
this is right it may help in feedback
into this and help develop artificial
intelligence so there's a lot of
potential here unfortunately for both
good or ill I'm hoping if you'll allow
me a brief aside I'm hoping by this
project that I'm engaged in to link as
much as I can and the people that I
wouldn't work with can and you know my
lab and my colleagues linked this
emergent scientific understanding very
tightly to the spiritual project of
addressing the meaning crisis rather
than letting it just run rampant
willy-nilly all right so if you
me that's a way in which we could give a
naturalistic account of our our in terms
of how neurons are firing there these
are firing patterns now I need another
scale-invariant thing but I need it to
deal with not how neurons are firing but
how their wiring what kinds of networks
they're forming I'm not particularly
happy with the wiring metaphor but it
has become pervasive in our culture and
it's mnemonic ly useful because firing
and wiring rhyme together so again there
is a sort of a new way of thinking about
how we can look at Network it's called
graph theory or network theory it's it's
gotten very complex in a very short
amount of time so I want to do just sort
of the core basic idea with you that
there's three kinds of networks right so
this is a neutral all right
this doesn't mean just networks in the
brain it can mean networks like how the
Internet is a network it could mean how
an airline is a network a rail railway
system etc this analysis this
theoretical machinery is applicable to
all kinds of networks which is part of
its power so you want to talk about
nodes these are things that are
connected and then you have connections
so these I'm drawing two connections
here this isn't a single thick one these
are two individual ones okay two
individual connections here so that's
sort of the same number right of
connections and nodes for each network
so there's are three kinds of network so
this is called a regular network it's
regular because all of the connections
are short distance connections okay and
you'll notice that there's a lot of
redundancy in this network everything is
double connected okay this is called a
random or a chaotic Network right it's a
mixture of short and long
connections and then this is called a
small world network this comes from the
Disney song it's a small world after all
because this was originally sort of
discovered by Milgram when he was
studying patterns of social
connectedness and it's a small world
after all all right now again these now
originally people were just talking
about these it's now understood that
these are just these are names for broad
families of different kinds of networks
that can be analyzed in too many
different subspecies and I won't get
into that detail because I'm just trying
to make an overarching core argument so
remember I said that this network has a
lot of redundancy in it and that's
really important because that means that
this network is terrifically resilient I
can do a lot of damage to this network
and no node gets isolated nothing falls
out of communication it has tremendous
it's tremendously resilient very
resilient but you pay a price for that
all that redundancy this is actually a
very inefficient network now your brain
might tricky because that looks so well
ordered it looks like a nice clean room
right and clean rooms look like they're
really highly ordered and that's oh this
is must be the most efficient because
cleanliness is orderliness and
orderliness is efficiency and you can't
let that mislead you you actually
measure how efficient the network is by
calculating what's called its mean path
distance I calculate the number of steps
between Eenie between all the pairs so
how many how many steps do I have to go
through to get from here to here one two
how many do I have to go to go from here
to here one two three four I do that for
all the pairs and then I get an average
of it and the mean path distance
measures how efficient your network is
at basically communicating information
these have a very very high mean path
distance so they're very inefficient you
pay a price for all that redundancy and
that's of course because we're done to
see inefficiency or in a trade-off
relationship now this and here's where
your Dania Bryn's gonna look this is so
messy right this is so messy well it
turns
this is actually efficient all right
it's right it's actually very efficient
because it has so many long-distance
connections it's very very efficient it
has a very low mean path distance but
because they're in a trade-off
relationship it's it's not resilient
very poor and resiliency right so notice
what we're getting here these networks
are being constrained in their
functionality by the trade-off in the
bio economics of efficiency and
resiliency Marcus breed as sort of
mathematical proofs about this in his
work on network configuration now what
about this one the small world network
well it's more efficient than the
regular network but less efficient than
the random network but it's more
resilient than the random network but
less resilient than the regular Network
but you know what it is it's optimal it
gets the optimal amount of both
it optimizes for efficiency and
resiliency but optimizes for efficiency
and resiliency now that's interesting
because that would mean that if your
brain is doing relevance realization by
trading between efficiency and
resiliency it's going to tend to
generate small world networks and not
only that the small world networks are
going to be associated with the highest
functionality in your brain and there's
increasing evidence that this is in fact
the case right in fact there was
research done by Langer at Al in 2012
that did the same thing similar thing to
what sure did so here we got this again
RR is G and it looks like our RS might
be implementing this is what I'm putting
here small world networks that's these
guys small world networks and what
Thatcher Dahl found is a relationship
between these the more your brain is
wired like this the better your
intelligence again is this conclusive no
still controversial that's precisely why
it's cutting edge however increasingly
we're finding that these kinds of
patterns of organization make sense
remember Marcus breed was doing work
from just looking at just artificial
networks neural networks and you want to
optimize between these so you're getting
design arguments out of artificial
intelligence you're starting to get
these arguments emerging out of
neuroscience interestingly lanyard al
did a second experiment in 2013 when you
sort of put extra effort tasks demands
on working memory you see that working
memory becomes even more organized like
a small world network Hilger Adel in
2016
found that there was a specific kind of
small world network having to do with
efficient hubs the thing is entitled
efficient hubs in the intelligent brain
nowtell efficiency of hub regions in the
salience network are correlated with
general intelligence so what seems to be
going on is again suggestive you know
not getting it you know not conclusive
but you know you let the lat you've got
the Langer work working memory goes more
like this and then you've got this very
sophisticated kind a species of this
recent work research correlated with the
salience network in the brain do you see
that that as your brain is moving to a
specific species of this within the
salience network you become more
intelligence in the salience network is
precisely that Network right by which
things are salient to you stand out for
you grab your attention one more time is
this conclusive no I'm presenting to you
stuff that's literally happening the
last two or three years there's
tremendous there should be there's
tremendous controversy in science
however this is what I'm pretty
confident of that that controversy is
progressive it's ongoing it's it's
getting better and better
such that it is plausible that we will
be able to increasingly explain and
it'll be increasingly convergent with
the ongoing progress in artificial
intelligence that we will be able to
increasingly explain relevance
realization in term of the firing and
the wiring remember the firing is
self-organizing criticality in the
wiring of small world networks and
here's something else that's really
suggestive the more a system fires this
way the more it wires this way so the
system is firing in a self-organizing
critical fashion it will tend to network
as a small world network the more it
wires this way the more it is wired like
it's a small world network the more
likely it will tend to fire in this
pattern these two things mutually
reinforce each other's development so
remember let's try to put this all
together I want you to I mean it's hard
to grok this I get this but remember
this is happening at a
scale-invariant massively recursive
complex self-organizing fashion right
this is also happening scale-invariant
at a very complex self-organizing
recursive fashion and the two are deeply
interpenetrating and affording and
affecting each other in ways that have
to do directly with engineering the
right the evolving fitted nests of your
salience right of your salience
realization and your relevance
realization within your sensory motor
interaction with the world this is I
think strongly suggestive that we are
getting we that this is going to be
given a completely naturalistic
explanation okay notice what I'm doing
here right I'm giving a structural a
theoretical structural functional
Organization for how this can operate so
we got last time right the last couple
of times we had this strong convergence
argument to this we have a naturalistic
account of this at least the rational
promise that this is going to be
forthcoming and then we're getting an
idea of how we can get a structural
functional organization of this in terms
of firing and wiring machinery now this
is again like I said this is both very
exciting and potentially I'm scary
because it does carry with it the real
potential to give a natural explanation
of the fundamental guts of our
intelligence I want to go a little bit
further and suggest that not only may
this help to give us a naturalistic
account of general intelligence it may
point towards a naturalistic account at
least of the functionality but perhaps
also perhaps also
of some of the phenomenology of
consciousness this again is even more
controversial all right but again my
endeavor here is not to convince you
that this is the final account or theory
it's to make plausible of the
possibility of a naturalistic
explanation
okay so let's remember a couple things
there's a deep relationship between
consciousness
remember the global work space Theory
the functionality and that that overlaps
a lot worth working memory this is
global work space theory so that should
be a t global work space Theory working
memory and we already know that there
are important overlaps in the
functionally very in areas that have to
do with general intelligence working
memory attention salience and also that
measures of this and measures of the
functionality of this are highly
correlated with each other that's now
pretty well-established we've also got
that we know from Lin hashes work that
this is doing relevance realization do
you remember also gave you the argument
when we talked about the functionality
of consciousness that many of the best
accounts of the function of
consciousness is that it's doing
relevance realization and so this should
all hang together they should all hang
together such that the machinery of
intelligence and the functionality of
consciousness should be deeply
integrated together in terms of
relevance realization we do know that
there seems to be some important
relationships between consciousness and
self-organizing criticality
this has to do with the work of cut
smelly at all and others ongoing their
work was in 2004 so they did what's
called a binocular rivalry experiment
basically you present two images to
somebody and they're positioned in such
a way that they right they are going to
the different visual fields and they
compete with each other because of their
design and then so what happens in
people's visual experience is they let's
say it's a triangle and a cross that
what they'll have experientially is I'm
seeing across oh now I'm seeing a
triangle I'm seeing the cross and I'm
seeing a triangle and and don't forget
that but that's not obscure to you right
so you know the Necker cube right when
you watch the nectar cube it flips right
so this can be the front and it's going
back this way right or you can flip and
you can see it the other way and so you
are even doing bar napkin or this is the
front and it goes that way right so you
are constantly flipping between these
and you can't see them both at the same
time so you
that's what binocular rivalry is and so
what you have though is you do this a
little bit more controlled you present
it to two different visual fields so
different areas of the brain and so what
you can see is what happens when the
person is seeing the triangle well one
part of the brain goes into synchrony
and then as soon as the triangle that
goes asynchronous and the other part of
the brain that's picking up on the
plusses right because that's a different
area the brain gets more basic right
that goes into synchrony and what you
can see is as the person flips back and
forth in experience different areas of
the brain are going into synchrony or a
synchrony so that is suggestive of a
relationship between consciousness and
self-organizing criticality again
suggestive but we've already got
independent evidence meaning a lot of
convergent evidence that the
functionality of consciousness is to do
relevance realization which explore
it's strong correlation via working
memory with measures of general
intelligence and so and we knew we know
that this is plausibly associated with
self-organizing criticality so again
convincing no suggestively convergent
yes there's another set of experiments
done by Monte Adel in 2013 and what
you're basically doing is you're giving
people a general anaesthetic and then
you're you're observing their brain as
they pass out of consciousness and back
into consciousness and what did they
find they found that as the brain passes
out of consciousness it loses its
overall structure as a small world
network and breaks down into more local
networks and then as it returns into
consciousness it goes into a small
network small world of network formation
again so that consciousness seems to be
strongly associated with the degree to
which the brain is wiring as a small
world network now I want to try and
bring these together in a more concrete
instance where you can see the
intelligence the consciousness and this
dynamic process of self-organization all
that work I want to bring it back to the
machinery of insight the machinery of
insight so if you remember we talked
about this because we talked about the
use of disruptive strategies and we
talked about the work of Stefan and
Dixon
do you remember that what they found was
they kind of way a very sophisticated
way but nevertheless a very reliable way
of measuring how much entropy is in
people's processing when they're trying
to solve the inside problem remember
they were tracing through the gear
figures and what they found is that
entropy goes up right before the insight
and then it drops in the brain becomes
even right that sorry the behavior that
was a mistake on my part the behavior
becomes even more organized right now
that's plausibly and they suggested
that's plausibly an instance of
self-organizing criticality that what's
happening is you're getting right the
neural Avalanche it's breaking up and
then that allows a restructuring which
goes with the restructuring of the
problem remember so you're breaking
frame with the neural Avalanche and then
you're making frame like the new round
right as you restructure right your
problem framing and you get the insight
and you get a solution to your problem
now interestingly enough Schilling has a
Malthus is linking insight to Salk very
clearly Shelley has a mathematical model
from 2005 linking insight to small world
networks she argues quite persuasively
that's what this is very interesting
since what you can see happening in an
insight is that people's information is
initially organized like in a regular
Network just think about that
intuitively like so my information is
sort of integrated here right local
organization a regular network local
organization right so the whole thing is
right right all I've got is a regular
Network but what can happen right is
here's my right my regular network I'll
make that a little more clear here's my
regular network right and what happens
is right
one of these I get a long-distance
connection that forms so my my regular
network suddenly is altered into a small
world network which means right I lose
some resiliency right listen resiliency
but I gained a massive spike in
efficiency I suddenly get more powerful
so insight is when a small sort of when
a regular network is being converted
into a small world network because that
means this is a process of optimization
right because remember this is more
optimal than this and you can see that
in how people's information is organized
in an insight they take two domains
here's right think about how metaphor
affords insight you take two domains Sam
as a pig and you get suddenly this
connection between it and those two
regular networks are now coalesced into
a small world network okay that's great
so in some of the work I've done with
other people I've been suggesting
because of this the following that what
happens in insight Allah Steven and
Stefan in Dixon is you get herself you
get self-organizing criticality and that
self-organizing criticality right breaks
up a regular network and converts it
into a small world network so what
you're getting is suddenly sudden a
sudden enhancement increase the
optimization of your relevance
realization and once it accompanied with
its accompanied with a flash in salience
never and then that can be extended in
the flow experience you're getting an
alteration of consciousness an
alteration of your intelligence right an
optimization of your fitted nough stew
the problem space okay again
like I'm gonna say this again right I'm
trying to give you stuff that is makes
this plausible I'm sure that in
specifics it's going to turn out to be
false because that's how science works
but that's not what I need right now
what I've tried to show you is how
progressive the project is of
naturalizing this and how so much is
converging towards it that it is
plausible that this will be something
that we can scientifically explain and
more than scientifically explained that
we'll be able to create as we create
autonomous artificial general
intelligence okay
let's return back if I've at least made
it plausible that there's a deep
connection between relevance realization
and consciousness I want to try and
point out some aspects to you about
relevance realization and why it is
creating a tremendously textured
dynamically flowing salience landscape
okay so remember how relevance
realization is happening at multiple
interacting levels so we can think about
this right we're right you're just
getting features that are getting picked
up remember the multiple object tracking
there's no this is this this so basic
salience assignment right and then this
is Bob based on work originally from
Mason in 1976 his book on sentience as
I've mentioned that before and then some
work that I did with Jeff Marshman and
Steve Pierce and then later work that I
did with Anderson Todd and Richard woo
the feature ization is also feeding up
into for grounding and feeding back
alright so a bunch of there this this is
all these features and then presumably
I'm for grounded and other stuff is back
rounded right
this then feeds up in configuration
you're configuring me together and
figuring me out think of that language
right so that I have a structural
functional organization I'm aspectual
eyes for you
that's feeding back and of course
there's feedback down to here and then
that of course feeds back to and up to
framing how you're framing your problems
and we've talked a lot about that and
that feeds back right so you've got
right this happening and it's giving you
this very dynamic and textured salience
landscape and then you have to you have
to think about how that's the the core
machinery of your perspectival knowing
right notice notice what I'm suggesting
to you here you've got the relevance
realization that it is the core
machinery of your participatory knowing
it's how you are getting coupled to the
world so that coevolution reciprocal
realization can occur
that's your participatory knowing this
feeds up to feeds back to write your
salience landscaping
right this is your perspective of
knowing this is what gives you your
dynamic situational awareness your
dynamic situational way this textured
salient landscaping this of course is
going to and we'll talk more about that
it's going to open up an affordance
landscape for you certain certain
connections right affordances are we
going to become obvious to you all right
and you say oh man does anybody look
this is how people are trying to wrestle
with this now here's an article from
frontiers in human neuroscience
self-organizing free energy minimization
that's Tristan's work and it has to do
with ultimately about getting you're
processing as efficiency as possible and
optimal grip on a field of affordances
using all of this language that I'm
using with you right now
that's by Bruno Berg and ret veld from
2014 frontiers in human neuroscience
just just as one example among many okay
so this is feeding up and what it's
basically giving you is affordance
aviation certain affordances are be are
being selected and made obvious to you
okay
that of course is going to be the basis
of your procedural knowing knowing how
to interact okay and I think there might
be a way in which that more directly
interacts here maybe through kinds of
implicit learning but I'm not going to
go into that we'll come back later and
to how propositional knowing relates to
all of this okay I'm putting it aside
because this is where we do most of our
talking about consciousness all right
with this I think at the core the
perspective of knowing but it's the
perspective of knowing that's grounded
in our participatory knowing and it's a
perspective of knowing like you're
to a tional awareness that obviates
affordances is what you need in order to
train your skills that's how you train
your skills and and we know that
consciousness is about doing this higher
order relevance realization because
that's what this is this is higher order
relevance realization that affords you
solving your problems okay so this is I
mean I'm trying to say I mean all of
this when I'm talking about your
salience landscaping I'm talking about
it as the nexus between your relevance
realization participatory knowing and
your affordance ovation procedural
knowing your skill development right
respectable knowing at the core and then
what's happening in here is this if
that's the case then you can think of
your salience landscape as having at
least sort of three dimensions to it
right
so one is pretty obvious to you which is
the aspect shoe ality your salient as I
said your salience landscape is a
sexualizing things things right okay so
the features are being for grounded and
configured and they're being framed so
this is a marker it is a specialized
member whenever I'm representing or
categorizing it I'm not capturing all of
its properties and just capturing an
aspect so this is a specialized
everything is a specialized for me
alright there's another dimension here a
centrality I'll come back to this later
but this has to do with the way
relevance realization works relevance
realization is ultimately grounded in
how things are relevant to you right
literally literally how they are
important to you you import right how
they are constitutive right at some
level the sensorimotor stuff is to get
stuff that you literally need to import
materially and then at a higher level
you literally need to import information
to be constitutive of your cognition
we'll come back to that transition later
but what you have is right the
prospective knowing is there's doing a
spec shoe ality and then everything is
centered it's not right it's not non
valence t' it's vectored on to me and
then it has temporality because this is
a dynamic process of ongoing evolution
timing small differences in time make
huge impacts huge differences in such
dynamical processing Kairos is really
really central when you're when you're
intervening in these very comp massively
recursive dynamically coupled systems
small variations can unexpectedly have
major changes so things have a central
relevance in terms of their timing not
just their place in time so think of
your salience landscape is on four
holding like in these three dimensions
of aspectual resent reality and right
temporality there's an acronym here act
this is an enacted kind of perspective
we'll know it all right so you've got
consciousness and what it's doing for me
functionally is all of this but what
it's doing in that functionality is all
of this and what that's giving me is
perspectival knowing that's grounded in
participatory knowing that affords
procedural training and that it has a
sexuality a salience landscape that has
our sexuality centrality and temporality
it has look at what it has sensuality is
the hereness my consciousness is here
because it is indexed on me
of course it has nowness because timing
is central to it nope like that that was
intended that move right and it has
togetherness unity how everything fits
together I don't want to say unity
because unity makes it sound like
there's a single thing but how there's a
it there's a oneness to your
consciousness it's all together you have
the hereness the nowness that
togetherness the salience the
perspectival knowing how it is centered
on a lot of the phenomenology of your
consciousness is explained along with of
the functionality of your consciousness
is that a complete account
no but it's a lot of what your
consciousness does and is it's a lot of
what your consciousness does and is
so I would argue that at least what that
gives us is an account that we're going
to need for the right hand of the
diagram why altering states of
consciousness can have such a profound
effect on your reaching down to your
identity up into your agency why it's to
be linked to things like profound a
profound sense of insight we've talked
about this before when we talked about
higher states of consciousness how it
can feel like a dramatic coupling to
your environment
that's that participatory coupling that
we found in flow this all I think hangs
together extremely well which means that
looks like I have the machinery I need
to talk about that right hand of the
diagram before I do that I want to make
a couple of important points to remind
you of things relevance realization is
not cold calculation it is always about
how you write how your body is making
risky affect Laden choices about what to
do with its precious but limited
cognitive and right metabolic and
temporal resources relevance realization
is deeply deeply always and think about
how this also connects to this and
consciousness it's always always an
aspect of caring that's what Reed
Montagu argues the neuroscientist in his
you know in his book your brain is
almost perfect that what makes us
fundamentally different from computers
because we are in the finitary
predicament is we are caring about our
information processing and caring about
the information processed therein so
this is always affect it's it's things
are salient they're catching your
attention they're arousing they're
changing your level of arousal remember
that
arousal is an ongoing evolving part of
this right and they are constantly
creating affect motivation moving
emotion moving you towards action you
have to hear how at the guts of
consciousness intelligence there is also
caring and that's very important right
that's very important because that
brings back I think a central notion and
what will I know many of you are
wondering why I haven't spoke about him
yet but I'm going to speak about him
later
right from Heidegger that at the core of
our being in the world is a foundational
kind of caring and this connection I'm
making this is not far-fetched look at
somebody deeply influenced by Heidegger
who is central to the third generation
or for econo science that's the work of
Dreyfuss and others and Dreyfuss has had
a lot of important history in reminding
us that our knowing is not just
propositional knowing it's also
procedural and ultimately I think
perspective on participatory he doesn't
quite use that language but he points
towards it he talks a lot about optimal
gripping and importantly right if you
take a look at his work being in the
world on Heidegger when he's talking
about things like caring he's invoking
in central passages the notion of
relevance relevance when he talked about
what computers can't do and later on
what computers still can't do what
they're basically lacking is this high
daguerreian machinery of caring which he
explicate sin being in the world in
terms of the ability to find things
relevant and this of course points again
towards higher your notion Heidegger's
notion of design right that our being in
the world is in him to use my language
is inherently transmission eree is
inherently trans janet is something that
we do not make we and our intelligible
world Co emerge from it we participate
in it and I want to take a look more at
what that means for our spirituality
next time thank you very much for your
time and attention
[Music]
you