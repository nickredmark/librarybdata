howdy this is Jim rut and this is the
Jim rut show listeners have asked us to
provide pointers some of the resources
we talked about on the show we now have
links to books and articles referenced
in recent podcasts that are available on
our website
we also offer full transcripts go to Jim
rut show com that's Jim rut show com
today's guest is Tristan Harris Tristan
is the co-founder and executive director
of the Center for Humane technology hey
Jim it's good to be here
thank you for having me yeah great to
have you on some interesting stuff we're
gonna be talking about Tristan was the
first design ethicist at Google we'll
talk about what that is a little later
he's an expert on how technology is
steering us all Tristan has spent over a
decade understanding the subtle
psychological forces in play in our
online world and he's the co-host of the
podcast your undivided attention in
various writings and sayings you've said
that your experience as a magician as a
younger person have helped you inform
your perspective about the online world
can you tell us a little bit about your
magic days and what that perspective has
provided yeah I always start with magic
because if you start by looking at what
technology is doing that the common
metaphor is people think well it's all
about how we're using technology but
we're the ones using it and the the
premise of I think the rest of the
conversation will have today is really
more about how is technology influencing
us and the reason I go to the magic
metaphor so often in my childhood is
it's about recognizing a symmetries of
power like an asymmetric advantage and
that's what a magician essentially has
over you is there's a sense of they know
something about your mind that you don't
know about yourself because if you did
know that about yourself the magic trick
wouldn't work and what's especially
interesting to me about magic is that
the universality of the things that a
magician knows about all human minds and
the fact that even a nuclear physicist
can easily be fooled by a magician
there it's actually doesn't really
matter what domain of expertise or PhD
you have magic is about a subtler layer
of human vulnerabilities think of them
like the zero day vulnerabilities of the
back doors on the human mind in terms of
cause-and-effect reasoning in terms of
attention in terms of misdirection and
that's obviously the kind of basic stuff
that is you go more advanced there's a
whole long list of psychological
features and I think one last thing
there is that magicians are kind of like
the first applied psychologists because
way before we had in official fields of
psychology they were figuring these
things out and they didn't even name the
principles but they had this whole kind
of library of things that they were
aware of and started to to exploit makes
perfect sense you've also mentioned
working with the famous or maybe it's
infamous Stanford persuasive technology
lab who are they and what impact do they
have on our situation and on your
perspectives so at Stanford I studied
computer science and so I have a typical
computer science background but I was
mostly interested in the psychology
cognitive science sort of dimensions
there's actually a major at Stanford
called symbolic systems that you know a
lot of alumni famous alumni like Reid
Hoffman and Scott Forstall the guy who'd
been at the iPhone were actually in that
program that I was kind of more oriented
to and part of that program there was a
class by this one professor named BJ
Fogg and from his lab called the
Stanford persuasive technology lab where
they essentially took everything we knew
about the field of persuasion influence
social psychology robert cialdini's
influence slot machine design clicker
training for dogs click click get the
dog to you know eat the thing rewarded
and he applied all those disciplines all
those domains of expertise into hey what
if we were to embed that in technology
you know could technology be an
influence over your attitudes beliefs
behaviors habits and you know I that
class there's a whole history we can go
into there but the founders of Instagram
were in that class many of the early
alumni that went on to join the growth
teams at the early tech companies uber
LinkedIn Facebook many people went on
and because it gave a tool set how to
build more and more engaging products
and just want a name really quickly that
the professor
BJ Fogg is often vilified I think in
incorrectly because he warned the FTC
about the dangers of persuasive
technology and where this could go back
in the late 90s I think it was 1996 and
he has always tried to apply persuasive
technology for for good things like
world peace he actually had a project
called peace dot which was like peace
dot facebook.com/ peace Google calm
where each tech company would ask how
could it be persuading the world towards
world peace we can go into these topics
if you if you want to is there's a lot
of detail here yeah I think the takeaway
here and sort of something everyone
needs to understand is that these are
deeply designed systems
he's funny you mentioned slot machines
the first time this issue of deeply
cognitive science informed technology
was one a business friend of mine went
to work as the CEO of the third largest
slot machine company in the United
States oh really
yeah he did yeah this was been the 90s
we had dinner I don't know a year later
and he was you know telling me all about
what he was doing and he told me they
had this is the third largest thought
machine company they had 200 PhDs on
staff and he said they all came from the
disciplines where people are basically
torturing rats and make them do various
things so I go that's great
you know you know what a scary thing
that the you know really sharp PhDs are
tuning the finest details of slot
machines to increase their addictive
potential and then my second touch point
on this was last year I published an
essay called regaining our cognitive
sovereignty about how you know people
have become addicted to smartphones in
particular and one of the data points I
dug up when I was writing that was I
went to the Facebook internal job board
or I guess publicly facing job board and
typed in psychology and 708 openings at
Facebook had the word psychology in the
job description and so again that was
aha you know these people
makes perfect sense if I was them I'd be
doing the same thing it's trying to
bring in serious talent that understands
how the human mind works and uses that
insight to make these products way more
effective at pushing our buttons and
we'd have any idea and it's important to
note that this is a new unique
phenomenon I mean you know the hammer
that might be sitting in your shed does
not have a thousand phd's behind it
trying to figure out how do they get you
to use the hammer in the particular way
even your telephone back in the nineteen
seventies didn't have a thousand PhDs
behind it trying to figure out how to
use your social psychology and do
experiments on rats to figure out how to
get them to talk through the telephone
right so they these were tools and so
what's really changed this is why we
really focus on this aspect of
persuasive technology as what we're
accusing as the problem is the degree of
asymmetry you know that the fact that
there's a thousand engineers behind the
screen who knew a lot more about our
psychology than we know about ourselves
well I think you're certainly right
about the hammer but I'm gonna push back
just a little bit and this is a theme
I'm gonna push on a couple of times to
distinguish between what was going on in
the previous generation and the current
generation there was a technology before
online that was amazingly
psychologically informed both from
academic psychology and in practice and
that is TV advertising in fact
advertising and in general but
particularly TV advertising these things
were amazingly expensively produced
still are I assume and they did all
kinds of tests including EEG etc so it's
not like this has not been something
we've confronted before well this is
what always comes up I feel like I've
I've spent a decade now having the
conversation of but haven't we always
had propaganda TV marketing advertising
etc what could be so alarming or bad
about what we have now it really is a
new unprecedented situation I think that
you know if someone was telling you
about an atomic bomb but you were
already we're irregular bombs I said
well I said this bombs just a bigger
bomb they didn't show you like an entire
city getting obliterated you say it just
sounds like it's a bigger bomb than the
other bombs I mean there's nothing new
here and you know in this case with
atomic bombs you can actually visually
see the just stunning degree of
exponential damage and scope that you
can create
whereas in technology I think the degree
of advancement in the persuasion
capacity is not in the visible domains
you can't use your eyeball to see how
big it is right and we don't have
intuition for what billions of people
being influenced by these things really
looks like if we wanted to slow it down
there's at least four major distinct
things that are that are different
usually supposed to say there's three
things because three so much easier to
remember let's go with four the first is
the intimacy and pervasiveness of the
infrastructure so TV you know used to
have to watch TV you choose to watch it
it was in your living room maybe it's on
periodically but you know an
advertisement has to reach you through a
channel you happen to be watching but
it's not like built into the fabric of
your walls in your home whereas the
smartphone we check 150 times a day we
have 2.7 billion people using Facebook
that's about one and a half times the
size of Christianity in terms of a
psychological footprint we wake up with
the thing ever since you know we turn
off our alarm in the morning so the time
we go to bed at night so that first
characteristic is the sort of intimate
infrastructure aspect the pervasiveness
the second is the social persuasive
element one of the things I learned at
the Stanford persuasive technology lab
is how much more powerful persuasion can
be if you can control the social
psychological cues that people are using
to make sense of what's true or what's
real or what to do and if you can say
that well 300 of your friends liked or
shared this or you have five people
watching you as you're doing this action
right now they're little eyeballs or on
the screen the social persuasiveness is
very powerful this gets into things like
the Asch conformity experiments and so
on the third aspect is AI and this is
probably the one that's hardest for the
brain to kind of into it but it's that
there is a the ability to make and run
predictions about what you're going to
do and what would cause you to do
something you know every single time you
use YouTube and you say I'm gonna watch
this one video and then I'm gonna be
done you know and then you end up
watching for two hours and you're like
what the hell just happened to me you
know I fell into a YouTube trance and
you say well I guess I should have had
more self-control but what was really
going on is when you hit play on a
YouTube video
there was an AI on the other side of the
screen you know you activate all of
Google's multibillion-dollar computing
infrastructure and it wakes up a little
avatar voodoo doll version of you it
does you know a calculation on a hundred
million different recommended videos it
could show you next and it makes a
prediction about which one I could show
you that would cause you to stay here
and so we don't really think about the
unfairness of this fight between our
Paleolithic brain on one side of the
screen and then the biggest
supercomputing infrastructure in the
world on the other side the last thing
I'll mention is that AI is used for
personalization so the fourth
characteristic is the degree that each
of us get our own Truman Show or the
split testing works at a micro-targeted
level so instead of billboards or
advertising on TV where you're getting a
sort of broadcast capacity you don't
actually know who's watching what except
to some coarse grain levels this is now
fine grain levels micro-targeted
precision intimate access pervasive
social and AI based indeed indeed iein I
was involved in some businesses in the
80's 90's and even into the early double
lots were direct mail was a big part of
the program and you know the direct mail
was slowed a bit opaque and very costly
if we'd had these kinds of tools back
then oh my god could we have done some
stuff right not that we didn't do too
bad with our direct mail but you are
absolutely right in combination these
four things put us into a qualitatively
different regime that's exactly right
yeah interesting you talk about the
YouTube
what he call its role frankly I don't I
watch YouTube only one video following a
link somebody sent me or something I
found on Twitter I never just sit there
and let the damn thing run but I
understand a lot of people do and you
know one issue that you brought up for
sure and other people have as well is
that these algorithms not that they're
aimed to suck people in to extremism but
because of the way our cognitive systems
operate we respond more powerfully to
extremism and therefore even though it's
not an independent variable the results
of the YouTube algorithms
continuous role thing can be pulling
people towards a more extreme
perspective yeah what do you say about
that yeah well I'd like to go even
stronger and say that the regime of
social platforms and YouTube and so on
that we have basically jacked into the
brains of 2.7 billion people is
completely dismantling our information
environment it has poisoned the
information ecology it's like the Flint
water supply for our brains and that
might sound like extreme statements but
I hope we get into defending why that's
actually the case because I want people
to not see this as a conversation
between you know are we addicted to
YouTube or are we now
did it get me to watch one more video or
did it not it's more about the whole
grande set of sort of climate change
like effects it creates in the social
fabric but to answer your question
specifically so you know we have at the
Center for Humane technology a bunch of
ex tech whistleblowers who are building
some of these different systems and one
of them is Guillaume cheslow who's an
amazing researcher and whistleblower who
built part of the YouTube recommendation
system and you know obviously how much
have you paid for your YouTube account
recently yeah nothing all right nothing
so and how is YouTube well and Facebook
etc worth more than a trillion and a
half dollars a market value they sell
attention and obviously that is the
product that's actually more precisely
as Jaron Lanier would say the ability to
just imperceptibly change your identity
belief behaviors etc is the product so
the capacity to influence you is the
product and they need your attention to
do that and because you know YouTube is
competing with TV and with Facebook for
attention it needs to get more and more
aggressive over time so they'll start
adding things like autoplay etc and a
lot of people don't know that 70% of the
billion hours a day that people spend on
YouTube is driven by the recommendation
systems 70% it's not like we open up
YouTube there's this blank white box and
we just click the videos that we want to
watch or type type in the search terms
we want we do that too but we do that
vast minority of the time mostly it's
the autoplay so now you imagine okay so
that doesn't seem so bad let's say the
AI pointed at my brain gets me to watch
one more video doesn't seems like such a
bad deal the question is what is it
actually steering people to do and
notice that it's not us that are
responsible for what we watch it's
mainly YouTube here because it's got the
asymmetric influence and 70% is driven
by it and you know the metaphor we like
to give people here is if you were to
line up all the trillions and trillions
of videos that are on YouTube on one
axis one spectrum and on the left-hand
side of the spectrum you have Walter
Cronkite calm rational Carl Sagan Neil
deGrasse Tyson Richard Dawkins whatever
you want to say is the sort of calm
rational side of YouTube you know the
gym rat podcast and then on the other
side you have crazy town
you have the extreme conspiracy theories
hate speech white supremacy
movements you know all the crazy stuff
now any person you drop along that
spectrum starting at any video anywhere
from calm to crazy when YouTube wants
you to watch more on that axis which
which way is it going to send you if it
wants you to watch more videos
it's never gonna send you to calm videos
because those are not really good for
engagement and so conspiracy theories
extremism or things that further affirm
your existing worldview are the things
that get more traffic so now if you zoom
out and imagine a kind of godlike view
of the attention economy you've got 2.7
billion human animals just you know in
this little farm that's in front of you
and imagine we just tilted the entire
attention economy in the direction of
the more extreme stuff and I want people
to really feel the the kinesthetic
gravity of what that would feel like as
the whole world tilts towards just
slightly more extreme things three
examples of that were in two years ago
if a teen girl was watching a dieting
video and she what would she be
recommended she'd be recommended
anorexia videos if you were watching a
911 news video it would recommend 9/11
conspiracy theories and if you are
watching a Matt NASA moon landing video
it would recommend the Flat Earth
conspiracy theories now this might sound
just kind of funny like oh that you know
it occasionally recommended Flat Earth
but it recommended Flat Earth conspiracy
theories hundreds of millions of times
and I know the people who work in the
disinformation space who've actually
been studying some of the Flat Earth
movement and I think people really
underestimate the damage that it is done
because if you think about it if the
earth is actually flat and the
government's been lying and science has
been lying in NASA has been lying
everyone's been lying it means that all
of science is now under question you
can't trust anything that science is
telling you if you believe that there's
actually flat and everyone's been in on
it and so it's like a trust atomic bomb
is what we're gonna get to hopefully
more is in conspiracy theories there's
sort of an asymmetric power to fear
uncertainty doubt conspiracy theories
etc yeah that's an interesting one
because again you know my reaction is a
science oriented person is what the fuck
would anybody even contemplate something
like that is it some kind of poo
modernist ironic hack or what but
apparently there actually are people who
believe this stuff pretty amazing and
it's actually been used deliberately so
there's evidence that Russia actually in
their information operations actively
promotes conspiracy theories they've
actually been going into u.s. veterans
groups and actually trying to seed
conspiracy theories and doubt into those
groups Rodrigo Duterte the Philippines
authoritarian ruler there's evidence
that in the Philippines the populist
movement there was a specific use of the
Flat Earth conspiracy theory in the
populist movement in the in the
Philippines to dismantle trust in the
system when you don't trust things the
main thing you want is someone to keep
you safe
like strongmen dictators so they're
actually a very effective tool in the
arsenal of information weapons and of
course all sides are using it I mean I
think a good example on the other side
is Adam Schiff right he kept saying I
have proof that trope was colluding with
the Russians and I knew lots and lots of
people who when asked and I kept asking
what do you think the percentage is that
Muller is going to demonstrate
convincingly collusion with the Russians
and you know I kept saying hundred
percent I would say based on what I can
see more like thirty percent and so
there's another example of where the
same kinds of techniques are penetrating
into our everyday politics ya know I
mean it conspiracy theories are are
being used everywhere which is why I
take the Daniel Swanson burger view of
how do we actually reboot sense-making
and especially when you're in a low
trust environment how do you actually
get back to knowing what to trust and
ironically as the world in the race at
the bottom of the brainstem for
attention moves towards more clickbait
etc even our mainstream sources of news
information even high-quality
publications and I don't mean I said of
the New York Times I mean you know
whatever take a science magazine
Scientific American increasingly people
have to play into this game because it's
a win-lose game if I don't do the more
outrageous clickbait II thing and
exaggerate the climate change claim etc
then my competitors will and so if I
want to get people to even look at my
thing I've got to play the game yeah
and so that's why these multipolar traps
on attention are so pernicious and at
the root of our our problems here and of
course one ID
about how to make sense to sense making
is to do it collectively that you know
one of my favorite quotes that I've
coined is that humans are approximately
the stupidest possible general
intelligence as far as we know we're
just over the line in our evolutionary
tree mod nature's seldom profligate in
her gifts from evolution and one can go
into the cognitive psychology of
intelligence I'd like to go down a long
list on how you could make way smarter
intelligence than us so let's assume
we're the stupidest possible general
intelligence plus or minus epsilon some
small amount and so if we're gonna fight
back against these monoliths we probably
can't do it together and so you know
Daniel schmuck and Berger and Jordan
Hall others have been involved in
promoting the idea of collective sense
making and in fact there's a interesting
Facebook group called rally point alpha
where we try to help each other out
you know will float stories and say what
the fuck is this alright is this
horseshit or what or we'll also
deconstruct them and try to point out
the the techniques that are being used
to you know misson form what we call bad
faith discourse and so maybe at least
one answer to this problem is to
encourage people to join up with other
people and jointly use our week.but
collective intelligence to make sense of
this flow of you know bad faith and just
money oriented stuff that's coming
buyeth yeah well I think you know those
if those folks have been right to point
out how much of information has had a
for profit motive which means it's sort
of more propaganda than there is actual
information I'd like to do a quick thing
of I think people often underestimate
the degree to which we've hollowed out
our information environment and our and
our sense making and one thing I like to
do to give people as a metaphor for this
is you know newspapers thought that they
were in the truth business that their
product was selling truth but when these
tech companies come well actually
there's sort of two phases to this the
first phase was when Craigslist came
along newspapers realized actually they
weren't in the truth business they were
in the classifieds business because
classic Craigslist came along and ate
their lunch and suddenly they had to
figure out how to make the online
advertising thing work and do a hybrid
business model etcetera that was the
first phase of newspapers realizing that
weren't actually in the truth business
then the second phase comes along after
they figured out how to kind of
stabilize after Craig
came and took their classifieds which is
they thought they were in the truth
business again but then what they
actually realized as they were in the
attention business not the classifieds
business the reason that they found that
out is Facebook and YouTube come along
imagine two black black boxes right in
the black box on the left you have
essentially a news organization Wall
Street Journal New York Times etc you've
got to pay those journalists hundred
thousand dollars a year or two hundred
thousand dollars a year you got to pay
those editors you've got to pay for the
Iraq security details you've got to pay
for the long time it takes to do an
investigative story you've got to do
interview witnesses you've got to do
fact-checking
you're gonna get it wrong it's not going
to be perfect but there's some kind of
notion of process and there's human
moral judgment involved and that is an
expensive set of inputs to produce what
on the output side well an article or a
newspaper report and that generates a
certain amount of attention and then
that sold to advertisers but it comes at
a very high cost of paying all those
human beings so then YouTube Facebook
Twitter etc come along and especially
the social media companies say I have an
idea and I'm by the way I'm not saying
this is actually how it went but from a
business perspective this is effectively
why those businesses are so successful
is they said instead of paying those
journalists what if each person was
convinced to be narcissistically
addicted to how many followers they had
and how much attention that they could
broadcast that they'll essentially be a
useful idiot in the attention economy
and be publishing and generating
attention for free out of their own
narcissism because people want to be
seen as smarter and more thoughtful and
get people interested in their lives etc
so you start posting photos of your
breakfast or you start posting news
articles and saying how smart you are so
if you think about those two black boxes
the left is a very expensive way of
producing human attention the one on the
right the social media companies is a
very very cheap way of producing human
attention we're essentially the uber
drivers of the gig economy creating and
producing human attention essentially at
no cost for free not even getting paid
by the companies but then what that
means is that we replace that process of
Investigation of fact-checking of it was
witness reports of long cycles and human
discernment with essentially a bunch of
angry people who are yelling thinking
that they have the smartest
take so they do breaking news in all
caps they do cynical commentary they
take the least charitable example they
amplify it build a mob around it and say
look how bad the other side is and that
has become the default information
environment and when you realize that
that's actually what's happened and that
game theoretically now the New York
Times on the left-hand side has to
compete with that they have to now say
well let's at least play the clickbait
game to even get those articles you know
to get the same amount of attention so
it's really screwed up the entire
information environment and I think
that's what I think people deeply
underestimate about what's gone on yeah
the game theory dynamic you know my
friend Brett Weinstein good friend yes
he's that friend of mine for years and
great guy and part of his core analysis
of what's going on is that it's the same
old shit right it's evolutionary
dynamics and it's game theory dynamics
and what he always underlines is the one
that's doing us the most harm across the
board is the race to the bottom yep and
essentially what you're describing with
respect to the news dynamic is you know
that even now the New York Times it's
Foley's stupid-ass stories about some
British princess or some goddamn thing
right who gives a shit right on the
scale of things it's a zero but because
they're in the clickbait business you
know they're forced to fill the front
page full of crap so they can compete
with Facebook well this is all why the
fitness landscape needs to be a fitness
landscape that's that's coupled with
human values what what are the things
that that we actually want the
competition to be for but this is why
it's so dangerous if the competition and
the fitness landscape are basically
anchored on the resource of human
attention then that's basically reverse
engineering of the human psyche to
elicit responses out of your nervous
system I think of it like insider
trading on your nervous system
I have asymmetric access to know how to
do a trade to get the outcome that I
want from your behavior whether it's a
habit formation or a belief shift or
infinite scroll we didn't really go
through the other examples of the
persuasive things just to give one more
of the auto-playing videos or the
infinite scrolling feeds our co-founder
at the Center for Humane Technology is a
Raskin he actually invented the infinite
scroll features that's that thing where
scroll with your thumb and it never
stops right it wasn't always designed
like that
and the insight comes from a study that
was done I think it said Cornell there's
a food lab where they had six people
sitting down at a table each with a bowl
of soup in front of them and two of the
bowls of soup had a little pipe
underneath that was actually refilling
the bowl of soup with more soup as the
person drank and the question was
essentially about food psychology and
consumption do people know when to stop
on their own and it turned out that the
auto refilling soup bowls people ate
about 76 percent more calories and they
didn't really notice and so the point is
that our brains just like a magician
rely on stopping cues to know when to
stop like there's natural breaks in our
experience and the idea of technology is
well we don't want those stopping cues
showing up because that would have you
stop and reconsider what you're doing so
let's actually remove those stopping
cues and get you infinitely scrolling in
a trance and then game theoretically
once one guy does the infinite scrolling
feed the other guy has to do it then
when once one guy does the auto-playing
videos and the feed to make it more
engaging the other guy has to do it once
one guy does the filters you know that
show you beautification filters of your
face the other guy has to do it but the
vast set of harms that are emerging from
this game theoretic race on the fitness
landscape of whatever elicits responses
from the human nervous system are
basically the worst parts of us and
that's what's so dangerous about what's
coming out so whether it's distraction
addiction social isolation extremism
outrage narcissism disinformation
affirmation polarization these are all
not accidents but natural consequences
of the race for attention and that's
what I really hope people get yeah the
important thing that's I mean you just
almost set it right but I don't think
you said it quite right which is you
know these are you know the horizons
teen suicide is not the intent of
Facebook no but it is the inevitable
result of being caught in a race to the
bottom around attention hijacking
dynamics yes so if we want to really do
something about it we have to separate
out those two things and think about how
do we attack the game theoretic
attractors that produce the behavior
that produced the harm exactly and
before we get into that which is a
pretty deep topic and it you know
touches closely on the complex systems
perspective I tend to take on things
let's take a little sidebar here and
something I know you've talked about in
the past it
let's show what the actual tangible
harms are you know one could say all
right so they manipulate me I really
give a shit fuck at people manipulating
me for years I'll probably drink a
little more
Budweiser than I should but I don't
really care what are some of the real
harms that are coming out of this race
to the bottom around the attention
hijacking economy sure well we think of
it like you know prior to there being
the hyper object named of climate change
or global warming there's just these
different ecological problems that feel
separate you know you have the coral
reefs you have nitrogen runoff you have
species loss in the Amazon and they
might seem like separate problems but
then when you have a model like climate
change you see that they're all
connected so similarly with the
attention economy dynamics people say oh
like I'm a little bit more distracted we
have this distraction problem this
information overload problem then we
have this totally separate polarization
problem then we have this totally
separate shortening of attention spans
problem reductions and critical thinking
then we have this totally separate
disinformation Russian trolls seeding
doubt problem and the point is that
these are all one connected system that
are also mutually self-reinforcing
so you know if you look at some of the
harms you know our attention spans have
been going down we did a great episode
on our podcast your own divide it's
called your undivided attention with
this professor Gloria mark at UC Irvine
who's been studying the dynamics of
attention and distraction for a very
long time she's found out that it takes
about 23 minutes I think on average to
resume focus after an interruption and
we actually cycled through two unrelated
projects before we usually get back to
the thing that we were doing this is on
sort of desktop computers but she's also
found I think that I think the average
attention span or focus duration on a
screen is their desktop screens now is
about 40 seconds but I don't think
anybody would say that their attention
spans are going up or it's easier to
read books than ever but it's actually
harder and harder and harder because
we're training our nervous systems to
condition them to expect rewards or
juicy you know exciting things at a more
and more frequent rate so that's like
one of the first areas of harm then you
have addiction and isolation so you know
the more the race for attention is you
know to keep you on the screen that's
basically to keep you by yourself right
scrolling that means disassociated your
chin down your ass off
gets compressed at 45 degrees not really
breathing as much nothing less and less
sort of in your body
these isolation dynamics are really
costly I was just talking with the
former Surgeon General of the United
States v vector C and he was talking
about how he believed that isolation and
loneliness are one of the biggest
invisible social problems that is not
getting nearly enough attention 18% of
people in a big Cygnus study in 2018
said they had no one that they felt like
they could talk to when they felt alone
which is just horrible I mean we really
you know solitary confinement is one of
the worst punishments we give to human
beings in jail and we are doing that
essentially as a natural accidental
consequence of the attention economy
my biggest fear we can go through the
harms forever we have a project called
the ledger of harms on the Senate
framing technology website where we try
to outline many of these things but my
biggest fear is actually the breakdown
of the information environment the
dismantling of sense-making and the loss
of trust in society that emerges where
we don't even know what to trust anymore
that's the thing that if we just jump to
the chase we're really worried about I I
guess I should add one more thing
because the children aspect is very
important that after two decades in
decline of high depressive symptoms for
teenage girls so let's take teenage
girls I think it's 12 to 17 years old
this is in my recent congressional
hearing testimony there was two decades
in decline and then it surges up a
hundred and seventy percent after 2010
which is around the time of mobile
social apps like Instagram snapchat etc
and so teen suicides and teen depression
are up by a lot and Jonathon Heights
work and the coddling of the American
mind is very very relevant for that and
it's it's a really really serious issue
you know these are real consequences
yeah let's focus back on the one you
call out as the core yeah the breakdown
a sense making sometimes I also label it
information nihilism where people are
unable to make distinctions any longer
between you know what is sense and what
is nonsense yeah well we can go in a lot
of different directions on it I mean I I
think that your listeners have already
listened to Danish Martin burger
hopefully and and others so clearly
articulate that you know again speaking
to someone actually just recently high
up in the tech industry about this you
know we were talking about election
integrity and
you know what do we do to protect the
2020 elections and it's not like I mean
yes we should do more to protect against
bad actors but the amount of our natural
information environment now that's just
gibberish
clickbait and that that's the default
information environment I think that's
the invisible thing that's so bad is we
we need high quality sources of of
information and making sense of things
and at the very least I think we should
avoid anything whose business model is
chasing attention as soon as we know
that your business model is chasing
attention that just means that there's
distortion running all the way through
the way that you are publishing
information so things that are more
subscription-based Financial Times etc
are less likely to be incurring that
kind of damage yeah let's talk about
that a little bit I was involved in
building out fair amount of the
pre-internet and early internet online
information services including way back
yonder in 1980 the source the very first
consumer oriented online service and
throughout most of that time most
products were paid and were not
advertising supported the very end
around you know 1996 something like that
AOL started doing a little advertising
but it wasn't the core of the business
model until after the 2000s do you
really think it's possible to put the
genie back in the bottle and say no more
advertising supported businesses on the
Internet
well it's very very hard I mean it's I
think of the advertising business model
almost like the fossil fuel era of
energy in our energy economy you know
our entire economic infrastructure is
basically directly coupled with the you
know petro dollar petroleum in a global
economy and I think that you know much
of the growth of the the US stock market
in the last couple of years has been by
the major advertising based companies
and if you subtract that from the
economy you know how do you do that
that's one and a half trillion dollars
of market value at the very least if you
take Google and Facebook together who
are based on advertising so I'm not
saying it's easy but we have to be able
to name the pernicious nosov this
business model I mean I honestly think
I've been withholding from being the
outrage machine here but I think this is
leading to a kind of dark ages in terms
of
what we've already been heading into
we've been baking our society for about
you know six seven years in these
automated algorithms of YouTube and
whatever good they do now we have to
recognize that for six or seven years
they were recommending conspiracy
theories and extremism and outrage and
polarization affirmation on information
so you know first is to recognize that
the true costs are free we like to say
that you know free is the most expensive
business model we've ever created
because it destroys kind of everything
else you know and as you said I think in
a previous episode of your podcast we
can go back to things like subscriptions
I think you said that on average
Facebook would be $2 a month for a user
around the world and obviously the
developing world is different than the
mainstream world but we've we've
actually dealt with this with things
like Netflix I mean the average time
spent watching Netflix is something like
71 minutes a day and people pay eleven
ninety nine a month I think that's the
rate now I can't forget remember for
their Netflix account and you know
people actually spend almost as much
time on social media and they're not
paying anything but would you be willing
to pay you know if you already paid that
for Netflix why wouldn't you be willing
to pay for social media that was
actually in service of human values yeah
I keep talking about this as you just
pointed out and I don't get anybody to
nod their heads the fact that we did an
experiment once for $20 a month would
have brought one into a community both
political and online community and man
do people hate to pay yeah and I'm sure
you've read Chris Anderson's book free
the future of a radical price yeah and
unfortunately even one cent is very
different than free and part of that is
that people hate to have to fill out yet
another set of usernames and passwords
that have potentially high stakes
I either connected to their credit card
or to their bank account and gosh you
know I wish we could I could figure out
a way or somebody else come get a little
too old to be doing this stuff myself
but that the next generation of
entrepreneurs could figure out a way to
move back to an era where people pay for
value and advertising is not the core of
the business model but I don't see it
yeah well I think people like Apple you
know I always say that Apple is actually
one of the interesting actors in this
space because their business model is
not monetize
your attention in fact they're kind of
like the government of the attention
economy they choose which apps get to
participate in the App Store they choose
you know the rules and the sort of
building codes of what an app is and
what levels of notification access you
get and what you can and can't do and
all that stuff they can actually create
the kind of payments infrastructure that
could be based in you know built into
iCloud and you know for a few dollars a
month basically have dollars
automatically flow to things that
provide more value but again we need to
change the fitness landscape of the
attention economy so we're not actually
competing for attention at all but
competing for essentially help in our
lives and you know it's interesting I
use as metaphor sometimes of you know if
you ask me like five years ago would you
pay $11 multiple times a day to get
around a city you know to pay that much
money and I'd say well are you kidding
me that's like so expensive I'll I've
got a bike I've got public
transportation you know I'm not gonna
take taxis everywhere that would be
ridiculous that's so expensive but I
think that uber and lyft and the sort of
on-demand
ride-sharing provides such levels of
efficiency and value that we can
suddenly be the places that we need to
be that people are willing to pay
because it actually makes their time go
where we want it to go and I think that
that's what we're missing in this
economy is that if we actually lived in
sort of a time well spent world where
yes we're paying for you know
subscriptions or paying for money but
our use of technology aligns with where
we want it to go in the world and we
wouldn't be just left with this sort of
hyper distracted constantly overloaded
polarizing our society's going into a
dark ages I mean like I don't think it
sort of should be obvious to people by
now that this this business model is
just totally unsustainable well I don't
know if it's unsustainable except for
the fact that a crash society from a
business perspective it still seems very
dynamic and again if Chris Anderson is
right consumers have a gigantic
preference for free you know is there a
way for the market to solve this problem
or does this gonna require some
intervention from the political sphere
do you think you know I don't know how
it's gonna go down but obviously this
might be one of those things where look
if there's someone who's going to offer
a free service that ruins the world and
you know does public private profit
public harm and the balance sheets of
society but the balance sheet is so
expensive that it introduces a total
catastrophe
crash in that society if that crash
occurs later than the private profit
leading up to it they'll just keep doing
it and gain theoretically if the other
guys offering subscriptions for ten
dollars a month they can't compete with
that so be much better to you know
create a new Fitness landscape or
everyone's competing for subscriptions
that take us where we want to go it's
very hard to set the Overton Window of
society so that in culture all of these
consumers understand this trade that you
and I have just laid out right most
people don't understand these problems
are kind of big and diffuse harms and so
people just say no I'd prefer to have
the free Facebook so I can look at the
cat videos and see what my friends are
up to but if we all saw this cost if we
all saw that the true costs are free is
totally unaffordable and it kind of
sends us into a dark ages and then we
would support a government action or an
Apple action to say hey look you know
we're actually only doing subscriptions
for now on and it works in this totally
new way and by the way your time and
your values are gonna line up with what
you'd want them be yeah this is what we
call in political science a collective
action problem you know if we all could
somehow make it happen we'd be better
off but there's no real mechanism to do
so
yeah well I'm you know we're trying as
hard as we can I think that Congress and
other folks are getting more awake to
the issues and the public I think right
now the unfortunate thing is the public
has a kind of a diffuser or naive
concerned about the tech companies took
our data or they're doing something with
Russia we don't really like that but
people don't have a good articulation of
these issues so we need a clarifying
public awareness on the problem first
before we can take that public action
and of course you know the other issue
which I see fairly often I happen to be
unlike a lot of people in the kind of
spaces that I hang out in I actually
live in a electoral precinct that went
75% for Trump in 2016 and so I know a
lot of good decent solid people who have
non-coastal blue perspectives on things
I should say and part of their
resistance is they assume that the
executives probably rightly these tech
firms are blue coastal Social Democrats
and to give these people knobs to modify
and change discourse they look at it's a
probable censorship of their
point of view you know it's funny that
this is coming up so often among
especially conservative politicians that
the tech companies are actually
censoring them you know or down ranking
them but all the evidence points in the
opposite direction there's websites
showing the kind of traffic and
engagement to Breitbart and forgets it
daily
what's the Ben Shapiro one again I
forgot the name I don't know I don't
watch that shit daily I think it's a
Daily Caller or the wire or something
and you know they're actually getting
more attention than the other things
Alex Jones by the way was recommended 15
billion times by YouTube 15 billion
times it's very hard to people to get
their brains wrapped around these
numbers that's more than the combined
traffic of New York Times BBC Guardian
Fox News Breitbart combined right and so
anyway all this is to say that I
understand the concern about someone
putting their weight on the scales I
think we need something that's more
along the lines of the introduction of
public broadcast media you know and I
forgot when that was in the 60s or 70s
we can't have a personalization based
sense making infrastructure not a
hundred percent we have to have some
notion of a shared set of facts or a
shared set of truth if we don't have
that we're toast because it means that
no conversation can ever come to
agreement and consensus and that means
that things that are collective action
problems like climate change then we
already know the end of the story we're
all debt you know and so if we actually
are gonna take new actions and make new
choices that we didn't make yesterday we
need to be able to agree which means
that we're gonna need to have some
notion of a non fully personalized
information environment yeah that's of
course we talked about ads as one knob
the other is the you know one could
imagine outlawing the collection of
individual behavioral data in terms of
the system and how do you see that
solving the issues just just too curious
and understand your sense making well
let's put a sharper edge on it if
Facebook had no data about my behavior
it would be essentially impossible for
them to micro-target me and so the
efficiency of the whole cycle would go
down substantially probably by a factor
of five and we be getting closer back to
the economics of direct response us mail
as opposed to the unbelievably highly
efficient
targeting that can be done today so it's
essentially a major deoptimization
of the cycle yeah but I think one issue
is the collection of the data but it's
it's you know you have to ask like is
there a way that Facebook would be
unable or could they not have access to
your data I mean by definition they have
your friends by definition they have the
posts that we post by definition they
have to keep track of what gets clicked
so they can show you back later the
number of likes that you have so they
can't erase that information unless you
want to go to a totally ephemeral social
network or something like that the
second problem is that people actually
prefer personalized news feeds over
let's say chronological ones like we
could erase all of the engagement
metrics and say let's just sort news
feeds by you know what the exact
chronological order in time that they
posted them but people don't like that
because it's actually really inefficient
and you kind of scroll through a bunch
of stuff that's not so relevant we
actually prefer things to be filtered
for us and if the filtering requires
uses of data to make that process more
efficient people want that I think what
I'm pushing against is just we can't
have a 100% of our information
environment be tuned by personalization
because it eliminates the sort of shared
water water cooler or public broadcast
events you know in in countries in
Nordic countries I think it's in Finland
professor Fred Turner at Stanford was
just telling me about he's a great
history communications professor that
there's sort of a 10-minute public news
break that's at the center of their
soccer matches so whatever there's a big
football game yeah there's a 10-minute
break that everyone is tuning into and
they put that at the center of the
attention economy and create a sort of
shared set of information sense-making
for everyone and I think things like
that where we balancing the personal
with the public is what we need more of
how you let something's gonna fall back
up on the idea that we want personalized
services you know I've experimented
quite a bit with Facebook it used to be
you could actually set the setting to
show me the feed in chronological order
and I'm not sure it was worse than the
zuc algorithm but what Facebook does
they keep changing it back you know you
basically have to switch it once every
two or three hours if you want to keep
that on the other hand I have also
experimented with turning off history on
Google and you know for Google search
and at least for me I much prefer it
with his
Rhian I want Google to know that when I
type Python it usually means the
computer language and not the snake so
it's it's interesting then the domains
may differ a little bit to what degree
the manipulation is actually of consumer
benefit versus you know benefit for
presenting willing eyeballs for
advertisers and the ability to
micro-target yeah I think the principle
of data minimization and personalization
that is not for the interests of the
commercial entity but for our interests
more like a fiduciary you know Forrest
Landry and and I and others have kind of
articulated the argument of that
technology needs to act like a fiduciary
to our values based on the fact that
there's the degree of asymmetry between
its power over us and and our limited
power with it so if you if you compare
and you lined up these comparisons you
know you say let's say you know a doctor
knows way more about medicine than you
do and you're there lying unconscious on
the operating table you are fully in the
submission of what they can do to you
right so imagine the doctors entire
business model was just maximizing
profit shareholder value getting paid
entirely by Pharma pumping you with
drugs doing surgeries they didn't need
to do and that was the doctor about to
operate on you in that vulnerable
situation we would outlaw that kind of
fully commercial relationship of
something that has that level of
asymmetric power over the outcomes that
matter for you similar with a
psychotherapist who's dealing with the
very intimate details of your psyche you
know they could give you very misguided
advice they could lie to you they could
say oh and the to heal that trauma from
your childhood you have to have sex with
me they can say all sorts of things that
would be very vulnerable so we have a
name for that relationship it's a
fiduciary relationship instead of a
contract relationship to recognize the
asymmetry of power in those two things
and if you line up those two and the
degree of asymmetry how much do they
know about you that you don't know about
yourself how much are you trusting them
how vulnerable are you between a doctor
and a patient between a psychotherapist
and their client and then you ask how
much information does Facebook have
about you compared to you knowing about
yourself how about the difference of
intimacy between what a psychotherapist
knows about you versus what Facebook
knows about you you know you've all her
Ari gives this example that Facebook can
know that you're gay before you know
you're gay based on just the way that
you
dwell your mouse over ads of the person
of the same sex or something then then
the ads that don't have the person of
the same sex and you may not even be
aware of that yourself and so in general
technology is able to make predictions
about people that are asymmetric you we
would never allow that relationship to
be a commercial relationship because
it's extractive and unfair to the
vulnerable party so that's the kind of
change that actually we can use to get
at the business model and I think
addresses some of the the other issues
that we've raised one last metaphor for
people that I think is really helpful
imagine a priest in a confession booth
which is another sort of asymmetric
situation where you're there and you're
sharing all your confessions you're
sharing the most vulnerable stuff about
your life and your your your horrible
things that you've done in this your
sexual thoughts and all all of these
things except that priest is listening
to the confessions of not just the town
but 2.7 billion people right that's
Facebook except they also have a
supercomputer that's basically storing
and recording all the confessions made
by 2.7 billion people and then finding
patterns in the confessions so that they
can actually make predictions about the
next confession you're gonna say in the
confession booth before you know you're
gonna say it and so even as you're
walking up towards the confession booth
based on gait detection and how you're
walking they could make predictions
about confessions you're gonna make
because they have a supercomputer and
you don't we would never allow that
priest to have a commercial relationship
where they sell everything that they
learned about you and the predictions
that they can make about you that are
going to happen in the future
that you don't even know about to
another advertiser we should not have
those be commercial relationships we
need them to be a fiduciary or protected
relationship yeah like that's a very
very very vivid especially as a person
that was raised a Catholic I kicked that
particular curse when I was 11 but you
know I understand exactly what you're
saying and it's a very powerful image
one of the things I saw in something
that you wrote or want maybe it was on
the humane tech one is very similar an
idea I had which was what would happen
if we just taxed say internet
advertising very high say a hundred
percent tax on internet advertising do
you think that would be enough to get
the virtuous circle to spend the other
way yeah it's a good question you know
we haven't done a full policy economic
analysis of you know how to how to do
that part of this is like saying okay we
have
bad business model that's you know we
want to get people off of it and one way
we've dealt with this in other you know
situations right now if we subsidized
oil so we keep using it but what if we
start taxing it to a degree that it
starts to get more expensive than the
regenerative alternatives then there's a
crossing point and then suddenly the
economy flips into the regenerative
alternatives and so the issue here is
that all of our technology being
advertising backed is essentially
parasitic extractive and polluting and
we could tax it much like we want to tax
fossil fuels progressively carbon taxes
etc to try to create a transition plan
to ultimately fund a transition to more
renewable stuff here but the problem is
the you know the way we generate energy
is is different than the way we generate
these different kind of outcomes with
technology so I don't know the exact way
that this would work but one metaphor
I've given in the past to this is what
we've done with energy utilities if you
think about energy utilities they used
to have a perverse incentive they
actually make more money the more energy
you used including the more energy you
waste so please leave the lights on
leave the faucets on leave the shower on
you know we make more money the more
energy you use and we would actually
want to send that nest into your home
that actually encourages you with you
know points and rewards to keep using as
much as possible because that's how the
energy management prediction AI at
cetera would be incentivized and we
actually successfully went through
legislation in the United States as far
as I understand I think 50% of US states
now have what they call decoupling where
they fully decoupled how much energy you
use from how much money they make and
the way they do this is based on the
seasonal availability of energy and I'm
gonna tie this back by the way to
attention cuz it's another finite
resource you know a finite amount of
attention to me a finite amount of
environmental resources and and energy
how do we manage this so the way they
did this is you know let's say you're
sitting there with con Edison or PG&E
using energy and use some amount of
energy and they charge you based on how
much you use per performance but then
once you hit the sort of seasonal
availability they want to start
disincentivizing you so they double
charge or triple charge to make it more
expensive that stops you from using it
so now you have all these extra profits
showing up but those don't go into the
private
companies of PG&E or con Edison they
actually get put into a renewable energy
transition fund so that it's
collectively funding the transition to
the better system so you can imagine
something like this happening with
attention where right now Facebook
Twitter YouTube etc make more money the
more attention that they get from you
and that's a perverse incentive it gets
extractive at some point beyond the sort
of your feeling of being in control in
your own agency and imagine that they
can make some money from advertising and
attention up to a small point but then
the profits and revenue that are above
that point get reinvested into let's say
Humane Technology regeneration fund that
funds the alternatives and the antitrust
legislation and the public education and
the media literacy discernment against
this information cultural immune system
detection like all these kinds of things
to try to build the transition to the
infrastructure that we want we use the
profit-making capacities of this current
advertising based infrastructure which
is the most wealthy infrastructure we've
ever created these are the wealthiest
corporations in history to actually fund
the transition to something that doesn't
destroy civilization a lot of details
that need to be worked out there right
when you try to get government to invest
in new technologies it doesn't
necessarily work too well very often but
you know there's two parts of it even if
you just had the tax right it's supposed
yeah just had a tax 100 percent tax on
internet advertising it actually lowers
the activation energy for a subscription
service to be able to compete because
the guys with the advertising aren't
making anywhere near as much money as
they used to be making right so
obviously there's there's some more
straightforward plans like that I just
because I imagine that it doesn't have
to be done progressively in some way
much like carbon taxes have to it you
know build up slowly over time there's
different ways of doing it yeah and I
will say but as part of my prep for this
call I watched your testimony in front
of Congress and I gotta say our Congress
critters well maybe they're trying to
get it now unlike a few years ago
they're still pretty vague on the whole
thing seems to me yeah those you know
there's differences here you know we've
said in a previous episode in our
podcast we actually this was a recent
house congressional hearing
Jim's referring to on online deception
it was called Americans at risk
deception in the online age
she was with deep fakes and I was the
House Energy and Commerce Committee on
consumer protection so I will admit that
you know many of those members really
were not as aware certainly to the
degree of harm that's being created and
they were framing it as a problem of bad
apples there's a you know we have these
really good social platforms that are
you know the the gems of the American
economy and doing good in society and
then we just have these bad guys these
bad BOTS bad Russian BOTS we've got
these bad deep fakes we've got this bad
fake news content and we've got these
bad dark patterns we have to get this
bad stuff off these good platforms and
the main reframe I was trying to provide
is that's not the situation at all the
natural functioning of the social media
advertising extracting attention
extracting platforms are to cause these
harms and that's not something that I
think the members were aware of but I
will say you know something else that
which is you know if you go back to the
Zuckerberg hearings you know if I ask
you what do you remember do you remember
Jim about what's the one talking point
or sort of memorable thing coming out of
those hearings I truthfully I didn't
even watch them I did watch his
Georgetown speed I see but I did not
watch him on Capitol Hill well for those
who who were tracking that the typical
answer is there's a one moment where I
think it's orrin hatch asks Zuckerberg
so mr. Zuckerberg how do you make money
and Zuckerberg replies senator we sell
ads and that being the most memorable
line coming out of what was probably
more than eight to ten hours of hearings
right it basically makes the public
remember one thing which is that
government is incompetent doesn't get it
and they didn't even know his business
model etc how would you ever trust
regulation coming from a body like that
and if I were Zuckerberg and if I were
Facebook I would have paid for that
moment to happen because it generated
the kind of trust imbalance that I would
want people to be storing in their
brains and what it makes you forget is
that there is plenty of other questions
asked by other senators and Congress
members that were actually very good
senator Kennedy from Louisiana had us
some really fantastic questions
Blumenthal Warner the honest ads Act
there's a lot of good stuff happening on
Capitol Hill it's not passing but the
point is there's a distribution of a
and there's great groups like Tech
Congress that are trying to actually
bring more people with technology
expertise into government you still a
left this problem which if we kind of
zoom out a little bit is the problem
statement that we and our nonprofit the
Center for human technology kind of
focus on which is the EO Wilson quote
that the problem of humanity is we have
Paleolithic emotions medieval
institutions and godlike technology and
that those three operate on different
clock rates because the Paleolithic
emotions in our brains are baked 200,000
years ago our medieval institutions
update very very slowly craft laws very
slowly elect people very slowly and then
our godlike ik technology is
accelerating at faster and faster rates
meta acceleration and it you know what
happens when your steering wheel is
lagging four years behind your your
accelerator you're gonna crash so some
way or another we have to bring the
clock rates of these three things into
more alignment and have a better control
system which is why you know Daniels
Martin burger says I think referencing
Barbara Marx Marx Hubbard that if we
have the power of gods we have to have
the wisdom love and prudence of gods and
discernment so that that's kind of the
broader situation that when we asked
about regulation it needs to be
thoughtful about the godlike capacities
that that we've created it would be nice
if it could happen I'm not sure I
believe that it can happen within the
status quo I think as you know Daniel
Schumacher Berger myself and bunch of
other people are working on something we
call game B which is to fundamentally
reform society in a way to be able to
actually deal with these issues you know
everything I've seen everything I've
heard I just don't see it I mean how are
we gonna solve climate change with a
collective action problem like we have
and bad faith discourse etc how are we
gonna get you know trillions of dollars
worth of capital in terms of these
advertising supported business models
under control unless we have a
fundamental rework of our political and
social institutions yeah no I mean it's
it's it's an enormous problem but I'll
say right now that the technology
companies aren't helping I mean right
now more than about 50 percent of
recommendations about climate change on
YouTube are for climate denial videos
and climate
hoax videos etc and so when we have
polarization and climate denial actually
being the default information
environment I mean Jim the reason I work
on these issues actually in the well I'm
so concerned is I actually think the
climate you know ecocide issues are the
thing we really have to pay attention to
and the technology makes it impossible
for us to ever come to agree because
it's polarizing us into different
realities and that is the most
existential issue for solving any of our
issues so this is almost like the
infrastructure issue we have to solve to
to address the other ones yeah it's the
forcing function or at least it it's the
outer forcing function right if we don't
destabilize their society other ways you
know for instance the level of
inequality gets so high that we have a
violent revolution with the usual
outcome of some bad dictator in charge
you know if we don't accidentally create
a genetically engineered organism which
produces Giga deaths you know if we
avoid nuclear war then climate change
will get us some time early in the next
century so we have a lot of existential
risk problems not just climate change
but it's the one that we know is coming
the other ones are more speculative and
lower probability yeah yeah we've got
we've got a lot of sense making that we
need to do to deal with all of the the
shorter term threats I'm with you there
yeah and is there any hope at all of
these incremental approaches driven by
the US Congress can get us there well I
think that there's what I am optimistic
about and I'm not saying that I'm
optimistic about the whole system but we
have to keep going on is that at least
in Congress what I found is it's not
that everybody knows everything we've
laid out and the sort of information
ecology destruction dark ages sort of
thing and they just disagree it's
actually this that they don't know and
when people wake up to what's actually
going on they're terrified and they want
to help so I actually think there's you
know we sort of say like unlike climate
change where you have to convince
thousands of companies in hundreds of
countries and get lots of governments to
coordinate to change all of this
infrastructure to be carbon neutral you
know in the case of technology there's
probably only about a thousand people in
one country regulated by you know couple
key states in one federal
to basically make some of the needed
changes so at the end of the day about
only a thousand people technology
leaders product managers designers
executives metrics people a few
regulators may be a few strategic
litigation cases would need to actually
be involved in this process and so you
know we sort of have this thinking piece
that we haven't published that sort of
how do we get to humane technology by
2030 and it involves essentially you
know those thousand people activating in
these specific ways that would be
interesting I've looked very much
forward to reading that paper when you
guys put it out something you mentioned
in your congressional testimony and I
think you mentioned elsewhere is the
current weakness of third party
fact-checking services you know there
are two guys on a folding table
basically trying to hold back the flood
of bad faith this course a friend of
mine guy named dick brass who was I
don't if you know dick he's a very
interesting guy he was a senior
executive you know SVP level at
Microsoft he was also at the same level
at Oracle on the few people that was
somehow managed to be personal friends
with both Larry Ellison and Bill Gates
he's got an idea he's been floating
around that the big tech companies you
know that the handful that you're
talking about ought to get together and
fund a hundreds of millions of dollars a
year fact-checking enterprise
not-for-profit engineered rigorously to
be of that objective transparent you
know as good as it can possibly be and
then let all these platforms use the
output the signals output from this
third party the fact-checking system to
start to down-regulate bad faith this
course any hope along those lines well I
like the idea of funding with hundreds
of millions of dollars a sense making
protection layer for society that's
shared among technology companies but I
actually think what we really need is to
build the kind of cultural sense making
infrastructure you know what kind of
Daniel in forests and Zack Stein talk
about of just better sense making you
know I use the analogy sometimes of back
in the 1940s when the United States saw
Germany a country that had the most
sophisticated philosophy science culture
you know as an epitome from I mean a
kind of peak of a civilization in in
Europe
how that culture that was so
sophisticated could be the one that
would fall into authoritarian Nazi sort
of rule and it sort of broke their
expectations about what is psychology
like we thought we knew ourselves we
thought we knew human nature but the
fact that this happened made us question
everything and so there's a surge of
interest in psychology and the United
States there was a group called the
committee for national morale that was
funded to help cultivate what they
called the Democratic personality
because they didn't take a democratic
mindset our Democratic personality as
something you know to be taken for
granted something you get for free it
was something we had to grow and
cultivate and people like Gregory
Bateson and Margaret Mead I know you had
Nora Bateson on this podcast people like
this were actually brought together to
say how do we cultivate a gala terian
tolerant empathetic mindsets and you
know this this is what in the United
States was was developed there was also
a related group called the Institute for
propaganda analysis that helped dissect
in pamphlets and in libraries they did
lots of programs in schools that they
helped dissect foreign propaganda
because they were worried that fascist
propaganda would enter into the minds of
US citizens and they had to make people
more aware of how that happened and you
know so the point now is that we have
actually more knowledge than ever more
than we had in the 1940s about how the
human mind can get influenced cognitive
biases behavioral science social
psychology we have way more encyclopedia
knowledge about how the mind is
influenced by propaganda than ever but
instead of defending the American psyche
were actually still behaving in a
libertarian way as if each mind is this
free instrument uninfluenced by anything
you know and this this is the thing that
really has to change so I would love to
see a hundred million dollars go into
protecting that you know and I gave this
metaphor and the congressional hearing
that if Russia tries to fly a plane into
the country we have a pentagon to
protect against that from happening in
the physical world in other words our
physical world is governed by
institutions and and protection and
systems of protection but when you go up
a layer into the mimetic virtual
infrastructure of the web and and these
private Facebook universes
you don't have a pentagon you just lost
your protection mechanism so now if
Russia Iran North Korea Israel Saudi
Arabia all countries who have been found
to be running information operations in
the United States start dropping
information bombs micro-targeted two zip
codes going into US military veterans
groups in sowing disenchantment we don't
have a pentagon to protect that from
happening
we actually have 50 people on Facebook's
trust and safety team who are actually
not even incentivized to look at the
problem until there's public pressure to
do so so this is this is the kind of
thing that I think we really need is a
recognition that each society each open
society online is effectively in a
global information war that can't be
solved it's going to be a chronic issue
and so we need the kind of public
education and sense-making capacity that
we haven't had yeah I mean that would be
good but what is it I mean you know how
do you go from a point where there are
some people actually believing in Flat
Earth and Alec Jones to a point where we
have developed a immune system to that
you know is it a membrane that we keep
the bad stuff out is it more like an
immune system internally where because
we work collectively in small groups
we're able to sort through bad faith
discourse sort of architectural II how
might this actually work yeah it's a
great question I don't think anybody who
claims to have a clear answer I would
doubt I think the first thing for me is
people have to see this problem as it
occurred because if you try to say to
someone like it's change your beliefs
from these crazy conspiracy theories and
Alex Jones that's not really gonna work
well you if you if you tell someone like
we did with cigarettes this is bad for
you that doesn't actually stop someone
from from from doing it you have to show
them how they were manipulated and
that's why I have always with the
magician frame and the persuasive
technology frame focus this conversation
on what is the artificial illusion the
kind of hypnotic spell that's been cast
upon Society for years and years and
years to say this is how the world went
crazy if we don't actually have looking
backwards the last seven years a view of
how this is actually what happened to
sense making so we can all see this
thing that happened that gives you the
kind of metacognitive perch you can
climb up to to now see these are the
distortions that happen in our society
and I think that's the first thing is a
collective understanding
of the process of how conspiracy
theories came to win and how Trust came
to lose and all of that so that's kind
of the the first step and then I think
we need an immune system that makes it
easier for us to you know not pollute
the information ecology we need more
deterrence rather than defense so
instead of whack-a-mole you know AI
super lasers that are looking at
shooting down the bad information that's
coming in and trying to scan for fake
news and trying to catch it at scale we
need more deterrence consequences for if
something were to go wrong you would be
held accountable and that would cause
more people to be more careful about how
they contribute to the information
environment this reminds me of things
like you know in China for example they
will allow you to post a deep fake but
only if you disclose and label it as
such that this is a deep fake and you
can actually go to jail if you publish a
deep fake without labeling it as such
and that's an interesting example where
you know you can post certain things but
you have to take responsibility for
labeling it clearly you could have
something like that on Facebook where
they could even say hey look there's all
these mainstream news channels whether
it's MSNBC or Fox News or Breitbart or
whoever who might publish a deep fake
and not label it and Facebook could say
hey we're actually gonna suspend your
account for 36 hours on Facebook if you
actually post a deep fake without
labeling it so they're not saying you
can't post it they're saying you have to
say that this was edited and labeled and
I think we need more pre-emptive
liability and deterrence than we need
defense
yeah the examples you just gave there
are things that are well within the
power of the platforms to do today yeah
absolutely
how close do you think they are to
feeling like they're an under enough
pressure from the political sphere to
actually spend some of their resources
on things like this well they're not
spending nearly enough I mean we've
given the analogy in the past that if
you compare how much Facebook spends on
security to their total revenue it's 6
percent of the revenue you compare that
to a city you know Zuckerberg likes to
claim that Facebook is a global
community of 2 billion people like one
big city well let's take a city like Los
Angeles and how much do they spend on
security they spend 25 percent of their
budget on security so Facebook is under
spending
by about four to five times basically so
you know certainly one thing is they
could spend then more money on it but my
I think deeper issue is I think that
they're not thinking about it the right
way because they're focused on catching
the bad guys catching the bad speech and
then they have these free speech issues
in the United States so they don't want
to get in on it
so I think we need to change the
conversation from from free speech to
about distribution context reach breech
disclosures another thing that they
could do for example is whenever there's
an information operation they can back
notify everybody who was impacted there
is I think something like 55 million
people impacted by a recent Saudi
Arabian information operation I believe
that the Russian 2016 information counts
operations that they was affected about
a hundred and fifty million Americans it
was a hundred and twenty six million on
Facebook and if you counted Instagram
it's a hundred and fifty million but
those people aren't even aware that they
were affected or impacted so one of the
things that Facebook could be required
to do is to back notify everybody who's
been affected and this is be very
similar to things like when there's a
security breach and Equifax says hey
there's a breach our data got out there
of all these users we have a
responsibility by law to notify
everybody that whose data might be at
risk well we have should have the same
thing for information accounts and the
joke is that NPR actually does this
where if there's a correction to a story
that they have to issue like a fact
check and they have to go back and
notify people they actually through
their app have successfully done that so
the joke is if NPR can do it than
Facebook can he would think so of course
there's a lot of kind of subtle
distinctions there what constitutes
sufficient exploit that would require a
notification right you know we all see
dodgy crap on our Facebook feeds all the
time but some of most of it is not
objectively that bad faith discourse as
you said it's more people have peculiar
framings which distort the meme space
you know where do you draw the line get
into very interesting philosophical
questions about for example what is an
authentic Texas secessionist because you
have a lot of domestic people with ideas
that might be more extreme or wanna say
things but if I'm Russia I don't even
have to plant new information operations
in your country get people to believe
something new I can just find existing
people who believe something that might
be more fringe but I can dial them up by
sending a hundred Russian BOTS at them
and making them get thousands and
thousands of retweets and and and make
them go viral and so there's this
question of well Russia's not even and
isn't to be Russia by the way it can be
anybody and just using them as an
example my friend Renee di Resta was on
the Senate Intelligence Committee and
studied the full 200-thousand meme
Russian dataset so I just happen to know
a bit about it and one of the things
that they did go after her beyond
african-americans was the state
secessionist movements so the California
secessionist movement and the Texas
session is movement and so you could say
well what's the harm there is the
question I hear you asking medium
because those people believe that anyway
why should we notify you that you've
been affected by an influence operation
when I was just thinking and believing
those things and agree with it anyway
the point is that people feel more
discussed in repugnance when they're
realized that they're the target of
something of something that's trying to
influence them and so I think that
that's what we have to introduce is the
common knowledge that there are large
influence operations happening from
foreign countries all the time and that
this is not just this sort of safe space
so our our trust is essentially
miscalibrated you know we're calibrated
to trust things to a certain degree in
the physical world but we should have
less trust in the virtual world and
we're not really operating as such at
least a different kind of trust I don't
have to worry about somebody robbing me
at gunpoint online but I do have to
worry about them inserting perverse
memes into my head by repetition if
nothing else that's right and I just
think that overall people underestimate
the cost of what we're talking about in
the scale because it sounds so small and
so subtle right I mean you know like
those military weapons that you can
point it at someone and you can generate
sound that only they will hear or even
it sounds so loud that will actually
catch them off guard there's like these
specific weapons that do that so if you
move an inch away from that person you
won't hear anything that's like micro
targeting right I can micro target
information bombs all throughout your
society two zip codes or two individual
user lists as using Facebook custom
audiences or look-alike models I can
even take all the flatterer with
conspiracy theorists in your population
use Facebook look-alike models to say
hey give me a thousand users who look
like that in your society and I've just
found all the conspiracy theorists in
your society so basically your whole
society is up for grabs and I can
my fingers and go in and wreak havoc and
you don't even think it's happening so I
just want people to really get the
degree to which we are living in a
full-on global information war there's
an article that something like 70
countries are now participating in
disinformation operations around the
world so it's a very common activity now
and we have to make sure that this is
part of the trust environment that we
are recognizing that this is the
situation as opposed to thinking that
it's some kind of conspiracy theory
because like I said the person next to
you could be the target of one and you
wouldn't even know interesting and of
course United States is probably in the
top two in spending on disinformation by
government actors of I certainly hope so
I hope our guys are doing it right I
actually think that from what I
understand we have not been very active
on social media sowing sort of social
media disinformation in other in other
countries there's some regulations
around this I forgot after Snowden and
domestic propaganda the US citizens
there's some rules that I I forgot Renee
knows more about that yeah they
certainly shouldn't be doing in the US
but interesting and now of course now we
talk about this we're playing
whack-a-mole we're putting makeup on the
skin cancer etc if we went all the way
up the stack right and banned all
advertising online banned all use of
behavioral data for targeting the whole
problem goes away yeah a lot of it would
really go away and I think I would love
to live in a world where technology is
actually back to being a servant and a
tool and empowering to the fabric of
society you know if you go back to
Facebook in 2005 Mark Zuckerberg gave a
speech saying you know what Facebook was
and he said it's an address book it's a
utility it's a social utility that's how
we described it it doesn't have to be a
newsfeed it doesn't have to be a news
platform they only did that because they
got caught in the attention game theory
of competing with Twitter and the race
to the bottom the brain stem etcetera
but we could actually as you said ban
these kinds of things from being the
information environments and go back to
a more open Internet of information that
would solve a lot of the problems
because we wouldn't be so hooked I mean
would solve some of the addiction issues
the teen mental health issues of
newsfeeds
it solved the sort of issues with
constantly being raided some of the
sense-making issues personalization
polarization it's just that's a
radically different society that's why I
always spend
a lot of time focusing on the costs and
the threats of all this because I think
people so underestimate why that kind of
drastic action might be needed yep and
you know the more I hear you're talking
about more we think about it it strikes
me that fighting the war around the
edges is a losing proposition perimeter
defense is always a very difficult
we may have to bite the bullet and go to
the root causes that's right that's
right I mean the perimeter defense might
work but when you're generating you know
I think Daniel Stockton burger is
framing on this is when you have
exponential tech where the
dimensionality or polynomial of the the
different kinds of harm you might be
generated is so vast you can't account
for like that level of a polynomial of
harm and sort of clean it up at the
edges you have to actually deal with it
at the source exactly so that's why we
have to have systems that are not
generating you know that dimensionality
of harm and you know it's at a point of
technology you know deep fakes are just
the first thing right deep fakes
amazingly haven't caused nearly as much
problems as people thought they would
maybe we've gotten smarter and how to
deal with that but you know the ability
to you know craft human sounding posts
with images embedded and spin up vast
armies of fully automated BOTS you know
that's gonna heighten these problems by
the natural growth of the of the
efficacy of AI yeah and I think actually
people underestimate deep fakes are now
starting to be used in ways that are
very real
recently Saudi Arabia was found to have
generated I think it was I think this is
right fifty five thousand is my
recollection accounts that were actually
the first to use the deep fakes for
generating fake faces so if you go to
the website this person does not exist
calm and you refresh the page over and
over again you'll Newt you get a new
fake face and these are faces that look
absolutely indistinguishable from real
people's faces and I believe that this
was used by an influencer operation in
which all of the fake accounts and BOTS
looks like genuinely real people using
tools like that the one that's also most
alarming I think that's coming is at GPT
or DBT - it's the deep faking text yes
exactly the ability to synthesize text
in a way that sounds
indistinguishable from you know the
action
voice of the person think of it like a
CSS stylesheet where I could take all of
your written writing Jim and then I
could generate text that you know it
sort of has all the gymnast to the way
that you say things but it's all in text
form and because the dimensionality of
text is so much less than the richness
of voice or voice synthesis it's much
easier to fake then you add things like
you know when the FCC asked for comments
on
I forgot exactly what it was they asked
for public comments on some new change
in policy and there was later a study
showing that something like more than
50% of the comments were actually
generated by BOTS when you have comment
fields that can be spun up with you know
bot farms by state actors or non-state
actors you know just spewing fake text
everywhere this is sort of the checkmate
on human trust mechanisms because when
we cannot trust the very things that are
in front of our eyes or the text that's
in front of our faces the thing that's
ultimately gonna matter the most in this
new world is what do we trust and I
think that we're gonna have more
preferences for in-person interactions
and real life experiences and hopefully
back to some institutions that can
hopefully be regaining our trust because
they'll actually admit some of the ways
in which maybe there was there was good
reason to not trust them in the past
yeah TPT 2 was exactly the kind of thing
I was alluding to in the in the growth
of AI and GPT 2 is overrated with
respect to its capabilities however I
can guarantee a year from now there'll
be something five times better and a
year after that five times better again
so exactly whether GPT 2 is the actual
problem or not we know we'll have that
problem in the short term which actually
reminds me of another top of the root
cause stack that doesn't get talked
about too much one of the things that
Facebook does which in general I think
is better than the alternative is they
attempt to establish an ecosystem where
real names have to be used however they
intermittently do it an okay job of
policing that if real name only was able
to be maintained at a higher level of
fidelity some of these exploits would be
a lot harder to do certainly the idea
being spin up bot armies would would
start to go away I'm like on Twitter
where there is no standard of identity
and there you can assume that
armies are running amuck yeah I would
actually say when we zoom out and look
at all of these harms we've already
named one of the biggest sort of
generator functions for the harm which
is the advertising business model but
the other generator function is the lack
of infrastructure that authenticates
that people are who they say they are
and there's a notion of a good faith or
a bad faith actor you know in the
internet for the first 20-30 years we
just assume most people were good faith
blog posts you read etc that's what's so
exciting about it is you just have good
faith use of a system and so everything
was positive everyone's posting things
that are interesting no one's trying to
sow disinformation propaganda it's like
mostly a pretty open good faith
participation system but essentially
because the borders are open and there's
no digital passport that says you have
to be who you say you are and I can use
VPNs to get around whatever system
you're asking me to do I can use US bank
accounts to even validate my US
citizenship and this is what you know
all the state actors have the Russian
trolls have US bank accounts so it's
really hard to solve these problems when
you don't have a mechanism to validate
that someone is who they say they are
real names is one tiny part of it but
I'm thinking about how do we have a
whole new identity and trust
infrastructure for that and that's
something we really need research into I
know there's some people working on
different parts of it I'm not an expert
on that topic but it really is one of
the things that addresses a huge chunk
of these problems and not addressing is
what enables so much of these problems
yeah we couldn't even get a national ID
card in the United States to handle
immigration issues right hard to imagine
the fringes what they would say if we
had mandatory strong digital know your
customer requirements for online right
and it's it's so hard I mean I say this
with a sigh because without this we're
gonna continue to have the chronic
information wars the lack of trust the
fact that anybody can sow doubt anywhere
they want and we won't know whether to
trust that person or not I mean that's
that's what's so hard and he keep in
mind people do the kind of long con
thing too so if you have a trust system
that says well based on the fact that
the patterns of behavior for this user
have looked good for a while and this
account creation dates from 2009 maybe
that means that they're a trustworthy
account right they can
been here for this long I would be more
skeptical if the account creation date
was last week during the Mueller report
coming out or something like that but it
turns out that there's actually whole
markets dark black markets where you can
buy vintage accounts right from that
were creative creation dates of
2009-2010 and so this is why we really
do need some kind of new authenticated
identity infrastructure for any kind of
social technology that self validates
over time it is interesting I laugh
because I joined Facebook a little late
2009 I think it was and the first week I
was on there I said I hope the CIA is
starting to grow some accounts to be
used for the background story for their
agents because otherwise this is gonna
break the whole spy business and of
course sure enough they were they made
the same inference and started growing
those legends that's the word I was
looking for they had the grow accounts
to provide legend in social media and as
you say you can go to the dark web today
and you know buy or sell you know a
well-aged it's reasonable looking
identity so age alone and the kind of
soft network generated cues that
Facebook provides aren't enough right
and this comes down to the
dimensionality of trust signals and cues
that we can use because you know you're
left with well do they what does their
facebook profile photo look like do I
have a mutual friend with them and
what's the account creation date if
there's only three cues to go off of
that's not a lot and when in the real
world we have many many different cues
and we have whole body language and it
is lots of ways that can act get hacked
- back to the magician background and
you know con artists but I think we have
to think about how can we increase the
dimensionality of trust signals to
better reflect the kinds of things that
we care about well Trish Don that's all
been very interesting but it kind of
leaves us at a dark point you know we
have multipolar traps we have game
theoretical races to the bottom we have
a political process that doesn't really
seem to be capable of dealing with the
problems on a timely enough basis and we
have platform companies with incentives
that aren't necessarily aligned to
solving these problems do you see
anything good out there some new green
sprouts that are hopeful signals that
maybe we are learning
making progress on these fronts yeah
absolutely and not that this is an easy
transition to make but I think you know
when the thing that's keeping you warm
is a tire fire that's also polluting
here your air you're still not gonna
walk out into the cold desert if you
don't know that there's something else
you can go to so it's really important
to talk about the fact that there is a
way to do this differently but we have
to have a you know a real deep
understanding of what the failure modes
are what the mistakes we made are in how
we got to the environment we're in today
so the main thing that we focus on with
humain technology is that technology can
be designed to be almost organic to our
Paleolithic emotions to be much more
aware of how our you know emotional
systems and our cognitive biases and our
choice making systems can get distorted
so you know if you think about what
world we want to be heading into going
into 2030 what is the role of Technology
do we want a future where most of our
time is spent on screens disembodied
disassociated etc do we want a world
where culture is been basically hijacked
by technology or do we need a world
where there's a sort of primacy to
culture human values and social life
meaning you know that we actually have a
kind of human based discussion process
we have you know human choice making
we're not just sort of making choices
through screens so if you zoom out and
say okay what that would look like well
there's let me give you some examples of
things that might embody that cm Poe was
a Android OS home screen launcher that
had mindful features so part of the
insight they had was hey you know when
there's color when you have a color home
screen it's kind of addictive why don't
we make the home screen grayscale hey
instead of you know doing notifications
one at a time why don't we do it batch
them all and deliver them at a certain
time during the day and have a
distinguishing kind of notification for
things that genuinely need your
attention there's something called
breeding day I which actually uses a
camera to track your breathing and heart
rate variability and dynamically adjusts
the color and fonts this might sound
like really small stuff by the way but
what I'm doing is building up a stack
saying okay well how would it work from
the foundation of the thing we're
holding in our hand and the disruption
to attention cognition memory breathing
social isolation addiction polarization
mental health we have to kind of ask
like all the way up from the bottom all
the way up to the top how could
this work so those are the two examples
you can imagine things like habits being
baked into the way that your phone works
so instead of just you know when you
open up your phone and it's not really
aware of your broader life goals or what
you how you want to wake up in the
morning it just sort of says here's your
last social media app that was open when
you wake up in the app switcher instead
have modes where when we wake up in the
morning next to our alarm it sort of
lets us create that space and say what
do you want to do and you could say I
want to pick these habits whether it's
meditation or you know stretching or
just 10 minutes of silence and actually
design the interface around that and
things like headspace or waking up Sam
Harris meditation app or calm calm would
kind of fall into those kinds of
categories instead of being lonely and
not knowing where your friends are and
feeling addicted all the time you could
have things that help you find out where
your friends are and when there's sort
of serendipity around you baked into the
way that phones work Apple with its find
my friends feature which they I think
they renamed find my could be baked more
directly into the way that the operating
system work so instead of just an app
switcher it could be like essentially a
GPS for where people are or which people
are available and create more salient
easy ways to signal our presence and
availability to reconnect with people
because we all know that moment when
you're feeling kind of alone and it's
really hard to reach out to friends
because you just kind of are in a
disassociated state it's really hard to
knock yourself out of that state but if
there's lighter-weight ways we can mark
ourselves available you know that's
that's like a step in the right
direction things on sense-making and
finding congruence and agreement I know
Jim you've been participating in letter
dot wiki which is a really great service
for long-form almost like you know
letters between the founding fathers
debates between public intellectuals and
big thinkers about the right framing of
big topics so instead of saying climate
change is just gonna kill everybody
actually have a debate about climate
change versus economic growth and so
that's been a great service there's
something called V Taiwan which is the
or pole dot is which is Taiwan's
sense-making platform so the digital
Minister of Taiwan Audrey Tang has this
platform that basically lets people
draft statements about how some matter
of public society should be resolved and
they can respond to other suggestions by
either agreeing or disagreeing with them
and when there's a kind of a rough
consensus it
tries to gamify and enhance consensus in
other words so that's a really
interesting example I recommend people
check out I could go on and on I mean
there's there's other things like artery
which helps people find living room
concerts for so it's like an air B&B but
for for basically renting space out for
more physical events if one of the
problems of Technology is hollowing out
our physical world hollowing out
libraries public spaces town squares for
culture etc what if technology were
essentially instrumenting that an artery
is something that does that hip camp
helps people do sort of Airbnb but for
online camping spots this is sort of
hard to organize into a list because
there's so many different things but
there are examples of technologies that
are actually caring about and tending to
the social fabric and the physical
spaces around us and caring for
intending to you know the ergonomics of
human biases and cognition and trying to
ask how do we make it really work to
empower the kind of meaningful choices
that you know Joe Edelman talked about
in one of your recent podcasts so those
are hopefully some examples of things
for people that to check out if they are
interested yeah that's good it's
certainly important that we don't
despair that even when things seem like
they're overwhelming there are ways to
make progress and those are some good
examples yeah absolutely and just one
last thing is I think one of the other
things we have to look at is just making
sure that the expertise guiding the
decisions and how we make technology
matches the expertise that would be
needed if you're affecting things like a
social fabric you know I think about
this like should Economist be the ones
running the infrastructure that are
gonna affect let's say the viability of
the pollinators the decomposers
and the web of life if they don't even
know what pollinators or decomposers are
so we have to have a kind of match
between the people who are running the
management infrastructure and the kind
of substrate that actually creates the
foundation upon which the economy can
stand so similarly with technology can
we have 22 you know 30 year old computer
science train stem trained engineers
making decisions about how social
fabrics and identity and children's
development work know if they don't
understand those things then they should
be instead making technologies that make
space for you know essentially that the
natural brilliance of children's
development and social development etc
to occur outside and make sure that the
technology is not taking over parts of
society it doesn't fully understand I
think that's another area in terms of
expertise matching that we need to think
about it's not been in the front of mind
of entrepreneurs starting companies
having been one I know you know we're
just trying to figure out how to build a
product that will sell and how we can
develop a bigger pro-social mission yeah
I think that's a beautiful dream we'll
see how we can make it happen
let's make it happen well Tristan thank
you very much I'm really glad there are
people like you and your organization
which keep from me again the name of
your organization the Center for Humane
Technology and your website humane
techcom and if people are curious to
learn more I definitely recommend
checking out our podcast your undivided
attention where we explore some of the
point issues and walk through it alright
thank you again it's been great to have
you on thank you so much Tim it's been
great to be here production services and
audio editing by Jared Jane's consulting
music by Tom Muller at modern space
music dot